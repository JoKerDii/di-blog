<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/di-blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/di-blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/di-blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/di-blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/di-blog/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jokerdii.github.io","root":"/di-blog/","images":"/di-blog/images","scheme":"Muse","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/di-blog/js/config.js"></script>

    <meta name="description" content="Substack  Generative AI is revolutionizing how code is written. In just the past 6 months, coding assistant tools like Cursor, Windsurf, Lovable, Bolt, and Replit have evolved from being cute ways to">
<meta property="og:type" content="article">
<meta property="og:title" content="2025 July - What I Have Read">
<meta property="og:url" content="https://jokerdii.github.io/di-blog/2025/07/04/2025-July/index.html">
<meta property="og:site_name" content="Di&#39;s Blog">
<meta property="og:description" content="Substack  Generative AI is revolutionizing how code is written. In just the past 6 months, coding assistant tools like Cursor, Windsurf, Lovable, Bolt, and Replit have evolved from being cute ways to">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-07-04T20:19:23.000Z">
<meta property="article:modified_time" content="2025-07-04T20:19:23.000Z">
<meta property="article:author" content="Di Zhen">
<meta property="article:tag" content="readings">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jokerdii.github.io/di-blog/2025/07/04/2025-July/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://jokerdii.github.io/di-blog/2025/07/04/2025-July/","path":"2025/07/04/2025-July/","title":"2025 July - What I Have Read"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>2025 July - What I Have Read | Di's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/di-blog/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/di-blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Di's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#substack"><span class="nav-number">1.</span> <span class="nav-text">Substack</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#articles-and-blogs"><span class="nav-number">2.</span> <span class="nav-text">Articles and Blogs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#papers-and-reports"><span class="nav-number">3.</span> <span class="nav-text">Papers and Reports</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#youtube-and-podcasts"><span class="nav-number">4.</span> <span class="nav-text">YouTube and Podcasts</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Di Zhen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/di-blog/archives/">
          <span class="site-state-item-count">43</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/07/04/2025-July/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="2025 July - What I Have Read | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2025 July - What I Have Read
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-07-04 16:19:23" itemprop="dateCreated datePublished" datetime="2025-07-04T16:19:23-04:00">2025-07-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="substack">Substack</h2>
<blockquote>
<p><em>Generative AI is revolutionizing how code is written. In just the
past 6 months, coding assistant tools like <a
target="_blank" rel="noopener" href="http://cursor.com/">Cursor</a>, <a
target="_blank" rel="noopener" href="https://windsurf.com/">Windsurf</a>, <a
target="_blank" rel="noopener" href="https://lovable.dev/">Lovable</a>, <a
target="_blank" rel="noopener" href="http://bolt.new/">Bolt</a>, and <a
target="_blank" rel="noopener" href="https://replit.com/">Replit</a> have evolved from being cute ways
to help with 10-20% of code to now generating the majority of code for
many startups. <a
target="_blank" rel="noopener" href="https://techcrunch.com/2025/03/06/a-quarter-of-startups-in-ycs-current-cohort-have-codebases-that-are-almost-entirely-ai-generated/">1
in 4 companies</a> in the latest YC batch have 95% of their code written
by AI.</em></p>
<p><em>This new way to build products is much faster and simpler than
before, it involves just 4 steps.</em></p>
<ol type="1">
<li><em>Prioritize features by impact</em></li>
<li><em>Ship simple version or clickable prototype</em></li>
<li><em>Test at scale with users, measure impact</em></li>
<li><em>Iterate or kill</em></li>
</ol>
<p><strong>― The Lean Startup is Dead - Fletcher Richman</strong> [<a
target="_blank" rel="noopener" href="https://www.fletcher.io/p/the-lean-startup-is-dead">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>A key part of being a lifelong learner is retaining what you are
learning and comparing ideas and putting learning into our
lives.</em></p>
<p><em>I choose a certain number of topics/books that I want to
read/learn each year and focus on reading those books
deliberately.</em></p>
<p><em>I find reading to be a more positive habit than scrolling
mindlessly on my phone or watching YouTube videos. I do those things as
well but I try to change my habits by choosing books instead. I also
read multiple books at a time. This helps me avoid feeling the dread of
picking up a challenging or long book when I am tired after a long
day.</em></p>
<p><em>I have tried different retention techniques over the years, and
have found these to work best for me. At first, these were slower and
felt less efficient, but I have gotten faster and better at utilizing
these tips with practice.</em></p>
<p><strong>― How To Remember What You Read - Ryan Hall, Read and Think
Deeply</strong> [<a
target="_blank" rel="noopener" href="https://ryandhall.substack.com/p/how-to-remember-what-you-read">Link</a>]</p>
</blockquote>
<p>Ryan Hall's top five tips for retaining more of what you are
reading:</p>
<ul>
<li><p><strong>Underline or highlight key ideas or phrases</strong>.</p>
<ul>
<li>When reading deeply, always have a pen or highlighter in hand.</li>
<li>On the first read-through, underline or highlight any key concepts,
ideas, characters, or quotes.</li>
</ul>
<p>This practice makes the reader interactive with the text and enables
quick review of key concepts after reading. Reviewing these key ideas
after finishing a chapter is helpful and increases focus as you actively
look for points to underline.</p></li>
<li><p><strong>Write in books</strong>.</p>
<ul>
<li>As you read and underline, write notes in the margins. These notes
can include key ideas, questions, or indications if you don't understand
a section or disagree with something.</li>
<li>Notes are often single words or short phrases, like "Habit Stacking"
when reading <em>Atomic Habits</em>. These words stand out when you
revisit a section or chapter, keeping your mind engaged.</li>
<li>For digital readers (like on a Kindle), keep a notes app open on
your phone to jot down words or phrases related to the chapter. (A
separate source comment also notes that Kindles allow unlimited marginal
notes without needing a separate app).</li>
</ul></li>
<li><p><strong>Briefly summarize each section or chapter immediately
after you have read it</strong>.</p>
<ul>
<li>Keep a notebook for reading notes, where you can write the date,
book title, and chapter. Highlighting different books with different
colors can help distinguish ideas from various books.</li>
<li>Immediately after finishing a chapter or section, briefly summarize
it in your own words, keeping it short (1-3 sentences). Putting ideas
into your own words helps formulate thoughts and allows you to test your
understanding of the concepts.</li>
</ul></li>
<li><p><strong>Talk to others or teach someone else</strong>.</p>
<ul>
<li>Tell someone else about what you are reading and learning. This
verbal processing forces your mind to recall what you have read and put
the pieces together, leading to greater retention.</li>
</ul></li>
<li><p><strong>Write reviews or summaries</strong>.</p>
<ul>
<li>After finishing a book, write a review or a summary. It doesn't need
to be elaborate; the goal is to start the process of putting thoughts on
paper or keyboard to let your mind work through what you've learned. Try
to recall key plot points, ideas, and quotes, referencing your notebook
notes and margin annotations.</li>
<li>Summarize what you've read and ideas you'd like to incorporate into
your life. For nonfiction, try to apply one idea into your life. Another
comment also suggests writing a summary paragraph of each chapter and
then summarizing those in a review.</li>
</ul></li>
</ul>
<p>Additionally, bonus tips:</p>
<ul>
<li><strong>Re-read classic or deeper non-fiction books</strong>, as
they are often meant to be revisited and "wrestled with".</li>
<li><strong>Listen to podcasts or interviews with the author</strong>
(especially for nonfiction) after reading the book, as authors may
provide more context or better explanations in an interview format.</li>
</ul>
<blockquote>
<p><strong>Write Everything Down (and not in your notes app) - Megan,
Typewriter Time</strong> [<a
target="_blank" rel="noopener" href="https://moooshh.substack.com/p/write-everything-down-and-not-in">Link</a>]</p>
</blockquote>
<p>The author found that digital notes were easily forgotten and lacked
the tangible connection and memory associated with handwriting. By
shifting to a dedicated creative writing notebook, the author
experienced improved recall, a more thoughtful writing process, and a
stronger connection to their ideas and progress. The piece advocates for
the benefits of physical writing for creative endeavors and personal
reflection, highlighting how it fosters a deeper engagement with one's
own thoughts and creations, a sentiment echoed by the included
comments.</p>
<p><strong>Suggestions</strong>:</p>
<ul>
<li>Switch to handwriting everything in notebooks instead of using your
phone.</li>
<li>Use a dedicated notebook for creative writing only.</li>
<li>Write down ideas and pieces by hand.</li>
<li>Constantly flip back through the pages of your physical
notebook.</li>
<li>Write out observations about your growth and areas for improvement
directly within the same notebook.</li>
<li>Create an index in your notebook so you can find things easily.</li>
<li>Tab pages of importance.</li>
<li>Scratch out things when you're stuck or frustrated. This allows for
a "messy and alive" notebook that reflects the organic nature of the
creative process, unlike the clean digital interface.</li>
</ul>
<blockquote>
<p><strong>Brookfield: Undervalued Giant In An Overvalued Market! -
Capitalist Letters</strong> [<a
target="_blank" rel="noopener" href="https://www.capitalist-letters.com/p/brookfield-undervalued-giant-in-an">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Context Engineering: Bringing Engineering Discipline to
Prompts - Addy Osmani, Elevate</strong> [<a
target="_blank" rel="noopener" href="https://addyo.substack.com/p/context-engineering-bringing-engineering">Link</a>]</p>
</blockquote>
<p>Context engineering components:</p>
<ul>
<li><strong>Systemic Approach:</strong> It's a system, not a one-off
prompt, where the final prompt is woven together programmatically from
multiple components (e.g., role instruction, user query, fetched data,
examples).<br />
</li>
<li><strong>Dynamic and Situation-Specific:</strong> Context assembly
happens per request, adapting to the query or conversation state. This
involves including different information depending on the situation,
such as a summary of a multi-turn conversation or a relevant excerpt
from a document.<br />
</li>
<li><strong>Blending Multiple Content Types:</strong> It covers
instructional context (prompts, guidance, examples), knowledge context
(domain information, facts via retrieval), and tools context
(information from tool outputs like web searches or database
queries).<br />
</li>
<li><strong>Format and Clarity:</strong> It's about <em>how</em>
information is presented, not just <em>what</em> is included. This means
compressing and structuring information for the model's comprehension,
using formatting like bullet points, headings, JSON, or pseudo-code, and
labeling sections (e.g., "Relevant documentation:").</li>
</ul>
<blockquote>
<p><strong>You can learn anything in 2 weeks - Dan Koe, Puture /
Proof</strong> [<a
target="_blank" rel="noopener" href="https://letters.thedankoe.com/p/you-can-learn-anything-in-2-weeks">Link</a>]</p>
</blockquote>
<ul>
<li>"skill acquisition = technique stacking." Instead of trying to learn
an entire skill (like playing the guitar or Photoshop), you should focus
on specific techniques needed for a direct purpose.</li>
<li>"pure focus" as the missing ingredient for rapid learning. To
achieve this, he suggests "tactical stress" – putting yourself in a
high-pressure situation with a strong deadline that forces you to learn
quickly to avoid negative consequences. This pain of the current
situation outweighs the pain of learning, propelling you forward.</li>
</ul>
<blockquote>
<p><strong>How to instantly be better at things - Cate Hall, Useful
Fictions</strong> [<a
target="_blank" rel="noopener" href="https://usefulfictions.substack.com/p/how-to-instantly-be-better-at-anything">Link</a>]</p>
</blockquote>
<p><strong>Suggestions</strong>:</p>
<ul>
<li>Mimic others, especially those better than you.</li>
<li>Simulate the thinking of experts: Even without direct observation of
someone's thoughts, you can improve by asking yourself "what would a
better [chess player/person/etc.] do?"</li>
<li>Mimic generally competent individuals for new tasks</li>
<li>Ignore existing standards and aspire to a higher level: Recognize
that many skills are "pre-competitive," meaning current standards don't
reflect the full potential. Aim to be better than anyone you've ever
seen, rather than just slightly better than those around you. This
involves a commitment to rigorous effort and exploration beyond
perceived limits.</li>
</ul>
<blockquote>
<p><strong>Cultivating a state of mind where new ideas are born - Henrik
Karlsson, Escaping Flatland</strong> [<a
target="_blank" rel="noopener" href="https://www.henrikkarlsson.xyz/p/good-ideas">Link</a>]</p>
</blockquote>
<p><strong>Techniques to maintain the creative state</strong></p>
<ul>
<li>Ritualistic work habits: Establishing consistent routines for
creative work (e.g., daily writing sessions at a specific time and
place) can induce a state akin to self-hypnosis, fostering a
non-judgmental zone.</li>
<li>Delaying exposure: Introducing a long delay between creation and
public presentation can reduce self-censorship, as the creator feels
detached from immediate judgment.</li>
<li>Viewing work in religious terms: Framing the creative process as a
service to a higher power can provide the necessary awe and daring to
push into the unknown.</li>
<li>Strategic collaboration: Working with supportive, open-minded
collaborators who challenge rather than conform can be beneficial.</li>
<li>Subverting expectations: Actively seeking out ideas or approaches
that feel slightly uncomfortable or that one might be "ashamed of
liking" can lead to truly original work.</li>
<li>Working at speed: Forcing oneself to produce work rapidly can bypass
self-censorship and allow raw, unfiltered ideas to emerge.</li>
</ul>
<blockquote>
<p><strong>Where do Tech Returns Come From? - Eric Flaningam, Generative
Value</strong> [<a
target="_blank" rel="noopener" href="https://www.generativevalue.com/p/where-do-tech-returns-come-from">Link</a>]</p>
</blockquote>
<p>The article suggests that successful technology investing requires
embracing uncertainty, understanding the "base rates" of different
company categories, recognizing where true differentiation lies (often
beyond just technology), viewing market size from a first-principles
perspective, and being aware of the unique opportunities unlocked by new
technology waves.</p>
<ol type="1">
<li><strong>The next <span class="math inline">\(\$100B\)</span> company
will not look like the last:</strong> Value in technology is driven by
"anomalous" companies founded by "anomalous" people, making pattern
matching ineffective. The most successful companies create new
categories.</li>
<li><strong>Know the game you’re playing:</strong> Different categories
have different "Slugging Ratios" (Value/Company). Consumer companies,
often network-driven marketplaces with winner-take-all dynamics, have
the highest upside, while Hardtech companies also have high slugging
ratios but are riskier. Enterprise software, while less "Power Lawed,"
offers more predictable returns and is suitable for an expanding venture
capital landscape due to its scalability, moats, and lower operating
costs.</li>
<li><strong>Software is like chicken, 80% of it tastes the
same:</strong> Technical differentiation in enterprise software is often
nuanced. Sales, marketing, and building "mindshare" are as, if not more,
important than technical moats, especially as software becomes easier to
build and features are quickly replicated. The "GPT Wrapper" argument
for AI applications is analogous to how many successful enterprise
software companies were essentially "database wrappers."</li>
<li><strong>“Market size” may be the single greatest reason for
investors missing great companies:</strong> Humans struggle with
uncertainty, and new markets introduce exactly that. Many successful
companies like Palantir, Shopify, and Uber created new markets that
didn't exist before, leading to investors underestimating their
potential market size. Companies with "multiple-expansion tailwinds" and
strong platforms also tend to be underestimated.</li>
<li><strong>Companies resemble the technology waves they ride in
on:</strong> New technology waves (internet, mobile, cloud, AI) unlock
the ability for new businesses to exist. AI, for example, is enabling
anyone to create software and automate voice/text-based workflows,
expanding the market significantly and allowing for the creation of
entirely new categories (e.g., legal software companies like Harvey
reaching <span class="math inline">\(\$5B\)</span>+ valuations
quickly).</li>
<li><strong>Don't underestimate the Power Law, ever:</strong> While
mentioned throughout, this point emphasizes the extreme concentration of
value in a very small number of companies. The article states that the
top seven companies in its dataset accounted for nearly 50% of the <span
class="math inline">\(\$13\)</span> trillion in value creation.</li>
</ol>
<blockquote>
<p><strong>The Great Mental Models: Visual Book Summary -
DoubleThink</strong> [<a
target="_blank" rel="noopener" href="https://dblthink.substack.com/p/the-great-mental-models-visual-book">Link</a>]</p>
</blockquote>
<p><strong>The Map is Not The Territory:</strong> This model highlights
that maps (including mental models) simplify reality and are imperfect
reductions of what they represent. While useful, they lack perfect
fidelity and should be used carefully, as the real world is complex.</p>
<p><strong>Circle of Competence:</strong> Emphasizes the importance of
knowing what you know and, critically, what you don't know. It's
dangerous to incorrectly assume knowledge. The advice is to operate
within your area of expertise and outsource the rest.</p>
<p><strong>Falsifiability:</strong> States that for a theory to be
confirmed, it must be challengeable. Instead of trying to prove a theory
correct, one should try to prove it incorrect. A theory becomes stronger
when rigorous experimentation fails to disprove it.</p>
<p><strong>First Principles Thinking:</strong> Involves breaking down a
problem into its fundamental, non-reducible parts to challenge
pre-existing assumptions. It's an effective way to clarify and approach
complex problems by building solutions from the bottom up, often using
techniques like Socratic Questioning and the Five Whys.</p>
<p><strong>Thought Experiment:</strong> Refers to mentally simulating
situations to test theories or reach conclusions, rather than conducting
physical experiments. It allows for gaining confidence in answers, as
illustrated by comparing hypothetical basketball games.</p>
<p><strong>Necessity and Sufficiency:</strong> Explains that having all
necessary conditions does not guarantee all sufficient conditions are
met. Meeting necessary conditions might make success possible, but it
doesn't assure it (e.g., knowing how to write vs. being a New York Times
Bestseller).</p>
<p><strong>Second-Order Thinking:</strong> Encourages looking beyond
immediate consequences to consider the "consequence of the consequence"
or further. It involves thinking several steps ahead to avoid short-term
positive decisions that lead to long-term negative effects.</p>
<p><strong>Probabilistic Thinking</strong>: Acknowledges that the future
cannot be predicted perfectly, but this model helps improve the accuracy
of guesses using three main concepts:</p>
<ul>
<li><strong>Bayesian Thinking:</strong> Using all relevant prior
information for informed decisions in unfamiliar scenarios.</li>
<li><strong>Fat-Tailed Curves:</strong> Understanding that the more
extreme scenarios are possible, the higher the likelihood of any one of
them occurring.</li>
<li><strong>Asymmetries:</strong> Assessing the probability that your
estimates accurately reflect the real world.</li>
</ul>
<p><strong>Correlation vs. Causation:</strong> Highlights that a
correlation between two things does not necessarily mean one causes the
other. Large datasets can yield strong correlations purely by chance, as
demonstrated by the unrelated alignment of Walgreens customer
satisfaction and Russell Crowe's movie appearances.</p>
<p><strong>Inversion Principle:</strong> A thinking tool that involves
approaching a situation from the opposite end of the usual starting
point to reframe a problem into a solution (e.g., "make money" becomes
"avoid going into debt").</p>
<p><strong>Hanlon’s Razor:</strong> Suggests that one should "never
attribute to malice that which can be adequately explained by
stupidity." It implies that actions that seem ill-intended are often
accidents or misunderstandings, and the explanation assuming the least
intent is most likely correct.</p>
<p><strong>Occam’s Razor:</strong> States that when multiple
explanations are possible, the one that makes the fewest assumptions is
generally the most probable and closest to the truth. In essence, the
simplest explanation is usually the correct one.</p>
<blockquote>
<p><strong>Amazon: Betting The Farm - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/amazon-betting-the-farm">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Tesla: From Bad to Worse - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/tesla-from-bad-to-worse">Link</a>]</p>
</blockquote>
<p>On the <strong>EV Landscape</strong> (specifically Tesla's automotive
business):</p>
<ul>
<li>The author highlights that Tesla's year is going "from bad to
worse". Global deliveries have fallen by 13%, marking their steepest
quarterly drop ever.</li>
<li>Tesla's revenue is declining, margins are compressing, and cash flow
has dried out. Automotive revenue specifically fell by 16%
year-over-year. The author notes that "Q2 results remain very poor" if
Tesla is viewed purely as an auto business.</li>
<li>Historically strong margins, supported by gigafactory scale,
direct-to-consumer sales, and minimal marketing costs, are now being
eroded by price cuts and rising competition.</li>
<li>A return to growth, which was predicted earlier in the year, now
"looks unlikely". The company has also withheld full-year guidance due
to factors like trade policy and political backlash, adding to the
uncertainty.</li>
<li>The author expresses concern about "mounting evidence of brand
erosion".</li>
</ul>
<p>On the <strong>AV Landscape</strong> (specifically Tesla's Robotaxi
program):</p>
<ul>
<li>The author acknowledges that Tesla's robotaxi program "could unlock
tremendous value". Elon Musk himself emphasizes that "autonomy is the
story" for Tesla.</li>
<li>Despite its significant potential, the author cautions that even if
these "moonshots in robotaxi and robotics succeed," they are "years away
from offsetting collapsing vehicle demand". This indicates that the
robotaxi program is not seen as an immediate solution to Tesla's current
financial woes in its automotive segment.</li>
<li>The robotaxi program faces considerable regulatory hurdles.</li>
<li>The author raises a critical question about whether the ongoing
"brand erosion" could "undermine even the most ambitious upside" of the
robotaxi program.</li>
<li>The author foresees an "upcoming robotaxi war" among Big Tech
companies, suggesting a highly competitive environment for autonomous
vehicles.</li>
</ul>
<blockquote>
<p><strong>Microsoft: AI Crossroads - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/microsoft-ai-crossroads">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Figma Files for IPO - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/figma-files-for-ipo">Link</a>]</p>
</blockquote>
<p><strong>Figma's Current State and Potential:</strong></p>
<ul>
<li>The author asserts that Figma is "not just a great product—it’s a
great business" and describes its growth since monetizing in 2017 as
"one of the most explosive runs in SaaS history".</li>
<li>Figma achieved <strong>$ $749$ million in FY24 revenue, up 48%
year-over-year</strong>, with <strong>over 1,000 customers paying <span
class="math inline">\(\$100\)</span>K+ annually</strong>, and
<strong>95% of Fortune 500 companies using Figma</strong>. The author
considers its product-led, freemium "Land and Expand" growth model to be
"hard to manufacture—and even harder to replicate".</li>
<li>The author highlights Figma's transformation "from a design tool
into a full-stack product platform". It is "evolving into a full product
development suite", with new tools like FigJam, Dev Mode, and Figma Make
(AI-driven prototyping) expanding its reach across the entire product
lifecycle.</li>
<li>Figma is positioned as a "productivity platform disguised as a
design tool", which, in the author's view, separates it "from legacy
tools and what opens the door to much broader enterprise budgets" by
serving designers, engineers, product managers, marketers, and
executives.</li>
<li>Figma's "web-first" and "multiplayer by default" approach gave it a
"distinct edge over incumbents like Adobe".</li>
</ul>
<p><strong>Figma's Uncertainty and Challenges:</strong></p>
<ul>
<li>The author notes the <strong>collapse of the <span
class="math inline">\(\$20\)</span> billion Adobe acquisition due to
"antitrust concerns in the US, UK, and Europe"</strong>. While Figma
received a "<span class="math inline">\(\$1\)</span> billion breakup
fee" and regained independence, this past scrutiny highlights a
challenging regulatory environment that companies of Figma's scale can
face.</li>
<li>The author points out a significant "catch" in Figma's reported net
dollar retention of 132% in Q1 FY25. They state that the metric "only
includes customers still spending over <span
class="math inline">\(\$10,000\)</span> today, then looks back at what
those same customers were spending a year ago." This indicates a
potential lack of clarity or transparency in how a key growth metric is
presented, which could be an uncertainty for investors trying to assess
actual customer retention.</li>
</ul>
<h2 id="articles-and-blogs">Articles and Blogs</h2>
<blockquote>
<p><strong>How we built our multi-agent research system -
Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/engineering/built-multi-agent-research-system">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Building the Hugging Face MCP Server - Hugging Face</strong>
[<a target="_blank" rel="noopener" href="https://huggingface.co/blog/building-hf-mcp">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Claude for Financial Services - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/news/claude-for-financial-services">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Cursor on Web and Mobile - Cursor</strong> [<a
target="_blank" rel="noopener" href="https://cursor.com/en/blog/agent-web">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How to Build an MCP Server in 5 Lines of Python - Hugging
Face</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/blog/gradio-mcp">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Claude Code in Action - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://anthropic.skilljar.com/claude-code-in-action">Link</a>]</p>
</blockquote>
<p>This is a 10-lesson guide covering GitHub automation, custom
workflows, and MCP integration. Teaches you how to use Claude Code to
automate dev tasks in 36 minutes.</p>
<h2 id="papers-and-reports">Papers and Reports</h2>
<blockquote>
<p><strong>A Survey of Context Engineering for Large Language
Models</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13334">Link</a>]</p>
</blockquote>
<h2 id="youtube-and-podcasts">YouTube and Podcasts</h2>
<blockquote>
<p><strong>Grok 4 Wows, The Bitter Lesson, Elon's Third Party, AI
Browsers, SCOTUS backs POTUS on RIFs - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vFgjDSySYo8&amp;list=WL&amp;index=24&amp;pp=gAQBiAQB">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Trump vs Powell, Solving the Debt Crisis, The $10T AGI Prize,
GENIUS Act Becomes Law - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=wu-p5xrJ8-E&amp;list=WL&amp;index=5&amp;t=2480s&amp;pp=gAQBiAQB">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Silicon Valley Insider EXPOSES Cult-Like AI Companies | Aaron
Bastani Meets Karen Hao</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=8enXRDlWguU&amp;list=WL&amp;index=29&amp;pp=gAQBiAQB">Link</a>]</p>
</blockquote>
<p>Karen Hao, an expert in mechanical engineering and journalism,
provides a comprehensive critique of the A industry, detailing her
opinions, arguments, and proposals across various topics during the
interview.</p>
<ul>
<li><p><strong>Understanding AI and its Definition</strong></p>
<ul>
<li>Hao argues that the term "artificial intelligence" is poorly defined
and was originally coined in 1956 by John McCarthy to attract more
attention and funding for his research, essentially as a marketing term.
She notes that while AI generally refers to recreating human
intelligence in computers, there is no scientific consensus on what
human intelligence is, contributing to the term's ambiguity.</li>
<li>AI serves as an "umbrella" term encompassing various technologies
that simulate human behaviors or tasks, ranging from Siri to ChatGPT,
which operate on vastly different scales and have different use cases.
Hao uses the analogy that AI is like the word "transportation" to
illustrate its vagueness: just as "transportation" can refer to bicycles
or rockets, AI can refer to vastly different technologies with different
purposes and costs. She finds it frustrating and unproductive when
politicians use the term vaguely, suggesting it means "progress" without
specifying the type of AI or its potential costs, which she compares to
promoting rockets for commuting when more efficient alternatives
exist.</li>
</ul></li>
<li><p><strong>Environmental and Public Health Costs of AI
Development</strong></p>
<ul>
<li>Hao emphasizes that the resource consumption required to develop and
use generative AI models is quite extraordinary. She cites a McKinsey
report projecting that within the next five years, current data center
and supercomputer expansion for AI will require adding around half to
1.2 times the amount of energy consumed in the UK annually to the global
grid. A significant portion of this energy will be serviced by fossil
fuels, including natural gas and the extended lives of coal plants.</li>
<li>Hao highlights that this acceleration not only impacts the climate
crisis but also exacerbates public health crises, citing Elon Musk's
xAI's Colossus in Memphis, Tennessee, which is powered by 35 unlicensed
methane gas turbines pumping toxic air pollutants into the community.
She argues that "unlicensed" means the company completely ignored
existing environmental regulations.</li>
<li>She stresses the undertalked about issue of water consumption: AI
data centers require fresh, potable water for cooling to prevent
corrosion and bacterial growth, often using public drinking water
infrastructure. She notes that two-thirds of new AI data centers are
being built in water-scarce areas, providing the example of Montevideo,
Uruguay, where Google proposed a data center during a historic
drought.</li>
</ul></li>
<li><p><strong>The Business Case and Ideology Driving AI</strong></p>
<ul>
<li>Hao contends that the business case for AI is currently unclear,
noting that even Microsoft has started pulling back investments in data
centers and its CEO, Satya Nadella, has expressed skepticism about the
"race to AGI". She argues that what drives the fervor in the absence of
a clear business case is an ideology or a "quasi-religious fervor".
People genuinely believe in the ability to fundamentally recreate human
intelligence, seeing it as the most important civilizational goal.</li>
<li>She explains that this ideological drive from startups like OpenAI
and Anthropic pressures larger, more traditional tech giants to invest
heavily, as shareholders demand an AI strategy, often due to consumer
shifts (like using ChatGPT as search). Hao explains that OpenAI's pitch
to investors is that funding could lead to being the first to AGI for
"the biggest returns you've ever seen" or, failing that, could automate
human tasks to replace labor, generating significant returns.</li>
<li>She warns of a "bandwagon mentality" among investors. Crucially, she
highlights that if the AI bubble pops, the risk is not just for Silicon
Valley but will have ripple effects across the global economy, as
investments often come from public endowments.</li>
</ul></li>
<li><p><strong>OpenAI's Origins and Sam Altman's Leadership</strong></p>
<ul>
<li>Hao reveals that OpenAI started as a nonprofit in late 2015,
co-founded by Elon Musk and Sam Altman, as an "anti-Google" initiative
to conduct fundamental AI research without commercial pressures. Musk
specifically feared Google's DeepMind could lead to AI going "very badly
wrong" (sentience, harming humans). The original "open" in OpenAI stood
for open source, and for its first year, the company genuinely
open-sourced its code and research. Hao speculates that the nonprofit
status was a recruitment tool to attract talent, as they couldn't
compete with Google's salaries but could offer a compelling sense of
mission. However, within less than a year, the bottleneck shifted from
talent to capital, leading to the decision to convert to a for-profit
entity. This shift also led to a falling out between Musk and Altman
over who would be CEO.</li>
<li>Regarding Sam Altman, Hao portrays him as a "master manipulator" and
"understander of human psychology". She notes that Altman was not
publicly well-known but was a critical "lynchpin" within the tech
industry, having cultivated relationships with powerful networks and
policymakers early in his career as president of Y Combinator. Hao
states that people who worked with Altman consistently told her they
didn't know what he truly believed because he would often say he
believed what the person he was talking to believed, even if those
beliefs were diametrically opposed. She concludes that Altman's
comparative advantages as a leader include his ability to persuade
people to join his "quest," acquire necessary resources (capital, land,
energy, water, laws), and instill a powerful sense of belief in his
vision among his team. She describes his work as being most effective in
one-on-one meetings where he can tailor his message to achieve his
goals.</li>
</ul></li>
<li><p><strong>Critique of Big Tech as a Corporate Empire</strong></p>
<ul>
<li>Hao argues that if allowed to expand unfettered, these corporate
empires will ultimately erode democracy. Hao states that tech leaders
view the rest of the world, including other Western countries, as
"resources"—territories from which to acquire land, labor, minerals,
energy, and water for their data centers. She highlights that data
center expansion often targets economically vulnerable communities in
rural areas of the US and UK, which are often uninformed about the true
costs, such as bans on new housing construction due to massive
electricity consumption, or the depletion of fresh water supplies. Hao
laments that politicians are often unaware of these negative
consequences.</li>
<li>She argues that the idea that "you need colossal data centers to
build AI systems" is a "false trade-off". Before OpenAI, AI research was
trending towards "tiny AI systems" requiring little computational
resources, showing that AI innovation can occur without these massive,
resource-intensive approaches. Hao points out that most AI experts today
are employed by these companies, which she likens to climate scientists
being bankrolled by oil and gas companies, leading to biased information
that serves the company's interests rather than scientific
grounding.</li>
</ul></li>
<li><p><strong>Exploitative Labor Practices</strong></p>
<ul>
<li>Hao exposes grueling exploitative practices in the global AI supply
chain, particularly regarding content moderation for OpenAI. Kenyan
workers were contracted to sift through "reams of the worst text on the
internet," including child sexual abuse, hate speech, and violent
content, to build content moderation filters for ChatGPT. She details
how this work traumatized workers, causing PTSD, personality changes,
and family breakdowns, like the story of Moffat, whose family left him
due to his changed demeanor. These workers were paid only a few dollars
an hour.</li>
<li>She also discusses data annotation, a long-standing part of the AI
industry. Venezuelan refugees in Colombia, highly educated but desperate
due to their country's economic crisis, became cheap labor for labeling
data for self-driving cars and retail platforms. Hao describes the
structural exploitation where workers compete for tasks on platforms,
leading to immense anxiety and control over their lives, exemplified by
a woman who wouldn't walk outside during weekdays for fear of missing
tasks and would wake up at 3 AM if an alarm signaled a new task. She
asserts that there is no moral justification for why these workers,
whose contributions are critical, are paid pennies while company
insiders receive multi-million dollar compensation packages; the only
"justification" is an ideological one that some people are
superior.</li>
</ul></li>
<li><p><strong>Proposals for Public Action and Shaping the Future of
AI</strong></p>
<ul>
<li>Hao believes that anyone in the world can take action to shape the
AI development trajectory. She proposes thinking of AI development as a
"full supply chain of AI development", where various resources (data,
land, energy, water) and deployment spaces (schools, hospitals, offices)
are points of democratic contestation.</li>
</ul></li>
<li><p>She suggests the public can reclaim ownership over resources. She
encourages people to contest the spaces where AI is deployed. Hao also
advises people to research AI technologies and vendors to make informed
choices about which AI systems to use.</p></li>
<li><p>She expresses optimism that widespread, democratic contestation
at every stage of the AI development and deployment pipeline can
"reverse the imperial conquest of these companies" and lead to a more
broadly beneficial trajectory for AI.</p></li>
</ul>
<blockquote>
<p><strong>How to Gamify Your Life (And Reinvent Yourself ... Fast) -
Dan Koe</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=8enXRDlWguU&amp;list=WL&amp;index=29&amp;pp=gAQBiAQB">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The Future of Work (How to Become AI-First) - Dan
Koe</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vFgjDSySYo8&amp;list=WL&amp;index=24&amp;pp=gAQBiAQB">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Winning the AI Race Part 1: Michael Kratsios, Kelly Loeffler,
Shyam Sankar, Chris Power</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Nmw43vcSFhw&amp;list=WL&amp;index=11&amp;pp=gAQBiAQB">Link</a>]</p>
<p><strong>Winning the AI Race Part 2: Vice President JD Vance</strong>
[<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=8qT2SAJI_HQ&amp;list=WL&amp;index=14&amp;t=452s&amp;pp=gAQBiAQB">Link</a>]</p>
<p><strong>Winning the AI Race Part 3: Jensen Huang, Lisa Su, James
Litinsky, Chase Lochmiller</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9WkGNe27r_Q&amp;list=WL&amp;index=16&amp;pp=gAQBiAQB">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Turbo-Scaling GenAI at DoorDash: From Product Knowledge Graph
to Real-Time Personalization - Predibase</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=DJewWjXmHIQ">Link</a>]</p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/di-blog/tags/readings/" rel="tag"># readings</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/di-blog/2025/06/15/Zero-to-One/" rel="prev" title="Zero to One">
                  <i class="fa fa-angle-left"></i> Zero to One
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/di-blog/2025/08/21/2025-August/" rel="next" title="2025 August - What I Have Read">
                  2025 August - What I Have Read <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Di Zhen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/di-blog/js/comments.js"></script><script src="/di-blog/js/utils.js"></script><script src="/di-blog/js/motion.js"></script><script src="/di-blog/js/sidebar.js"></script><script src="/di-blog/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/di-blog/js/third-party/math/mathjax.js"></script>



</body>
</html>
