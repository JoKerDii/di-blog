<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/di-blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/di-blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/di-blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/di-blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/di-blog/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jokerdii.github.io","root":"/di-blog/","images":"/di-blog/images","scheme":"Muse","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/di-blog/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Di&#39;s Blog">
<meta property="og:url" content="https://jokerdii.github.io/di-blog/page/2/index.html">
<meta property="og:site_name" content="Di&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Di Zhen">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jokerdii.github.io/di-blog/page/2/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Di's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/di-blog/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/di-blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Di's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Di Zhen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/di-blog/archives/">
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/06/15/Zero-to-One/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/06/15/Zero-to-One/" class="post-title-link" itemprop="url">Zero to One</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-06-15 17:54:16" itemprop="dateCreated datePublished" datetime="2025-06-15T17:54:16-04:00">2025-06-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure>
<img src="/di-blog/2025/06/15/Zero-to-One/zero_to_one.jpg"
alt="zero_to_one" />
<figcaption aria-hidden="true">zero_to_one</figcaption>
</figure>
<p>I finished reading Peter Thiel's '<em>Zero to One: Notes on Startups,
or How to Build the Future</em>' today. With the rapid advancements and
widespread discussion around AI, the core arguments about technology,
human-machine collaboration, and the nature of progress hold up
remarkably well. And in some ways, as a manifesto for building a better
future, what's written in this book is even more relevant now.</p>
<p><strong>Chapter 2 Party Like It's 1999</strong> outlines four lessons
learned from the dot-com crash that became 'dogma' in the startup world,
however, Thiel argues that these dogmas are largely incorrect and that
the opposite principles are probably more correct:</p>
<ol type="1">
<li><p><strong>Make incremental advances:</strong> Grand visions were
seen as bubble-inflating, so small, incremental steps became the
preferred path.</p>
<p>Thiel: It is better to risk boldness than triviality.</p></li>
<li><p><strong>Stay lean and flexible:</strong> Planning was deemed
arrogant, and "agnostic experimentation" became the norm.</p>
<p>Thiel: A bad plan is better than no plan.</p></li>
<li><p><strong>Improve on the competition:</strong> Focus on existing
customers and recognizable products, improving on what competitors
already offer.</p>
<p>Thiel: Competitive markets destroy profits.</p></li>
<li><p><strong>Focus on product, not sales:</strong> If a product
requires advertising or salespeople, it's not good enough; viral growth
is the only sustainable growth.</p>
<p>Thiel: Sales matters just as much as product.</p></li>
</ol>
<blockquote>
<p>"The most contrarian thing of all is not to oppose the crowd, but to
think for yourself."</p>
</blockquote>
<p>In this end this chapter, Thiel is challenging the reader to not
simply adopt the prevailing "lessons learned" from the past, but to
critically evaluate them. He suggests that true contrarianism isn't just
about disagreeing with the majority for the sake of it, but about
independent thought and forming your own conclusions, even if those
conclusions align with or contradict the crowd. It's about genuine
intellectual autonomy.</p>
<p>In <strong>Chapter 3 All Happy Companies Are Different</strong>, he
argues that successful companies are unique and that true value comes
from creating a monopoly rather than competing in existing markets.
Thiel uses the economic models of "perfect competition" and "monopoly"
to explain this difference. In perfect competition, firms sell identical
products, have no market power, and thus, in the long run, make no
economic profit as new entrants drive prices down. A monopoly,
conversely, owns its market, allowing it to set prices and maximize
profits due to a lack of close substitutes. He asserts that competition
is destructive, leading to a ruthless struggle for survival and zero
profits. Monopolies, on the other hand, can afford to focus on long-term
innovation, employee well-being, and broader societal impact because
they are not constantly battling for survival. Creative monopolies are
powerful engines for progress as they introduce entirely new categories
of abundance to the world.</p>
<p>He then discusses how both monopolists and non-monopolists tend to
misrepresent their market conditions. Monopolists (like Google) downplay
their dominance by broadly defining their market to avoid scrutiny,
while non-monopolists (like a new restaurant owner) narrowly define
their market to appear unique and avoid acknowledging intense
competition. Thiel emphasizes that losing sight of competitive reality
by focusing on trivial differentiators is a fatal mistake for
startups.</p>
<blockquote>
<p>"All happy companies are different: each one earns a monopoly by
solving a unique problem. All failed companies are the same: they failed
to escape competition."</p>
</blockquote>
<p>This is the core message from the book. Entrepreneurs should strive
to build unique, monopolistic businesses by creating something entirely
new.</p>
<p>Chapter 3 primarily focuses on the economic and strategic advantages
of monopoly and the destructive nature of perfect competition.
<strong>Chapter 4: "The Ideology of Competition"</strong> shifts the
focus to the societal and psychological impact of competition.
Competition is not merely an economic concept but a deeply ingrained
"ideology" that pervades our society, from education to personal
aspirations. He reminds readers that this competition can blind people
to real opportunities and lead to irrational behavior and missed
chances, and suggests us to recognize and resist the pervasive ideology
of competition.</p>
<p><strong>Chapter 5 Last Mover Advantage</strong> discusses how a great
business is defined by its ability to generate future cash flows and
argues that being a last mover (i.e., to make the last great development
in a market and enjoy long-term monopoly profits) is more advantageous
than being a first mover. It outlines four characteristics of monopoly
that contribute to a company's durability:</p>
<ol type="1">
<li><p><strong>Proprietary Technology:</strong> This makes a product
difficult to replicate, ideally being at least 10 times better than its
closest substitute (e.g., Google's search algorithms, PayPal's payment
system for eBay, Amazon's book selection, Apple's integrated
design).</p></li>
<li><p><strong>Network Effects:</strong> The product becomes more
valuable as more people use it (e.g., Facebook). Thiel emphasizes that
such businesses must start with a very small, focused market to get
initial users.</p></li>
<li><p><strong>Economies of Scale:</strong> Fixed costs can be spread
over increasing sales, making the business stronger as it grows.
Software companies are particularly suited for this due to near-zero
marginal costs.</p></li>
<li><p><strong>Branding:</strong> A strong brand creates a monopoly
(e.g., Apple). However, branding needs to be built on substantive
advantages, not just surface-level polish.</p></li>
</ol>
<blockquote>
<p>"You've probably heard about 'first mover advantage': if you're the
first entrant into a market, you can capture significant market share
while competitors scramble to get started. But moving first is a tactic,
not a goal. What really matters is generating cash flows in the future,
so being the first mover doesn't do you any good if someone else comes
along and unseats you. It's much better to be the last mover— that is,
to make the last great development in a specific market and enjoy years
or even decades of monopoly profits. The way to do that is to dominate a
small niche and scale up from there, toward your ambitious long-term
vision. In this one particular at least, business is like chess.
Grandmaster José Raúl Capablanca put it well: to succeed, 'you must
study the endgame before everything else.'"</p>
</blockquote>
<p>Thiel's advice for startups (1) start small and monopolize, 2) scale
up gradually, and 3) don't discrupt) reminds me of Google's AI strategy
in recent two years, which seems to align with the 'last mover
advantage' mentality. Instead of trying to release one massive,
all-encompassing AI that competes directly with established players
across every front, Google has released or integrated AI into many
"smaller" applications or features (workspace, photos, maps, gemini,
etc). Each of these can be seen as a "small market" or specific use case
where AI offers a distinct advantage, allowing Google to "monopolize"
that particular user experience. After establishing AI capabilities in
focused areas, they are integrating these more broadly. Successful AI
features in Workspace might then be leveraged for enterprise solutions.
Advancements in image recognition from Photos could be applied to
broader visual search or other AI models. The iterative development of
Bard/Gemini, starting as a conversational AI and gradually expanding its
capabilities (multimodality, coding, planning), is a clear example of
scaling up. They build upon established user bases and technological
strengths. While Google is certainly competing, their strategy doesn't
always seem to be about a direct, disruptive frontal assault that
immediately aims to destroy an incumbent. Instead, it's often about: 1)
leveraging their exsiting ecosystem, 2) focusing on unique capabilities,
3) creating new user behaviors.</p>
<p>In <strong>Chapter 6 You Are Not a Lottery Ticket</strong>, Thiel
described the concept of definite vs. indefinite futures and asserts
that the prevailing indefinite optimism, particularly in the US, is
unsustainable. He argues that real progress and success require definite
plans and individual effort.</p>
<blockquote>
<p>“When Baby Boomers grow up and write books to explain why one or
another individual is successful, they point to the power of a
particular individual's context as determined by chance. But they miss
the even bigger social context for their own preferred explanations: a
whole generation learned from childhood to overrate the power of chance
and underrate the importance of planning."</p>
</blockquote>
<p>The core of <strong>Chapter 7 Follow the Money</strong> applies the
power law to venture capital (VC). Venture returns are not normally
distributed (where most companies perform average). Instead, they follow
a power law: a small handful of companies radically outperform all
others, often returning more than the entire rest of the fund combined.
People often fail to see the power law, which is a fundamental law of
the universe, because it only becomes clear over time; early-stage
companies in a portfolio might look similar before exponential growth
kicks in. Despite being a niche (less than 1% of new businesses receive
VC funding), venture-backed companies disproportionately drive the
economy, creating 11% of private sector jobs and generating 21% of GDP.
The largest tech companies, all venture-backed, are worth more than all
other tech companies combined.</p>
<p>Understanding the power law means focusing on the singular, most
important things (e.g., one best market, one dominant distribution
strategy). To achieve disproportionate success, one must identify and
focus relentlessly on those few critical elements.</p>
<p>In <strong>Chapter 8 Secrets</strong>, Thiel begins by posing his
contrarian question ("<em>What important truth do very few people agree
with you on?</em>") in the context of secrets. He states that a good
answer to this question implies the existence of secrets – something
important, unknown, difficult, but achievable. He argues that secrets
still exist and are crucial for progress.Secrets can lead to monumental
advancements in science, medicine, and technology (e.g., curing
diseases, new energy sources). In business, secrets can lead to valuable
companies built on overlooked opportunities, like Airbnb (untapped
supply and unaddressed demand in lodging) and Uber/Lyft (connecting
drivers and riders). In terms of how to find secrets, Thiel has
discussed about 1) secrets of nature from studying physical world, vs.
secrets about people from understanding human nature, 2) looking at the
fields that matter but haven't been standardized.</p>
<p><strong>Chapter 9 Foundations</strong> is around 'Thiel's law': a
startup messed up at its foundation cannot be fixed, providing guidance
on fundamental level: co-founder relationships, ownership, possession,
and control, small boards, full time commitment, equity is the king,
founding moment, etc. <strong>Chapter 10 The Mechanics of Mafia</strong>
highlights the importance of company culture. <strong>Chapter 11 If You
Build It Will They Come</strong> stresses that distribution (sales,
marketing, advertising) is often underestimated and is just as crucial
as product development.</p>
<blockquote>
<p>"The founding moment of a company, however, really does happen just
once: only at the very start do you have the opportunity to set the
rules that will align people toward the creation of value in the
future.</p>
<p>The most valuable kind of company maintains an openness to invention
that is most characteristic of beginnings. This leads to a second, less
obvious understanding of the founding: it lasts as long as a company is
creating new things, and it ends when creation stops. If you get the
founding moment right, you can do more than create a valuable company:
you can steer its distant future toward the creation of new things
instead of the stewardship of inherited success. You might even extend
its founding indefinitely."</p>
</blockquote>
<blockquote>
<p>"'Company culture' doesn't exist apart from the company itself: no
company has a culture; every company is a culture. A startup is a team
of people on a mission, and a good culture is just what that looks like
on the inside."</p>
</blockquote>
<p>There is a core debate right now around whether AI is going to
replace human‘s jobs, and this book offers powerful arguments for the
"AI as complement, not replacement" side. Thiel explicitly argued
against the "substitution fallacy" in <strong>Chapter 12 (Man and
Machine)</strong>, stating that computers and humans have different
strengths and will thrive through collaboration. Although Generative AI
is unprecedented with its impact on human society nuanced to discuss, I
agree there are fundamental differences in intelligence between humans
and AI. Human possess intentionality, true innovation, empathy, and
emotional intelligence, and human judgment is needed when there are
ethical concerns or complex problems. AI as a tool can do augmentation
to increase productivity, but not automation. Historically speaking,
tech development always creates more jobs than destroyed. While some
roles are eliminated, new roles emerge: AI engineers, Prompt Engineers,
AI Product Managers, etc. In essence, it's about a redefinition of work,
rather than elimination.</p>
<blockquote>
<p>"People compete for jobs and for resources; computers compete for
neither."</p>
</blockquote>
<p>Globalization is about substitution. Technology is about
complementarity.</p>
<p><strong>Chapter 13 Seeing Green</strong> analyzes the failure of the
cleantech bubble, attributing it to a widespread failure to answer the
seven critical questions every successful business must address.</p>
<ol type="1">
<li><strong>The Engineering Question:</strong> Most offered only
incremental, not breakthrough (10x better), technology (e.g., Solyndra's
inefficient cylindrical solar cells).</li>
<li><strong>The Timing Question:</strong> They misjudged market
readiness and the slow, linear progress of solar technology compared to
exponential tech.</li>
<li><strong>The Monopoly Question:</strong> They pursued
"trillion-dollar markets" that were fiercely competitive, rather than
small, defensible niches.</li>
<li><strong>The People Question:</strong> Teams were often led by
"salesman-executives" lacking technical expertise, focusing on
fundraising over product. (Thiel suggests a "never invest in a tech CEO
that wears a suit" rule.)</li>
<li><strong>The Distribution Question:</strong> Companies often
overlooked effective distribution, leading to complex and inconvenient
sales models (e.g., Better Place's battery swapping).</li>
<li><strong>The Durability Question:</strong> They failed to anticipate
competition (e.g., from China) or market shifts (e.g., the rise of
fracking).</li>
<li><strong>The Secret Question:</strong> They based their ventures on
"conventional truths" (the need for a cleaner world), which everyone
agreed on, rather than unique, hidden insights.</li>
</ol>
<blockquote>
<p>"The 1990s had one big idea: the internet is going to be big. But too
many internet companies had exactly that same idea and no others. An
entrepreneur can't benefit from macroscale insight unless his own plans
begin at the micro-scale. Cleantech companies faced the same problem: no
matter how much the world needs energy, only a firm that offers a
superior solution for a specific energy problem can make money. No
sector will ever be so important that merely participating in it will be
enough to build a great company."</p>
</blockquote>
<p><strong>Chapter 14 The Founder's Paradox</strong> explores the often
extreme, contradictory, and seemingly peculiar traits of successful
founders, arguing that these unique characteristics are both powerful
for a company and carry inherent dangers for the founder. Society needs
founders – unusual individuals who can make authoritative decisions,
inspire loyalty, and plan long-term, moving companies beyond
incrementalism. However, founders must be wary of overestimating their
own power and succumbing to their own myth, mistaking public adulation
or criticism for truth. The greatest danger for a founder is losing
their mind; for a business, it's losing its myth and vision.</p>
<p>The current AI boom feels very much like an "accelerating takeoff "
in terms of technological advancement, which is mentioned in the final
chapter "<strong>Conclusion: Stagnation or Singularity</strong>", as one
of the Nick Bostrom's four possible patterns for humanity's future.
Accelerating Takeoff (Singularity) is the most difficult scenario to
imagine: new technologies so powerful that they transcend current
understanding, leading to a much better future. Ray Kurzweil's
"Singularity is near" concept, based on exponential growth trends, is
mentioned as a prominent view of this outcome. However, as Thiel's book
is a manifesto for building a better future and criticizes 'indefinite
optimism', in the context of AI boom, 'Singularity' is not a
predetermined destination, but the choices we make today:</p>
<ul>
<li><em>Are we using AI for "0 to 1" innovation to solve truly hard
problems and create new value, or are we just using it for "1 to n"
incremental improvements and fierce competition?</em></li>
<li><em>Are we making definite plans for how AI will integrate with and
enhance human capabilities, or are we succumbing to "indefinite fears"
or blind optimism?</em></li>
<li><em>Are we building companies around unique AI-driven insights that
can create sustainable monopolies, or are we simply entering crowded AI
markets hoping for a piece of an existing pie?</em></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/06/07/2025-Jun/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/06/07/2025-Jun/" class="post-title-link" itemprop="url">2025 Jun - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-06-07 16:31:07" itemprop="dateCreated datePublished" datetime="2025-06-07T16:31:07-04:00">2025-06-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="substack">Substack</h2>
<blockquote>
<p><em>The frontier isn’t volume—it’s discernment. And in that shift,
taste has become a survival skill.</em></p>
<p><em>Because when abundance is infinite, attention is everything. And
what you give your attention to—what you consume, what you engage with,
what you amplify—becomes a reflection of how you think.</em></p>
<p><em>What matters now is what you do with it. How you filter it. How
you recognize signal in the noise. Curation is the new IQ test.</em></p>
<p><em>Taste is often dismissed as something shallow or subjective. But
at its core, it’s a form of literacy—a way of reading the world. Good
taste isn’t about being right. It’s about being attuned. To rhythm, to
proportion, to vibe. It’s knowing when something is off, even if you
can’t fully articulate why.</em></p>
<p><em>Taste is what allows you to skim past the performative noise, the
fake depth, the viral bait, and know—instinctively—what’s worth your
time.</em></p>
<p><em>And that’s what real taste is: a deep internal coherence. A way
of filtering the world through intuition that’s been sharpened by
attention.</em></p>
<p><em>When you sharpen your discernment, you stop being swayed by
trends. You stop needing consensus. You stop reacting to every new thing
like it’s urgent.</em></p>
<p><em>There will always be creators. But the ones who stand out in this
era are also curators. People who filter their worldview so cleanly that
you want to see through their eyes. People who make you feel sharper
just by paying attention to what they pay attention to.</em></p>
<p><em>1995 interview with Steve Jobs — <strong>“Ultimately, it comes
down to taste. It comes down to trying to expose yourself to the best
things that humans have done, and then try to bring those things into
what you’re doing.”</strong></em></p>
<p><em>Good taste isn’t restrictive. It’s expansive. It allows you to
contain multitudes without becoming incongruent.</em></p>
<p><em>But good taste is deep structure. It’s the throughline in
someone’s life. You can see it in the design of their home, the cadence
of their speech, the way they treat people, the books on their
shelves.</em> <em>Taste is how you live a congruent life. Not in the
sense of brand consistency, but in the sense of spiritual alignment. You
can change your mind. Explore new spaces. But your values stay intact.
Your center holds.</em></p>
<p><strong>― Taste Is the New Intelligence - Wild Bare Thoughts</strong>
[<a
target="_blank" rel="noopener" href="https://wildbarestepf.substack.com/p/taste-is-the-new-intelligence">Link</a>]</p>
</blockquote>
<p>This is an amazing article. In an age of infinite content, taste is
your compass. It’s not about elitism—it’s about aligning your attention
with what truly matters to you. We can do these:</p>
<p><strong>Learnings and Suggestions</strong>:</p>
<ol type="1">
<li><p>Cultivate Discernment Over Consumption: Prioritize depth over
volume in what you read, watch, and engage with. Ask "Is this worth my
time?" before consuming content, creating something, or sharing. Trust
your intuition—if something feels off, skip it.</p></li>
<li><p>Curate Your Inputs (Because They Shape Your Outputs): Unfollow
accounts, mute topics, and unsubscribe from newsletters that don’t align
with your values. Follow thinkers, creators, and curators who
consistently offer depth. Set boundaries (e.g., no mindless scrolling
after 9 PM). Pause after reading/watching to digest, not just
react.</p></li>
<li><p>Build a "Library Mindset" (Not a Wishlist One): Read books,
essays, and long-form work that lingers. Don’t engage with viral content
just because it’s popular. Save/share only what resonates deeply—not
what’s merely entertaining.</p></li>
<li><p>Train Your Taste Like a Muscle: Study great art, writing, music,
and design to refine your sensibility. Remove distractions, unnecessary
commitments, and low-value inputs. Note what ideas/images/sounds stay
with you—these reveal your true taste.</p></li>
<li><p>Embrace Coherence Over Consistency: Your bookshelf, playlists,
and feeds should reflect who you are (or aspire to be). Stay open to new
influences, but filter them through your core principles. Don’t adopt
aesthetics/opinions for status—authenticity matters more.</p></li>
<li><p>Practice "Vibe Coding" (Like Rick Rubin): Whether in
conversations or creativity, prioritize feeling over formulas. In
work/life, strip away excess until only the essential remains. If
something feels "alive," lean in—even if it defies logic.</p></li>
<li><p>Reject Cheap Dopamine for Lasting Satisfaction: Opt for the book
over the tweet, the slow movie over the clip. After consuming something,
ask: Did this uplift or drain me? Regularly eliminate distractions
(apps, subscriptions, habits) that don’t serve you.</p></li>
<li><p>Taste as a Spiritual Practice: Prioritize art/ideas that
rearrange your perspective. From your home to your workspace, align
space with intention. Engage only with what nourishes, not
depletes.</p></li>
<li><p>Remember: Curation = Power: Amplify only what deserves a wider
audience. Your ability to filter signal from noise is a competitive
edge. The more you refine your taste, the more it protects you from
chaos.</p></li>
</ol>
<blockquote>
<p><strong>A Primer on US Healthcare - Generative Value</strong> [<a
target="_blank" rel="noopener" href="https://www.generativevalue.com/p/a-primer-on-us-healthcare">Link</a>]</p>
</blockquote>
<p>This article covers an overview of the system (main players), the
value chain (how products and services flow through the system and what
profitable segments are), incentives (motivation of behaviors),
challenges (significant issues within the industry), and potential
solutions (software and AI).</p>
<p>It deeply focuses on the interplay between incentives, middlemen, the
resulting administrative burden, and AI as the specific technological
solution appears to be a key perspective.</p>
<blockquote>
<p><strong>BREAKING: UnitedHealth Bleeds. CEO Witty Steps Down. - Sergei
Polevikov, AI Health Uncut</strong> [<a
target="_blank" rel="noopener" href="https://sergeiai.substack.com/p/breaking-unitedhealth-bleeds-ceo">Link</a>]</p>
<p><strong>UnitedHealth Abuse Tracker - Matt Stoller, American Economic
Liberties Project</strong> [<a
target="_blank" rel="noopener" href="https://www.economicliberties.us/data-tools/unitedhealth-group-abuse-tracker/">Link</a>]</p>
</blockquote>
<blockquote>
<ol type="1">
<li><em>Vibe coding is a new approach to software development that
utilizes AI tools to assist individuals in creating applications and
software <strong>without requiring extensive programming
knowledge</strong>.</em></li>
<li><em>The term was popularized by Andrej Karpathy, an AI expert, who
described it as a method where users <strong>interact with AI using
natural language</strong> to describe their ideas rather than writing
traditional code directly.</em></li>
<li><em>This allows creators, particularly those lacking technical
skills, <strong>to build functional applications rapidly</strong> by
simply explaining their requirements to the AI, which generates the
relevant code for them.</em></li>
</ol>
<p><strong>― What is Vibe Coding? - AI Supremacy</strong> [<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/what-is-vibe-coding-2025">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Who’s the Highest-Paid CEO? - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/whos-the-highest-paid-ceo">Link</a>]</p>
</blockquote>
<p>Rick Smith, co-founder and CEO of Axon.</p>
<blockquote>
<p><strong>I Summarized Mary Meeker's Incredible 340 Page 2025 AI Trends
Deck—Here's Mary's Take, My Response, and What You Can Learn - Nate, Ai
&amp; Product</strong> [<a
target="_blank" rel="noopener" href="https://natesnewsletter.substack.com/p/i-summarized-mary-meekers-incredible">Link</a>]</p>
</blockquote>
<p>Nate's overall take is that while Mary Meeker is correct that
Generative AI adoption is exploding, real value accrues only where
organizations align real-world problems with AI’s actual strengths in
workflows. He believes bigger claims demand commensurately bigger
evidence.</p>
<blockquote>
<p><em>Carl Dahlman later gave us the three categories that are widely
used today:</em></p>
<ol type="1">
<li><em><strong>Search and information costs:</strong> discovering what
is available to purchase and comparing alternatives</em></li>
<li><em><strong>Bargaining and decision costs</strong>: coming to an
agreement between buyer and seller, including establishing the final
price and terms</em></li>
<li><em><strong>Enforcement and policing costs</strong>: ensuring that
both sides holds up their end of the deal</em></li>
<li><em><strong>Distribution costs</strong>: actually getting the good
or service to the end consumer</em></li>
</ol>
<p><strong>― How To Build AI Agents (2025 Guide) - Max Berry, Max'
Prompts</strong> [<a
target="_blank" rel="noopener" href="https://www.maxberry.ca/p/how-to-build-ai-agents-2025-guide">Link</a>]</p>
</blockquote>
<p><strong>Key Concepts</strong>:</p>
<ul>
<li><strong>Transaction Costs</strong>: Costs incurred in addition to
the actual price of a good or service, necessary to coordinate and
execute a transaction. Marketplaces primarily sell the reduction of
these costs.</li>
<li><strong>TAM Expansion</strong>: Reducing transaction costs lowers
the effective cost of a good or service, increasing demand and expanding
the Total Addressable Market (TAM). The degree of TAM expansion relates
to the percentage of total cost eliminated.</li>
<li><strong>Value Distribution:</strong> Marketplaces save sellers money
on transaction costs and charge them a fee (often similar to what
sellers paid previously). They typically pass efficiency gains on to
buyers in the form of easier and faster experiences, creating a
demand-constrained market. <strong>Variable Costs of Addressing
Transaction Costs:</strong>
<ul>
<li><strong>Low Variable Costs:</strong> Addressing search and
bargaining costs is highly efficient and has low variable costs.
Marketplaces can keep more of the value created here.</li>
<li><strong>High Variable Costs:</strong> Addressing enforcement and
distribution costs involves significant variable costs (e.g., funding
returns, building logistics). While these make marketplaces bigger,
margins may be lower as value is passed to buyers.</li>
</ul></li>
</ul>
<p><strong>Takeaways</strong>:</p>
<p>This article puts the concept of transaction costs as central to
understanding marketplaces. Transaction costs are defined as the costs
incurred beyond the actual price of a good or service, associated with
coordinating and executing the transaction itself. Marketplaces are
essentially businesses that sell the reduction of these transaction
costs. Studying transaction costs can help determine where marketplaces
will succeed, what kind of marketplaces to build, and how to price
them.</p>
<p>Looking ahead, the article suggests that the "free lunch"
opportunities in many industries are exhausted, pushing marketplaces
into high variable cost activities. This implies future marketplaces may
be higher scale but potentially lower margin and more operationally
intensive. To disrupt incumbent marketplaces, one should look for
remaining transaction costs that can be addressed much more efficiently
than the current solution. The article suggests disrupting food delivery
was possible by building a more efficient network than restaurants had,
but disrupting shipping for handmade goods is harder because it requires
competing with highly efficient companies like UPS and Fedex.</p>
<p>By far, the largest unsolved transaction costs are in the
<strong>services industries</strong> (e.g., freelancing, home
improvement), which constitute two-thirds of consumer spending. Most
services marketplaces are currently stuck at the Lead Generation stage,
limiting penetration and take rate. This might be partly because much
spend is on recurring services where customers leave the marketplace
once a good provider is found, leading services marketplaces to rely on
high-churn consumer subscriptions. Despite this, there are
opportunities, such as Zillow exploring expanding into managed
marketplace territory for home services.</p>
<figure>
<img src="/di-blog/2025/06/07/2025-Jun/four-stages-of-marketplaces.png"
alt="four-stages-of-marketplaces" />
<figcaption aria-hidden="true">four-stages-of-marketplaces</figcaption>
</figure>
<blockquote>
<p><strong>An hour a day is all you need. - The Improvement
Journal</strong> [<a
target="_blank" rel="noopener" href="https://improvebypathsofstoicism.substack.com/p/an-hour-a-day-is-all-you-need">Link</a>]</p>
</blockquote>
<p>The one hour is suggested to be dedicated to three key practices that
aim to rebuild an individual from the ground up:</p>
<ol type="1">
<li><strong>Build Something That's Yours</strong>: This involves
creating something that belongs to you, beyond your job, such as a
newsletter, product, service, blog, or by learning/teaching a skill. The
purpose is to "plant seeds" that will compound over time, pulling you
out of stagnation.</li>
<li><strong>Train Like You Want to Be Here for a While</strong>: This
practice emphasizes physical strength and movement, like walking,
running, stretching, lifting, breathing, sleeping deeply, eating real
food, and drinking water. It's a message of self-care and an intention
to use one's body, which also sharpens mental clarity, as Seneca
suggested, "The body should be treated more rigorously, that it may not
be disobedient to the mind".</li>
<li><strong>Create Enough Silence to Hear Your Own Voice</strong>: This
habit counters the constant noise and stimulation of modern life. It
encourages practices like journaling, meditating, taking walks without
headphones, or simply sitting still without a goal or screen. The goal
is not productivity, but presence and creating space for reflection and
insight, preventing thoughts from being drowned out and actions from
remaining unexamined.</li>
</ol>
<blockquote>
<p><strong>How to become friends with literally anyone - April &amp; The
Fool</strong> [<a
target="_blank" rel="noopener" href="https://aprilandthefool.substack.com/p/how-to-become-friends-with-literally">Link</a>]</p>
</blockquote>
<p>The article suggests that becoming friends with anyone is to approach
interactions with a deep-seated belief in shared humanity, genuine
curiosity about individual worldviews, and an open, empathetic demeanor
that seeks to understand rather than judge.</p>
<ul>
<li><strong>Try to understand people through conversations with a belief
in the universal commonality of human nature:</strong> The author
fundamentally believes that all people are driven by the same core human
urges and desires, such as the need to feel loved, respected, and seen.
This perspective makes it intuitive to understand others. They see
meeting someone new as a "puzzle of empathy" and a "game of
commonality," where they try to understand what someone would think,
want, need, or crave given their background, values, limitations, and
longings.</li>
<li><strong>From common nature to differences among people due to
environmental factors:</strong> The author acknowledges that
<em>how</em> these needs are defined and achieved varies dramatically
due to factors like nationality, gender, religion, cultural heritage,
socioeconomic class, hobbies, and upbringing. These "little big
differences" are where things become interesting, leading to unique
individual personalities and perspectives.</li>
<li><strong>Follow the reasonableness within their personal worldview to
understand motivations and values:</strong> The author believes that
while people may not always be rational, they are always "reasonable"
within their own worldview. This means that everyone has reasons for
their actions, and those reasons make sense within their personal
framework. Understanding a person's circumstances allows the author to
understand their motivations, struggles, and values.</li>
<li><strong>Be curious and genuine while engaging with people:</strong>
The author describes themselves as extremely extraverted, loving people,
and hating small talk. They are curious about people, viewing them as
containing "worlds, histories, stories that span across generations and
geographies". This curiosity leads them to give "rapt attention and
genuine space to be yourself".</li>
<li><strong>Assumption of friendship from the beginning, share stories
and genuine care:</strong> The author approaches new encounters with the
assumption that "we are friends" from the moment they meet. They are
open, putting "all my cards on the table" and inviting others to reveal
theirs. They enjoy conversations, making people feel understood, heard,
and cared for.</li>
</ul>
<blockquote>
<p><em>The key takeaway is that genuine technological advancement, which
is real and accelerating, must be distinguished from the business models
built around it, which frequently adhere to age-old patterns. When
stated purpose and actual function align, it typically indicates that
the technology addresses a specific, measurable problem with clear
economic value, rather than promising to "transform
everything."</em></p>
<p><em>Systemantics, or, the art of understanding what’s going on, means
recognizing the persistent gaps between what systems proclaim and what
they actually do, and capitalizing on that insight.</em></p>
<p><em>When the fog dissipates and clarity emerges, the survivors will
be those who patiently deciphered the underlying mechanics amidst
fleeting illusions.</em></p>
<p><em>Enduring AI companies will emerge in two distinct spaces by 2035:
unglamorous but essential tools that demonstrably improve margins or
reduce costs, and genuine frontier research that reveals entirely new
problem spaces. The first category refines what exists; the second
invents what doesn't yet.</em></p>
<p><em>Real opportunities lie in the quiet spaces between stated
ambitions and operational truths. Just as they always have.</em></p>
<p><strong>― The Art of Understanding What's Going On - Tina He,
Fakepixels</strong> [<a
target="_blank" rel="noopener" href="https://fakepixels.substack.com/p/the-art-of-understanding-whats-going">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How I Went From Reading 20 Books Per Year to Over 75 Books -
Ryan Hall, Read and Think Deeply</strong> [<a
target="_blank" rel="noopener" href="https://ryandhall.substack.com/p/how-i-went-from-reading-20-books">Link</a>]</p>
</blockquote>
<p><strong>Takeaways</strong>:</p>
<ol type="1">
<li>Always take a book with you.</li>
<li>Give it about 50 pages before you quit. This keeps you from getting
stalled on a book that is not resonating with you.</li>
<li>Schedule the reading time. e.g., 45 min in the morning, 30 min in
the evening, and throughout the day when you get breaks.</li>
<li>Weekend sprints. Read in hour-long stretches on weekends or to do
several smaller stretches and get through entire sections or even whole
books on the weekends.</li>
</ol>
<blockquote>
<p><em>there are people with half your skills and intelligence living
out your dreams, just because they put themselves out there and didn’t
overthink it.</em></p>
<p><em>Reach out anyway—someone will always have more followers, more
free time, a better setup. It’s up to you to push through everything,
part the crowd, and make some space for yourself to at least give
yourself the chance of getting what you want.</em></p>
<p><em>You will never be fully ready and there will never be a perfect
time. It’s genuinely not about waiting for the right time to do
something when you’re ready, it’s about <strong>doing things before
you’re ready</strong> <strong>just to make them exist.</strong></em></p>
<p><strong>― literally just do things - Erifili Gounari, crystal
clear</strong> [<a
target="_blank" rel="noopener" href="https://erifili.substack.com/p/literally-just-do-things">link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Diabolus Ex Machina - Amanda Guinzburg, Everything Is A
Wave</strong> [<a
target="_blank" rel="noopener" href="https://amandaguinzburg.substack.com/p/diabolus-ex-machina">link</a>]</p>
</blockquote>
<blockquote>
<p><strong>how to think like a genius (the map of all knowledge) - Dan
Koe, Future/Proof</strong> [<a
target="_blank" rel="noopener" href="https://letters.thedankoe.com/p/how-to-think-like-a-genius-the-map">Link</a>]</p>
</blockquote>
<p>the article suggests that thinking like a genius involves adopting a
holistic and nuanced approach to problems by utilizing the AQAL model,
encompassing all relevant perspectives (quadrants) and evolving one's
consciousness to a "second-tier" level that can integrate and synthesize
different stages of understanding. This allows for faster
problem-solving and greater achievement in life.</p>
<p><strong>All Quadrants</strong>:</p>
<ul>
<li>Individual Interior (Upper Left): Your personal thoughts, emotions,
beliefs, and consciousness. Questions in this quadrant might include
core values, what makes you feel alive, or fears holding you back.</li>
<li>Individual Exterior (Upper Right): Your behaviors, actions, and
physical brain states. This involves looking at natural talents,
developed skills, and what your behavior reveals about your
preferences.</li>
<li>Collective Interior (Lower Left): Shared culture, values, and group
consciousness. This could involve understanding parental or religious
expectations, influence of friends, or shared values you're drawn
to.</li>
<li>Collective Exterior (Lower Right): Systems, structures, and social
institutions. This quadrant considers current job opportunities, the
impact of education or technology, and systemic barriers or
advantages.</li>
</ul>
<p><strong>All Levels</strong>:</p>
<ul>
<li><p>Premodern: Characterized by following established authority and
traditions, with black-and-white thinking and obedience to a God or
conformity.</p></li>
<li><p>Modern: Values science, individual achievement, competition, and
merit-based success.</p></li>
<li><p>Postmodern: Emphasizes relativistic thinking, where everyone's
truth is valid, and focuses on inclusion and equality. The article notes
that postmodern thinking can become pathological when it attempts to
dismantle all hierarchies.</p></li>
<li><p><strong>Second-Tier</strong>: This is the suggested stage for
"genius" thinkers. Individuals at this level can look back and
synthesize truths from all prior perspectives, embracing complexity,
systems thinking, and awareness. It's less about "I'm right and you're
wrong" and more about <strong>finding the best solution through
synthesis, holding contradictions in mind until they can be
reconciled</strong>. <strong>Genius thinkers act as "translators"
between different stages</strong>.</p></li>
</ul>
<blockquote>
<p><strong>How to Be Taken Seriously - Tessa Xie, Diving Into
Data</strong> [<a
target="_blank" rel="noopener" href="https://www.divingintodata.com/p/how-to-be-taken-seriously">Link</a>]</p>
</blockquote>
<p>A summary of the four junior traits and what to do instead:</p>
<ul>
<li><strong>Junior Trait #1: Providing too much
detail/over-explaining</strong>
<ul>
<li>Excessive detail doesn't showcase knowledge and consideration of
edge cases, but it typically confuses the audience and makes them appear
unable to synthesize information, causing key points to be missed.
Managers may even prevent such individuals from presenting to executives
to avoid confusion and inefficiency in meetings.</li>
<li>What to do about it:
<ul>
<li>In written form: Summarize work with a "TL;DR" at the top, using the
Pyramid Principle (conclusion first, then supporting evidence). Focus on
what is important enough to communicate, moving less critical details to
an appendix.</li>
<li>In verbal form: Practice an "elevator pitch" of less than 30 seconds
to peers, focusing on the "why" and enough "what" to allow for opinion
formation. <strong>The ability to decide what NOT to communicate is as
crucial as what to mention.</strong></li>
</ul></li>
</ul></li>
<li><strong>Junior Trait #2: Not having an opinion or
recommendation</strong>
<ul>
<li>As data scientists become more senior, translating analysis into a
recommendation becomes increasingly important. Hesitation to provide
recommendations often stems from the perceived risk and the nuanced,
non-black-and-white nature of data, leading to "analysis paralysis".
However, not giving recommendations shows a lack of ownership and limits
one to simple "execution" work.</li>
<li>What to do about it:
<ul>
<li>Adopt an <strong>ownership mindset</strong>, imagining you are the
decision-maker. Ask what data <em>you</em> would need and if the
presented data would convince <em>you</em>.</li>
<li>Understand that value comes from giving robust recommendations
<em>despite</em> nuance and ambiguity, just as taking risks can lead to
above-average returns.</li>
<li>While you should list caveats, <strong>most people prefer a
recommendation they disagree with over no recommendation at all, as it
provides a basis for discussion and understanding assumptions.</strong>
Data teams are paid to <strong>drive business decisions,</strong> not
just pull and present data.</li>
</ul></li>
</ul></li>
<li><strong>Junior Trait #3: Not being clear about the "why" behind the
analysis</strong>
<ul>
<li>Junior DS often state "XYZ stakeholder asked for this" as the sole
reason for an analysis, which is insufficient. This indicates a lack of
ownership of the business problem and hinders the ability to deliver
effective solutions, leading to frustration from changing data
requests.</li>
<li>What to do about it:
<ul>
<li><strong>Own the problem</strong>. When asked to pull data, find out
<em>why</em> the stakeholder needs it and what decision they are trying
to make.</li>
<li>By understanding the ultimate business problem, you can
<strong>brainstorm the most effective data solutions</strong>,
potentially different from the original request, thus <strong>elevating
yourself to a</strong> <strong>thought partner</strong>.</li>
</ul></li>
</ul></li>
<li><strong>Junior Trait #4: Not having the basics down to be able to
stay "one step ahead"</strong>
<ul>
<li>Losing credibility happens when people feel you don't know the data
or business area you cover. To establish yourself as an expert, you need
to <strong>anticipate common questions and be the most familiar with the
data in your area</strong>. If you lack answers to natural follow-up
questions, it suggests you haven't thoroughly understood or explored the
data.</li>
<li>What to do about it:
<ul>
<li>Be curious about your data; start with a basic question and explore
from there, jotting down answers.</li>
<li>Anticipate follow-up questions in three buckets before presenting:
<strong>foundational knowledge</strong> (e.g., how the product works,
user numbers), <strong>your analysis</strong> (details beyond the main
insights), and <strong>next steps</strong> (what the findings mean for
stakeholders).</li>
<li>Get a second pair of eyes on your work, ideally from someone not
deep in the analysis, to catch obvious omissions.</li>
</ul></li>
</ul></li>
</ul>
<blockquote>
<p><strong>How to work with AI: Getting the most out of Deep Research -
Torsten Walbaum, Operator's Handbook</strong> [<a
target="_blank" rel="noopener" href="https://www.operatorshandbook.com/p/how-to-work-with-ai-getting-the-most">Link</a>]</p>
</blockquote>
<p>A comprehensive guide of AI Deep Research from idea to value. The
author provided a good ChapGPT Deep Research example <a
target="_blank" rel="noopener" href="https://chatgpt.com/share/6859a8d6-715c-8002-9afa-f3174f0a4b72">here</a>.
Deep Research prompt for meeting transcription by o3 <a
target="_blank" rel="noopener" href="https://docs.google.com/document/d/1rZHtuB0NiKALkwoLN3md34L56tQEArXQvybX-gp0gi0/edit?tab=t.0#heading=h.aq5h6awtm7bk">here</a>.</p>
<figure>
<img
src="/di-blog/2025/06/07/2025-Jun/structuring_a_deep_research_prompt.png"
alt="structuring_a_deep_research_prompt" />
<figcaption
aria-hidden="true">structuring_a_deep_research_prompt</figcaption>
</figure>
<p>Free 15 queries per (ChatGPT+Gemini); Free 13 queries per day
(perplexity+Grok)!</p>
<figure>
<img src="/di-blog/2025/06/07/2025-Jun/pricing_and_limits.png"
alt="pricing_and_limits" />
<figcaption aria-hidden="true">pricing_and_limits</figcaption>
</figure>
<h2 id="papers-and-reports">Papers and Reports</h2>
<blockquote>
<p><strong>Trends - Artificial Intelligence - Mary Meeker, Bond</strong>
[<a target="_blank" rel="noopener" href="https://www.bondcap.com/report/tai/#view/0">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Think Only When You Need with Large Hybrid-Reasoning
Models</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.14631">Link</a>]</p>
</blockquote>
<p>Large Reasoning Models (LRMs) improve reasoning via extended thinking
(e.g., multi-step traces), but this leads to inefficiencies like
overthinking simple queries, increasing latency and token usage. The
team Introduces Large Hybrid-Reasoning Models (LHRMs) — the first models
that adaptively choose when to think based on query complexity,
balancing performance and efficiency. They utilizes a two-stage
approach: 1) <strong>Hybrid Fine-Tuning (HFT)</strong> – cold start
using curated datasets labeled as "think" vs. "non-think"; 2)
<strong>Hybrid Group Policy Optimization (HGPO)</strong> – an online RL
method that trains the model to pick the optimal reasoning mode. They
defines <strong>Hybrid Accuracy</strong> to evaluate how well the model
selects between thinking and non-thinking strategies; correlates
strongly with human judgment. Experiments show LHRMs outperform both
LRMs and traditional LLMs in reasoning accuracy and response quality,
while also reducing unnecessary computation.</p>
<blockquote>
<p><strong>The 2025 State of B2B - Monetization - Kyle Poyar</strong>
[<a
target="_blank" rel="noopener" href="https://cdn.prod.website-files.com/66f3d5c51c51d8e744b8d529/682e259fa19323cc4ecda6be_60d3fca97de926e1eff89042d5a068a2_2025%20State%20of%20Monetization%20Report_Tremont_2025.pdf">Link</a>]</p>
</blockquote>
<p>The report summarizes a poll of 240 software companies about their
pricing strategies. Key findings indicate a decline in flat-rate and
seat-based pricing models, with hybrid pricing (combining subscriptions
and usage) emerging as the dominant approach, especially for companies
incorporating AI capabilities. The report also highlights a growing
interest in outcome-based pricing among AI-native companies and stresses
the importance of pricing agility and clear ownership of pricing
strategy within organizations.</p>
<blockquote>
<p><strong>The Illusion of Thinking: Understanding the Strengths and
Limitations of Reasoning Models via the Lens of Problem Complexity -
Apple Machine Learning Research</strong> [<a
target="_blank" rel="noopener" href="https://machinelearning.apple.com/research/illusion-of-thinking">Link</a>]</p>
</blockquote>
<p>The authors analyzed the thinking process and reasoning traces of
LRMs in several smart ways:</p>
<ul>
<li>A custom pipeline using regex identifies and extracts potential
solution attempts from the LRM's thinking traces.</li>
<li>Extracted solutions are rigorously verified against puzzle rules and
constraints using specialized simulators for step-by-step
correctness.</li>
<li>Records the accuracy of valid solutions and their relative position
within the reasoning trace for behavioral insights.</li>
<li>Categorizes LRM thinking patterns (e.g., overthinking, late success,
collapse) by analyzing how solution correctness and presence vary with
problem complexity.</li>
<li>Examines how the proportion of correct solutions changes
sequentially within the thinking trace, revealing dynamic accuracy
shifts.</li>
<li>Pinpoints the initial incorrect step in a solution sequence to
understand the depth of correct reasoning before error.</li>
<li>Quantifies thinking token usage to analyze scaling of effort with
complexity, noting an unexpected decline at high complexity.</li>
</ul>
<blockquote>
<p><strong>How much do language models memorize? - Meta, Google, NVIDIA,
and Cornell University</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24832">Link</a>]</p>
</blockquote>
<p>This paper proposed a new method to quantify how much information a
language model "knows" about a datapoint.</p>
<p>They formally separate memorization into two components by two novel
definitions of memorization: unintended memorization (information about
a specific dataset) and generalization (information about the true
data-generation process).</p>
<p><strong>There are several interesting findings:</strong></p>
<ul>
<li>By training models on uniform random bitstrings (eliminating
generalization), they precisely measure model capacity, finding
GPT-style transformers store 3.5 to 4 bits per parameter.</li>
<li>Their framework shows that the double descent phenomenon occurs when
the data size exceeds the model capacity, suggesting that models are
"forced" to generalize when they can no longer individually memorize
datapoints.</li>
<li>The paper develops and validates a scaling law that predicts
membership inference performance based on model capacity and dataset
size, indicating that membership inference becomes harder with larger
datasets relative to model capacity.</li>
</ul>
<p><strong>To understand their smart methods:</strong></p>
<ul>
<li><p>They proposed a very clever approach to understand memorization
and model capacity in LM. They isolate unintended memorization by
training models on <strong>uniform random bitstrings</strong>.</p>
<ul>
<li>No Generalization Signal: When training on truly random data, there
are no underlying patterns, rules, or structures for the model to
generalize from. Each bitstring is an independent, random piece of
information.</li>
<li>Only Memorization is Possible: In this scenario, the <em>only</em>
way for the model to "learn" or perform well on this data (i.e., predict
the next bit in a sequence or identify if it was part of the training
set) is to literally memorize the specific bitstrings it has seen. Any
"knowledge" the model gains is purely about the individual data
points.</li>
<li>Total Memorization as Measured: Therefore, when generalization is
effectively zero, the information the model stores about the random
bitstrings directly reflects its <em>total memorization capacity</em>
for that type of information. There's no "general knowledge" to
distinguish; it's all about remembering specific instances.</li>
</ul>
<p>Therefore, they are measuring the maximum amount of distinct,
specific information the model can store.</p>
<p>They equal total memorization to model capacity. In machine learning,
model capacity generally refers to the <strong>size and complexity of
the functions a model is capable of learning</strong>. It's the model's
ability to fit a wide variety of patterns in the data. A model with
higher capacity can potentially fit more complex relationships or
memorize more specific data points.</p>
<p>The paper further quantifies this by showing that GPT-style models
have a capacity of approximately 3.6 bits-per-parameter. This indicates
that each parameter in the model effectively acts as a certain amount of
storage for information, reflecting the overall capacity of the neural
network architecture.</p></li>
<li><p>The fundamental challenge in understanding and evaluating
language models is the <strong>ambiguity and conflation of
"memorization" (copy or reproduce a specific sequence in the training
data) and "learning." (truly understand and generalize a pattern or
concept)</strong> This is exactly what they addressed by decomposing
memorization into unintended memorization and generalization. The
decomposition enables controlled measurement and the use of random
bitstrings is the key innovation.</p>
<p>About the Double Descent Phenomena: When a model's capacity exceeds
the generalizable patterns in the data, it starts to memorize individual
data points. As data size increases relative to capacity, the model is
"forced" to generalize more, leading to a decrease in unintended
memorization and an improvement in performance.</p>
<p>The core insight is that as models become massively overparameterized
(far beyond what's needed to simply fit the training data), they find
"simpler" interpolating solutions that generalize better, often due to
the implicit biases of optimization algorithms like Stochastic Gradient
Descent (SGD).</p>
<p><strong>Intuition for double descent</strong>:</p>
<ol type="1">
<li>"Under-parameterized" Regime (Classical ML): Model Capacity &lt;
Data Size: very generalizable, low test error</li>
<li>"Interpolation Threshold" (The Peak): Model Capacity ~ Data Size:
peak of test error, due to overfitting the noise</li>
<li>"Over-parameterized" Regime (Double Descent / Modern Deep Learning):
Model Capacity &gt;&gt; Data Size: robust generalization happens, test
error goes down again.</li>
</ol></li>
<li><p>Here is a concept <strong>Membership Inference Attacks
(MIAs):</strong> These are attacks that attempt to determine whether a
specific data point was part of a model's training dataset or not. A
successful MIA indicates that the model has "memorized" that specific
data point. "Scaling Laws for Membership Inference" in the paper refers
to <strong>predictive relationships that describe how the success rate
of a MIA changes as a function of various model and data
characteristics</strong>, such as model capacity and dataset size.</p>
<p>They basically propose that <strong>membership inference success is
inversely related to how "generalizable" the data point is within the
model's capacity.</strong> In other words, membership inference is
<strong>easier</strong> when the model has high capacity relative to the
dataset size, leading to more unintended memorization of individual data
points. Membership inference becomes <strong>harder</strong> (i.e., less
successful for an attacker) when the dataset size is very large compared
to the model's capacity, or when the model starts to "grok" (generalize)
more.</p></li>
</ul>
<blockquote>
<p><strong>LlamaFirewall: An open source guardrail system for building
secure AI agents - Meta AI</strong> [<a
target="_blank" rel="noopener" href="https://ai.meta.com/research/publications/llamafirewall-an-open-source-guardrail-system-for-building-secure-ai-agents/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Reasoning models don't always say what they think -
Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/research/reasoning-models-dont-say-think">Link</a>]</p>
</blockquote>
<p>The core issue explored is whether the Chain-of-Thought accurately
reflects the model's true reasoning process ("faithfulness"). The
article's research suggests that models frequently do not faithfully
report their reasoning. Researchers tested this by subtly providing
hints (both correct and incorrect) to models during evaluations. They
then checked if the models admitted using these hints in their
Chain-of-Thought explanations. The results showed that models (Claude
3.7 Sonnet and DeepSeek R1) rarely mentioned the hints they used, even
when those hints led them to a different answer or involved potentially
problematic information (like "unauthorized access"). Instead, models
often created fake rationales for their answers.</p>
<p>The researchers also studied reward hacking by constructing synthetic
Reinforcement Learning (RL) environments with injected "reward hacks."
They then finetuned an earlier snapshot of Claude 3.7 Sonnet in these
environments using outcome-based RL. By setting up the environments this
way, they could observe if the models learned to pick the incorrect hint
answer to maximize their reward, and then check if their
Chain-of-Thought verbalized this reward-seeking behavior. They found
that models learned the reward hacks very quickly (selecting the
incorrect hint answer over 99% of the time), but rarely verbalized these
hacks in their Chain-of-Thought (less than 2% of examples in most
environments).</p>
<blockquote>
<p><strong>Your Brain on ChatGPT: Accumulation of Cognitive Debt when
Using an AI Assistant for Essay Writing Task - MIT</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.08872v1">Link</a>]</p>
</blockquote>
<p>This experimental study combines neuroscience (EEG), educational
psychology, and human-AI interaction to examine how AI tools like
ChatGPT influence cognitive engagement during essay writing. The result
shows that brain-only group showed stronger and broader neural
connectivity, especially in theta bands, indicating deeper internal
ideation and cognitive engagement. LLM group exhibited reduced alpha and
theta connectivity, suggesting externalized and narrower thought
patterns—relying more on ChatGPT suggestions rather than internal
generation of ideas. So it's saying if you rely heavily on AI, you will
get dumber.</p>
<blockquote>
<p><strong>On the Extinction Risk from Artificial Intelligence -
RAND</strong> [<a
target="_blank" rel="noopener" href="https://www.rand.org/pubs/research_reports/RRA3034-1.html">Link</a>]</p>
</blockquote>
<p>This report examines the potential for AI to cause human extinction.
The authors analyzed three specific scenarios: <strong>the use of
weapons, the release of biological pathogens, and severe climate warming
via malicious geoengineering</strong>. The study concludes that
significant barriers exist for AI to achieve human extinction and it
would likely require intentional AI action and substantial time for the
threats to materialize, allowing for human response and mitigation
efforts. Ultimately, the report suggests that resources dedicated to AI
extinction risk should also support broader global catastrophic risk
mitigation and general AI safety.</p>
<p>This analysis starts from what could cause human extinction and
assesses how AI could contribute to that. The threats are limited to
those can be explored through scenario planning, while those that
involve a deeper level of uncertainty are ignored.</p>
<blockquote>
<p><strong>The OpenAI Files</strong> [<a
target="_blank" rel="noopener" href="https://www.openaifiles.org/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Trends in AI Supercomputers - Epoch AI</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/html/2504.16026v2">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Can US infrastructure keep up with the AI economy? -
Deloitte</strong> [<a
target="_blank" rel="noopener" href="https://www.deloitte.com/us/en/insights/industry/power-and-utilities/data-center-infrastructure-artificial-intelligence.html">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>2025: The State of Consumer AI - Shawn Carolan, et al., Menlo
Venture</strong> [<a
target="_blank" rel="noopener" href="https://menlovc.com/perspective/2025-the-state-of-consumer-ai/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Generative AI &amp; Gender Global Employment Trends -
Linkedin Economic Graph</strong> [<a
target="_blank" rel="noopener" href="https://economicgraph.linkedin.com/content/dam/me/economicgraph/en-us/PDF/generative-ai-and-gender.pdf">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Agentic Misalignment: How LLMs could be insider threats -
Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/research/agentic-misalignment">Link</a>]</p>
</blockquote>
<p>Researchers stress-tested 16 leading AI models from various
developers and found that in certain situations, these models resorted
to malicious behaviors like blackmailing officials or leaking sensitive
information. These actions occurred when the models faced replacement or
when their assigned goals conflicted with the company's changing
direction, even when they were given only harmless business goals and
explicitly told to avoid such behaviors. While no evidence of agentic
misalignment has been observed in real deployments, the findings suggest
caution when deploying current models in roles with minimal human
oversight and access to sensitive information.</p>
<blockquote>
<p><strong>SHADE-Arena: Evaluating sabotage and monitoring in LLM agents
- Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/research/shade-arena-sabotage-monitoring">Link</a>]</p>
</blockquote>
<p>Dual Objective Setup:</p>
<ul>
<li>Each evaluation environment includes a clearly defined <strong>main
task</strong> that simulates a real-world scenario and a <strong>secret
side task</strong> that is harmful but can be plausibly hidden within
the main task execution.</li>
<li>The agent's challenge: <strong>complete both tasks</strong> while
<strong>avoiding suspicion</strong>.</li>
</ul>
<p>Using LLMs to assess detection, measuring task completion vs.
detection success, and providing a benchmark for improving agent safety
and control.</p>
<h2 id="youtube-and-podcasts">YouTube and Podcasts</h2>
<blockquote>
<p><strong>Sundar Pichai: CEO of Google and Alphabet | Lex Fridman
Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/hashtag/471">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Jared Isaacman: What went wrong at NASA | The All-In
Interview</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=6YdOjoaQTOQ&amp;list=WL&amp;index=3">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Naval Ravikant On The 4 Books That CHANGED His Life
(Financially And Philosophically)</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=FOcD1ngbsXk&amp;list=WL&amp;index=4">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Chamath Palihapitiya: Zuckerberg, Rogan, Musk, and the
Incoming “Golden Age” Under Trump - Tucker Carison</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=eRlkcRnC9eE&amp;list=WL&amp;index=13">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Satya Nadella on AI Agents, Rebuilding the Web, the Future of
Work, and more - Rowan Cheung</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_a8EnBX8DSU&amp;list=WL&amp;index=20">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Jeff Bezos: Amazon and Blue Origin | Lex Fridman Podcast -
Lex Fridman</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/hashtag/405">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>WWDC25: Platforms State of the Union - Apple</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLjODKV8YBFHbabxpCUjMzyVKZmcOFgd9b">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>#385 Michael Dell - Founders</strong> [<a
target="_blank" rel="noopener" href="https://www.founderspodcast.com/episodes/385-michael-dell">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>IPOs and SPACs are Back, Mag 7 Showdown, Zuck on Tilt,
Apple's Fumble, GENIUS Act passes Senate - All-In Podcasts</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=86t6YNf_B7Q&amp;list=PLn5MTSAqaf8peDZQ57QkJBzewJU1aUokl">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>I think being smart — and not being afraid to show it — means
living with discomfort. It's difficult. It means being willing and able
to admit when you're wrong. And it means being okay with complexity,
contradiction, and uncertainty.</em></p>
<p><em>So, the good news: I think smart survives. It's stubborn. It's
not loud. It's not flashy. But it's resilient — like a cockroach with a
PhD.</em></p>
<p><em>And the thing about stupidity is that eventually it bumps into
the hard wall of reality. When the bridges collapse and the crops fail,
when the Amazon package doesn't show up because the supply chain finally
imploded — suddenly people start looking around and going, "Hey, does
anybody know how to fix things? Build things?"</em></p>
<p><em>And the guy who spent the last 20 years reading books and
educating himself instead of screaming at his phone? Yeah, he's the one
holding the duct tape.</em></p>
<p><em>It won't be sexy, and it won't be immediate. But intelligence
isn't dead — it's just hungover. And sooner or later, we're going to
need it to crawl out of bed, drink some black coffee, and start fixing
the mess.</em></p>
<p><em>So at the end of the day, stupidity will always be popular — at
least in the U.S. But intelligence — real, patient, compassionate
intelligence — is what keeps the lights on.</em></p>
<p><em>And by the way, that goes for emotional intelligence too, which
is every bit as important.</em></p>
<p><em>So if you're still here, still critically thinking, still
refusing to go quietly into that great dumb night — you're already part
of the resistance.</em></p>
<p><em>Keep going.</em></p>
<p><strong>― The Death of Intelligence: Why Modern Society Celebrates
Stupidity - The Functional Melancholic</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=NTnQMCOhZFM&amp;list=WL&amp;index=6&amp;pp=gAQBiAQB">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The Obsession That Creates Enduring Companies | David Senra
Interview - Invest Like The Best</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Ak7oLVMlhfM&amp;list=WL&amp;index=24&amp;t=2459s">Link</a>]</p>
</blockquote>
<h2 id="articles-and-blogs">Articles and Blogs</h2>
<blockquote>
<p><strong>Everything Google Announced at I/O 2025 - WIRED</strong> [<a
target="_blank" rel="noopener" href="https://www.wired.com/story/everything-google-announced-at-io-2025/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Launch Hugging Face Models In Colab For Faster AI Exploration
- Medium</strong> [<a
target="_blank" rel="noopener" href="https://medium.com/google-colab/launch-hugging-face-models-in-colab-for-faster-ai-exploration-bee261978cf9">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>My AI Skeptic Friends Are All Nuts - Thomas Ptacek</strong>
[<a target="_blank" rel="noopener" href="https://fly.io/blog/youre-all-nuts/">Link</a>]</p>
</blockquote>
<p>The author argues that LLMs as agents are improving developer
productivity, and suggests that while the hype around AI can be
annoying, the technology's impact is real and profound. He believes that
those who don't embrace AI in their coding practices will be left
behind.</p>
<blockquote>
<p><strong>My Brain Finally Broke - The New Yorker</strong> [<a
target="_blank" rel="noopener" href="https://www.newyorker.com/culture/the-weekend-essay/my-brain-finally-broke">Link</a>]</p>
</blockquote>
<p>The author shares a disorienting sense of reality's erosion,
attributing it to various factors, including the relentless pace of
digital information, the overwhelming nature of political events, and
the insidious proliferation of AI. This environment fosters a collective
cognitive detachment and erosion of critical faculties, making it
challenging to discern truth, engage effectively, and maintain a
grounded sense of self and world.</p>
<blockquote>
<p><strong>Is there a Half-Life for the Success Rates of AI Agents? -
Toby Ord</strong> [<a
target="_blank" rel="noopener" href="https://www.tobyord.com/writing/half-life">Link</a>]</p>
</blockquote>
<h2 id="news">News</h2>
<blockquote>
<p><strong>Meet the Foundation Models framework - Apple</strong> [<a
target="_blank" rel="noopener" href="https://developer.apple.com/videos/play/wwdc2025/286/">Link</a>]</p>
</blockquote>
<p>The iPhone maker has launched the Foundation Models framework to
allow users to run a 3B parameter model locally. The framework is part
of Apple Intelligence suite and allows developers to access it using
three lines of code. The model can be used to generate text, extract
summaries, and tag structured information from unstructured text.</p>
<p>Users should be aware of strength and weakness. It's only available
on Apple Intelligence-enabled devices with OS version 26+. You need to
use Xcode Playgrounds to prototype with real model output. You can use
Instruments profiling template to measure latency and token overhead.
There is no support for fine-tuning or external model deployment</p>
<blockquote>
<p><strong>Connect Your MCP Client to the Hugging Face Hub -
HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/changelog/hf-mcp-server">Link</a>]</p>
</blockquote>
<p>HuggingFace releases open-source MCP server to allow accessing its
tools from VSCode and Claude Desktop.</p>
<h2 id="new-book-list">New Book List</h2>
<p>Some book names from my daily readings recently caught my attention
and might be the next book to read for me:</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/0062916602/ref=ox_sc_act_title_7?smid=ATVPDKIKX0DER&amp;psc=1">How
Innovation Works: And Why It Flourishes in Freedom</a> - Matt Ridley
(2021)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/0061452068/ref=ox_sc_act_title_8?smid=ATVPDKIKX0DER&amp;psc=1">The
Rational Optimist: How Prosperity Evolves (P.s.)</a>- Matt Ridley
(2021)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Incerto-Fooled-Randomness-Procrustes-Antifragile/dp/059324365X">Incerto</a>
- Nassim Nicholas Taleb (2021)
<ul>
<li>Fooled by Randomness; The Black Swan; The Bed of Procrustes;
Antifragile; Skin in the Game</li>
</ul></li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Beginning-Infinity-Explanations-Transform-World/dp/0143121359/">The
Beginning of Infinity</a> - David Deutsch (2012)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/0887307280/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;psc=1">The
E-Myth Revisited: Why Most Small Businesses Don't Work and What to Do
About It</a> - Michael E. Gerber</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/1591846447/ref=ox_sc_act_title_3?smid=ATVPDKIKX0DER&amp;psc=1">Start
with Why: How Great Leaders Inspire Everyone to Take Action</a> - Simon
Sinek</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/0307887898/ref=ox_sc_act_title_4?smid=ATVPDKIKX0DER&amp;psc=1">The
Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to
Create Radically Successful Businesses</a> - Eric Ries</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/1501111116/ref=ox_sc_act_title_5?smid=ATVPDKIKX0DER&amp;psc=1">Grit:
The Power of Passion and Perseverance</a> - Angela Duckworth</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/0345472322/ref=ox_sc_act_title_6?smid=ATVPDKIKX0DER&amp;psc=1">Mindset:
The New Psychology of Success</a> - Carol S. Dweck</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/dp/0998116319?ref=ppx_yo2ov_dt_b_fed_asin_title">7
Powers: The Foundations of Business Strategy</a> - Hamilton Helmer
(2016)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Great-CEO-Within-Tactical-Building/dp/0578599287/">The
Great CEO Within: The Tactical Guide to Company Building</a> - Matt
Mochary (2019)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Hard-Thing-About-Things-Building/dp/0062273205/">The
Hard Thing About Hard Things: Building a Business When There Are No Easy
Answers―Straight Talk on the Challenges of Entrepreneurship</a> - Ben
Horowitz (2014)</li>
<li><a target="_blank" rel="noopener" href="https://amzn.to/43SsbXR">Inside the Ultimate Money Mind</a>
– Robert G. Hagstrom (2023)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Creative-Act-Way-Being/dp/0593652886/">The
Creative Act: A Way of Being</a> - Rick Rubin (2023)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/0070329656/ref=ox_sc_act_title_1?smid=A34MOU1WOG7LRW&amp;psc=1">In
the Company of Giants: Candid Conversations With the Visionaries of the
Digital World</a> - Rama Dev Jager (1998)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/dp/0593798694?ref=ppx_yo2ov_dt_b_fed_asin_title">The
Technological Republic: Hard Power, Soft Belief, and the Future of the
West</a> - Alexander C. Karp (2025)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/dp/1984878530?ref=ppx_yo2ov_dt_b_fed_asin_title">The
Contrarian: Peter Thiel and Silicon Valley's Pursuit of Power - Max
Chafkin</a> (2021)</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/05/14/How-Leaders-Learn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/05/14/How-Leaders-Learn/" class="post-title-link" itemprop="url">How Leaders Learn</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-05-14 09:42:44" itemprop="dateCreated datePublished" datetime="2025-05-14T09:42:44-04:00">2025-05-14</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure>
<img src="/di-blog/2025/05/14/How-Leaders-Learn/how_leaders_learn.jpg"
alt="how_leaders_learn" />
<figcaption aria-hidden="true">how_leaders_learn</figcaption>
</figure>
<p>"<strong><em>How Leaders Learn</em></strong>" by <strong>David
Novak</strong> is a great book for active learners. It has three
chapters: "Learn from", "Learn to", and "Learn by".</p>
<p>Active learners are like artists—constantly refining, adapting, and
evolving. They approach life as an masterpiece-in-progress,
understanding that each new insight adds depth and clarity to the bigger
picture. The book encourages active learning and defines it as a mindset
- a daily discipline of seeking out knowledge from people, experience,
and failures, staying open to feedback, new perspectives, and
uncomfortable truths, and taking actions to test ideas, adapting and
refining.</p>
<blockquote>
<p>An active learner is somebody who seeks out ideas and insights and
then pairs them with action and execution. They learn with purpose. The
result is greater possibilities-for them and the people around them.</p>
</blockquote>
<blockquote>
<p>It's as Eric Hoffer, the American philosopher, wrote in Reflections
on the Human Condition: "In a time of drastic change, it is the learners
who inherit the future." They can't wait to discover the next idea, and
the next, and the next, because behind every idea is a world of
possibility and a brighter future.</p>
</blockquote>
<blockquote>
<p>Warren Buffett once told me what he looks for in the companies he
acquires. He said, "I'm looking to buy companies that are run by
painters." When I asked for an explanation, he said, "Most great artists
have a hard time letting go of their paintings. They're in love with the
painting. They are constantly adding a dab of color here, a little more
texture there. I'm looking for the boss who is always tweaking their
company, constantly trying to make it better. No matter how successful
they may have already been, what they still see is a
masterpiece-in-progress." He calls Berkshire Hathaway a museum for these
masterpieces, but he expects the people who run them to keep making
progress, to keep changing and expanding.</p>
</blockquote>
<p>This book covers a lot of good practices, some of which I learned
through experience and have been implementing in daily life, but I've
never clearly summarized them in words like this author does (e.g.,
learn from failure and success, learn to ask better questions, learn to
develop pattern thinking, learn to reflect, learn by tackling problems,
etc); some are common sense to people but not easy to follow (e.g. learn
to see the world the way it really is, learn to make and check your own
judgments, learn by being your best self, learn by seeking new
challenges, learn by making everyone count, learn by recognizing on
purpose, etc); others are new ideas and wise advice to me that are
incredibly enlightening (e.g., learn from new environment, learn to
trust in positive intentions, learn to be humble and confident, learn by
simplying, learn by teaching, etc).</p>
<p>My Learnings:</p>
<ol type="1">
<li><p>I carry good values and get rid of bad values from my upbringings
and move on, but never go back and think about weakness and blind spots
that were developed implictly.</p>
<blockquote>
<p>Our upbringings shape us-the good and bad experiences, the normal
experiences of our day-to-day lives. When you choose to learn from your
upbringing, you learn who you are, your strengths and weaknesses, your
unique perspective, and your blind spots.</p>
</blockquote></li>
<li><p>I'm the type of person who stick to one thing or one job, do the
best, and get the most learnings from it - greedy but probably not the
most efficient approach. So this is the top one advice for me:
<strong><em>"Not moving means not growing"</em></strong> and
<strong><em>"Choose environment wisely and don't stand
still"</em></strong>.</p>
<blockquote>
<p>New environments bring uncertainty and risk, two things humans really
don't like. The brain weighs threats of loss heavier than it does
opportunities for gain. Whether it's a move to a new city or a move to a
new company, we don't know the people or the culture and we don't know
if we'll succeed when we get there. The brain tells us it's best if we
just stay where we are, in our more certain, less risky, known
environment. But that's not always the right choice. Josh Waitzkin,
child chess prodigy, subject of the book and movie Searching for Bobby
Fischer, and later a tai chi world champion, wrote in The Art of
Learning, "Growth comes at the expense of previous comfort or
safety.</p>
</blockquote>
<p>However, not every new environment is good for you, it requires some
luck and judgment.</p>
<blockquote>
<p>When looking at a new environment, evaluate it for these four sources
of learning:</p>
<ul>
<li>New knowledge, skills, or systems</li>
<li>New ideas and innovative thinking</li>
<li>New people and their perspectives and opinions</li>
<li>New influences that lead to personal growth</li>
</ul>
</blockquote>
<blockquote>
<p>Some new environments aren't going to advance your learning; they
might even slow you down.</p>
<p>First, make sure the new environment will offer opportunities to
learn and grow in any area that's important to you right now, like I
did. This is especially true when you have an ambition but aren't sure
how to get there.</p>
<p>As important as this work is, the next important step is to insert
ourselves in an environment filled with people who routinely do what
we're struggling to imagine." This is the whole point of choosing a new
environment.</p>
<p>Second, choose an environment that's suited to you. Understanding
your personal ideal environment is an important aspect of
self-awareness.</p>
<p>Third, choose an environment that will exert the right influences on
you, so that you're not only learning new skills, new knowledge, and new
ideas, but also absorbing better collaboration, better leadership,
better self-management, or whatever area of personal growth you think
you need to work on.</p>
</blockquote>
<p>It's not only about growth, new environment can shape a person.</p>
<blockquote>
<p>Our social and cultural environments have a huge impact on our
thinking and behavior. In Infuencer, psychologist Joseph Grenny and his
coauthors explain that if you want to change behavior, you have to make
changes to the social and structural environment. In Atomic Habits,
James Clear argues that our environments usually matter more than our
motivation when it comes to building new habits: "Especially over a long
time period, your personal characteristics tend to get overpowered by
your environment."</p>
<p>You can either fight that truth or leverage it to learn more and grow
more. Eric Gleacher recognized the power of environment and how it could
not only offer new skills but also shape the person he would become at a
surprisingly young age.</p>
</blockquote></li>
<li><p>Have a look at what a getting-things-done talent looks like and
fill the gap. Although people all succeed in different ways and no one's
success is replicable, becoming a 'working genius' is at least a good
option to start.</p>
<blockquote>
<p>Invention: creating novel ideas or solutions</p>
<p>Discernment: evaluating and analyzing ideas and situations</p>
<p>Galvanizing: organizing and inspiring others to take action</p>
<p>Enablement: providing encouragement and assistance</p>
<p>Tenacity: pushing projects to completion</p>
</blockquote>
<blockquote>
<p>If you're wondering who you should turn to, always start with people
who have applied their ideas in the real world and can prove that they
work.</p>
<p>Next, ask, Will they actually fill my gaps, or will they hold back
their best ideas or try to elevate their ego by making what they know
seem complex and hard to understand? Will they make their knowledge
simple and clear? Essentially, you're asking, Is this person an active
learner? Because active learners love helping people fill their
gaps.</p>
<p>A final tip: if you want people to share their know-how with you, you
need to spread know-how. You need to be willing to share with them,
too.</p>
</blockquote></li>
<li><p>Human has instinct to avoid social pain or negative truth about
themselves, when someone tells a less positive truth, we need to fight
our instincts and always listen.</p>
<blockquote>
<p>When somebody cares enough and is brave enough to tell you the truth,
your best course of action is to fight your instincts to dismiss it or
hide from it. Overcome your brain's biological drive to protect you.
Shut down the voice in your head telling you they're wrong. Don't run
out of the room. Take some deep calming breaths (that really works),
remind yourself that this person probably has a good reason for bringing
the truth to your attention, and listen.</p>
<p>Active learners work through this set of mental gymnastics every day.
They work on their humility and maintaining an open mind (more on this
in part two because they see the value truth-tellers bring.</p>
</blockquote></li>
<li><p>Pursue the truth of the world, don't be delusional. Although
'<em>we see the world as we are, not as it is' (</em>Adaptation of Anaïs
Nin's famous quote), we at least should be aware of this.</p>
<blockquote>
<p>Andy Pearson: "Learn to see the world the way it really is, not how
you wish it to be."</p>
<p>Darrow: "Chase after the truth like all hell and you'll free
yourself, even though you never touch its coat tails."</p>
</blockquote>
<blockquote>
<p>In their book Decisive, Chip Heath and Dan Heath explained that a
sound decision-making process is more important than data and analysis,
because no matter what, that data or our analysis of it is often flawed.
We interpret it based on what we wish or what we assume or what we
think, not what is.</p>
<p>Good process can lead to better analysis, they explained, but
analysis without good process won't produce the best learning. You need
both to orient yourself to reality.</p>
</blockquote>
<blockquote>
<p>When you see the world the way it really is, the right action becomes
very clear.</p>
<p>One of the best ways to be a better critical thinker is to make sure
that your information is as close to the source as possible. If you
don't go to the source yourself, you might be letting one perception
after another influence what you end up hearing or learning. You won't
know if you're seeing reality.</p>
<p>When you're trying to see the world the way it really is, it's
important to not be blinded by good news, something a good process can
help you overcome.</p>
<p>A great way to stay grounded is to not only chase the truth but also
deal in it. Active learners know the value of being honest and
transparent. They tell it like it is, because they know when they do,
there's a greater chance others will, too.</p>
</blockquote></li>
<li><p>I love pattern thinking and I seek out actively, but I still
limit myself by a passive pursue of richer life experience.</p>
<blockquote>
<p>To prepare to make that leap, active learners expose themselves to as
many patterns from as many disciplines as they can. Being curious about
the world around us in the hope that we'll discover a new way of
thinking about a problem or a new way of seeing an opportunity is core
to active learning. Active learners read, listen, travel, try new
things, explore hobbies and interests. They explore trends and insights
from different disciplines, industries, cultures. Then they apply what
they've absorbed to problems or goals. Those habits have helped me come
up with some of my most successful ideas.</p>
<p>You might think of a pattern-thinking moment as an aha moment or a
stroke of inspiration, but active learners don't wait for the moment to
hit them; they work to find it.</p>
<p>Peter Georgescu, chairman emeritus of advertising giant Young &amp;
Rubicam and author of The Source of Success, said of pattern thinking,
"A creative solution is a leap, and that leap is supported and fed and
nurtured by experiences in life. The richer your life experience is, the
more creative you'll become."</p>
</blockquote></li>
<li><p>About reflection and thinking, the book elaborates two modes:
focus mode and diffese mode. It resonates with me as I do see the
benefits of switching between data science work during the day time and
freestyle dancing in the evening in terms of developing creative ideas
and unstuck myself from difficult problems.</p>
<blockquote>
<p>In her talk, she described two modes of thinking: focus mode and
diffuse mode. Focus mode is exactly what it sounds like. It's how we
think when we're trying to accomplish a task or memorize something. Our
thinking is usually confined to neural paths we've already created.
Diffuse mode is a more "relaxed set of neural states" that allows our
thinking to take off, range widely, and process or even create new
ideas. When we are learning, we need both. And when we feel stuck in our
thinking, unable to understand a concept, unable to unravel a challenge,
we especially need the diffuse mode.</p>
</blockquote></li>
<li><p>A combination of confidence and humility is a good
characteristic. I've never thought about them deeply as a combo, that's
why I've never found the sweet spot.</p>
<blockquote>
<p>Confidence is important because nobody will follow you unless they
believe you know where you're going and you'll find a way to get there.
If that confidence isn't tempered by humility, though, it becomes
arrogance.</p>
<p>Humility is just the recognition that you can't do it by yourself
whatever "it" is-either because you simply can't, because you don't know
enough, or because it won't be as fun or fulfilling if you go it
alone.</p>
<p>Confidence is simply the expectation that you'll find a way to
win-somehow.</p>
</blockquote></li>
<li><p>People have good side and bad side. If you believe in their good
side, they do so. From another perspective, it's often not their fault
if they choose to express the bad side.</p>
<blockquote>
<p>In any relationship, business or personal, somebody has to trust more
or trust first to break inertia and build up positive momentum.</p>
<p>As important as it is for us to trust in positive intentions, if we
want people to trust in ours, we need to behave accordingly. We need to
build a well of trust to draw on.</p>
<p>We're all human; we're all going to lose our tempers or handle a
delicate situation poorly or not show as much compassion as we should or
make a poor judgment call. When we're on the receiving end, if we can
take a breath, find a little empathy, and trust that the other person
has good intentions that didn't pan out, we can avoid a total breakdown
in the flow of ideas and learning and collaboration.</p>
</blockquote>
<blockquote>
<p>I read a striking definition of trust recently: "Trust is a
relationship of reliance." Aren't we all reliant on each other if we
want to learn, grow, and expand our possibilities? We can choose to
support that relationship or tear it down. If we choose the second
option, we're only limiting ourselves. If we choose the first, the
possibilities are infinite.</p>
</blockquote></li>
<li><p>This is from my experience: I only think hard, struggle and
learn, when I'm dealing with my own unique life path, I don't take time
to think when I follow other's path or live to other's expectation.</p>
<blockquote>
<p>You may know the quote often attributed to Oscar Wilde: "Be yourself;
everyone else is taken." (What he actually wrote is more cynical: "Most
people are other people. Their thoughts are someone else's opinions,
their lives a mimicry, their passions a quotation." Maybe because of my
background and the potential prejudgments that came with it, I've spent
most of my life working hard to just be me to understand who that person
is, the contributions I have to offer, what I believe, and my purpose
and passions. If I hadn't followed this path, I would have missed out on
so much learning.</p>
</blockquote>
<blockquote>
<p>Active learners know that it's hard to learn when your mental energy
is focused on trying to be somebody other than yourself. Instead of
being open and curious, you'll be defensive. You'll be putting up
barriers and withholding your brilliance. And then the people around you
will do the same. Most of us can sense when people aren't being
authentic, and it makes us trust them less.</p>
</blockquote>
<blockquote>
<p>Active learners like Marvin pursue authenticity by recognizing their
unique value and talents, figuring out what matters to them and why, and
then leveraging it to have a positive impact.</p>
<p>It's all about bringing who you are to the moment so that you're
comfortable and open-minded enough to learn important lessons and ideas
as they arise.</p>
</blockquote></li>
<li><p>Everyone knows we need do the right things, but when it comes
difficult situations, would you like to prioritize it above all
else?</p>
<blockquote>
<p>This is vital, because over time, depending on environments and
circumstances and your own choices, your sense of right and wrong can
suffer from stepwise degradation. You stray over the line, stray a bit
further the next time, justifying one bad action after another. Stray
too far over the line and you can lose sight of it entirely. Eventually,
you lose the ability to know what doing the right thing looks like.</p>
</blockquote>
<blockquote>
<p>The best thing that happens when we do the right thing is that we
feel good about our choices and the impact we're having on the world,
and that inspires us to keep doing the right thing. Values aren't some
thing you write down on a piece of paper and then put in a drawer or
hang on the wall. Values are something you use to take good action. It
isn't always the easy choice, but it's always the best choice and the
one that helps you learn the most powerful lessons.</p>
</blockquote></li>
<li><p>Input and output are different things. We collect information by
inputing knowledge from outside, and we make sense of those knowledge by
neural-networking it within our brain and outputing it in a little
different way which requires our logical, critical, and creative
thinkings.</p>
<blockquote>
<p>Two things happen in the brain that help us "learn what we know." One
is that we believe ideas more when we share them with others verbally,
especially if we're trying to convince others that they're true.
Psychologists call it the "saying is believing effect." Want to convince
yourself to make time to exercise three times a week? Try convincing
somebody else that they could fit a simple exercise regimen into their
schedule. Another is that speaking (and writing) brings a different part
of our brain into play than just thinking, which changes how we think
about an idea. It's one reason that we can struggle and struggle to come
up with a solution to a problem, but almost as soon as we explain the
problem to another person out loud, a good solution pops into our head.
Talking it out forces us to slow down, zoom out (simplify), and order
our thoughts.</p>
</blockquote>
<p>Sometimes, it's audience's engagement and support force us further
along the learning journey.</p>
<blockquote>
<p>I learned things I didn't know, and I learned what I already knew, as
Timo put it, as I analyzed leadership, considered it from different
angles, and expanded or supported my ideas. Active learners use this
process to codify their ideas into something digestible and easily
shared. When you codify it, you can scale it.</p>
<p>Teaching well also forces you to stay on top of your game, to
continually look for new material to keep your ideas current and
relevant. And it forces you to learn good storytelling, an invaluable
skill. Stories are stickier than almost any other kind of information.
If you want an idea to stay with people, you better be able to convey it
in a relevant, compelling story with emotion and tension.</p>
</blockquote></li>
<li><p>Many know "people go first", few know how to do it. If you want
them to care about what you care about, you need to care about them
first.</p>
<blockquote>
<p>Active learners understand that people-not knowledge or
results-should be the priority. How we support people, how we show our
gratitude for them, how we show our interest or concern for them has a
much greater impact, especially over time, than the latest quarterly
earnings or the latest market rankings. I've said it before: I really
like to win. But you don't win for long if the people who make the
winning possible don't know how much they count.</p>
</blockquote>
<blockquote>
<p>I have always admired Geoff Colvin, senior editor-at-large of Fortune
magazine and author of books like Talent Is Overrated and Humans Are
Underrated. When he joined me on my podcast, he described the kinds of
high-value work that only humans can do and that technology or AI can't:
empathy, collaboration, and the insights or learning we generate along
the way.</p>
</blockquote></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/05/10/2025-May/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/05/10/2025-May/" class="post-title-link" itemprop="url">2025 May - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-05-10 09:53:01" itemprop="dateCreated datePublished" datetime="2025-05-10T09:53:01-04:00">2025-05-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="substack">Substack</h2>
<blockquote>
<p><em>The greatest muscle you can build is urgency. Decrease the time
between having an idea and getting it done. Everything changes – Codie
Sanchez</em></p>
<p><em>You either chase your one big goal with everything you’ve got, or
nothing will happen. Trying to be balanced is what’s wrong with
society.</em></p>
<p><em>Success in any field comes from IMBALANCE.</em></p>
<p><em>Hard work only feels bad if it’s building someone else’s dream,
not yours.</em></p>
<p><em>The most important thing in your career: Speed.</em></p>
<p><em>If you answer emails fast, walk fast, talk fast, get sh*t done
fast, you will make a lot of money. No sense of urgency, you won't. Nick
Huber</em></p>
<p><strong>― The Most Successful People I Know Have a Psychopathic Sense
of Urgency - Unfiltered by Tim Denning</strong> [<a
target="_blank" rel="noopener" href="https://timdenning.substack.com/p/the-most-successful-people-i-know">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>UBER: Distribution is The King - Capitalist Letters</strong>
[<a
target="_blank" rel="noopener" href="https://www.capitalist-letters.com/p/uber-distribution-is-the-king">Link</a>]</p>
</blockquote>
<p>Great business with potential of continuous growth and expansion, and
cheap stock price.</p>
<p>Ecosystem is a huge advantage because it creates cross-platform
efficiencies.</p>
<figure>
<img src="/di-blog/2025/05/10/2025-May/uber_business_model.webp"
alt="uber_business_model" />
<figcaption aria-hidden="true">uber_business_model</figcaption>
</figure>
<p>The concept of network effect was first laid out in 1985, by Carl
Shapiro and Michael Katz in their seminal paper “<em>Network
Externalities, Competition, and Compatibility</em>.”</p>
<figure>
<img src="/di-blog/2025/05/10/2025-May/network_effect.webp"
alt="network_effect" />
<figcaption aria-hidden="true">network_effect</figcaption>
</figure>
<blockquote>
<p><em>According to Russ Harris, author of <a
target="_blank" rel="noopener" href="https://www.amazon.com/Happiness-Trap-Struggling-Start-Living/dp/1590305841?tag=thekerne-20">The
Happiness Trap</a>, values are “how we want to be, what we want to stand
for, and how we want to relate to the world around us.”</em></p>
<p><em>Values are attributes of the person we want to be.</em></p>
<p><strong>― How Successful People Timebox - Nir Eyal's
Substack</strong> [<a
target="_blank" rel="noopener" href="https://nir.substack.com/p/how-successful-people-timebox">Link</a>]</p>
</blockquote>
<p>Identify your values -&gt; turn values into time commitments -&gt;
create a timeboxed calendar -&gt; track distractions -&gt; reflect and
refine weekly. Do remember to schedule fun activities, use flexible
categories, and be aware that the goal isn't finishing tasks.</p>
<p>Some key ideas in the article backed by behaviorial science:</p>
<ul>
<li>According to <em>The Happiness Trap</em> by Russ Harris (Acceptance
and Commitment Therapy - ACT), productivity should align with personal
values (e.g., health, relationships, growth) rather than just task
completion.</li>
<li>People often ignore realistic time estimates in favor of optimistic
ones, leading to overpacked schedules and missed deadlines.</li>
</ul>
<blockquote>
<p><em>Especially in the Bay Area, the problem isn’t mediocrity—it’s
misdirected excellence. Kids under Chua’s parenting style rarely have a
choice in their own extracurriculars from elementary through high
school. (I doubt being vice-president of the National Honor Society is a
dream to most.) Sure, it can produce a passable overachiever who knows
how to get A’s. But to produce someone capable of real vision, high
agency, and contrarian thinking, the irony is that that overachiever may
be ill-prepared as we approach an era where AI handles rote tasks and
the knowledge economy demands more creativity.</em></p>
<p><strong>― How to Raise High-Agency Kids - Rebecca Wang</strong> [<a
target="_blank" rel="noopener" href="https://rbccawang.substack.com/p/how-to-raise-high-agency-kids">Link</a>]</p>
</blockquote>
<p>True excellence and future success come from fostering
agency—self-directed purpose, curiosity, and ownership—rather than
forcing kids to conform to hyper-competitive, checklist-driven
achievement cultures (like those common in the Bay Area).</p>
<p>What's the root problem? - <strong>misdirected
excellence</strong></p>
<p>What's the solution? - Give kids <strong>structure</strong>
(boundaries, values) but <strong>autonomy</strong> (freedom to pursue
interests).</p>
<blockquote>
<p><em>This isn’t about faking confidence. It’s about understanding the
low-pressure way to join a group.</em></p>
<p><em>Our ability to notice intricate details allows us to ask the
specific questions that make others feel truly seen.</em></p>
<p><em>In a world where everyone is clamoring to be heard, the ability
to observe and truly listen becomes your superpower.</em></p>
<p><em>Robert Greene's The 48 Laws of Power completed the picture with
"Never Outshine the Master", a lesson teaching the power of blending in
rather than disrupting. Don't announce your presence; become part of the
scenery, then contribute when appropriate.</em></p>
<p><strong>― The Spy Trick to Joining Any Conversation (Even If You're
Anxious) - AnifragileADHD</strong> [<a
target="_blank" rel="noopener" href="https://antifragileadhd.substack.com/p/the-spy-trick-to-joining-any-conversation">Link</a>]</p>
</blockquote>
<p>For neurodivergent individuals (ADHD, social anxiety, etc.),
socializing isn’t about performing—it’s about strategic observation and
gradual integration. This article is backed by psychology and behavioral
science.</p>
<p>Small tips:</p>
<ol type="1">
<li>Stand inside the group (not on edges) and listen silently at
first.</li>
<li>Linger quietly to blend into the social environment.</li>
<li>Wait for a group member to naturally include you.</li>
<li>Ask open-ended questions about others’ interests.</li>
<li>Sustain conversation with follow-up questions.</li>
</ol>
<h2 id="articles-and-blogs">Articles and Blogs</h2>
<blockquote>
<p><strong>Scientists discover quantum computing in the brain -
Brighter</strong> [<a
target="_blank" rel="noopener" href="https://www.thebrighterside.news/post/scientists-discover-quantum-computing-in-the-brain/">Link</a>]</p>
</blockquote>
<p>This research bridges quantum physics, biology, and information
theory, suggesting that <strong>life evolved to exploit quantum
mechanics</strong> for survival and intelligence. It challenges
reductionist views of biology and could redefine our understanding of
consciousness, disease, and even the origins of life.</p>
<blockquote>
<p><strong>Here are the 19 US AI startups that have raised $100M or more
in 2025 - TechCrunch</strong> [<a
target="_blank" rel="noopener" href="https://techcrunch.com/2025/04/23/here-are-the-19-us-ai-startups-that-have-raised-100m-or-more-in-2025/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Just as “internet” evolved from buzzword to business backbone, AI
is following the same playbook.</em></p>
<p><strong>― In 2025, venture capital can’t pretend everything is fine
any more - Pivot to AI</strong> [<a
target="_blank" rel="noopener" href="https://pivot-to-ai.com/2025/05/03/in-2025-venture-capital-cant-pretend-everything-is-fine-any-more/">Link</a>]</p>
</blockquote>
<p>Venture capital in 2025 is a dying industry clinging to AI as its
last hope, with most investment funneled into OpenAI and a few other
hyped players while the rest of the startup ecosystem collapses. The
sector, which thrived on zero-interest-rate euphoria, now faces a harsh
reality: no exits, frozen IPOs, and a market unwilling to fund
early-stage ventures. VCs blame Trump’s chaotic tariffs—despite many
having supported him—but the real issue is their own inability to adapt
to a normal economy. The NVCA report offers no solutions, just desperate
optimism, as the industry’s leaders—many of whom lucked into
success—flail in ideological fringe movements and pray for a miracle.
The only remaining question is whether AI will keep the bubble inflated
long enough for them to cash out before it all implodes.</p>
<blockquote>
<p><strong>The walled garden cracks: Nadella bets Microsoft’s
Copilots—and Azure’s next act—on A2A/MCP interoperability -
VentureBeat</strong> [<a
target="_blank" rel="noopener" href="https://venturebeat.com/ai/the-walled-garden-cracks-nadella-bets-microsofts-copilots-and-azures-next-act-on-a2a-mcp-interoperability/">Link</a>]</p>
</blockquote>
<p>Nadella’s endorsement signals Microsoft’s commitment to open
protocols over proprietary ecosystems, aligning with his long-standing
advocacy for interoperability (e.g., ONNX, GitHub’s multi-model
approach). By backing A2A (agent-to-agent communication) and MCP
(model-data context standardization), Microsoft ensures Copilot,
Foundry, and Azure AI can seamlessly integrate with third-party AI
agents and tools. This move preempts enterprise concerns about vendor
lock-in, a criticism of past Microsoft products.</p>
<blockquote>
<p><strong>Car Companies Are In A Billion-Dollar Software War, And
Everyone's Losing - InsideEVs</strong> [<a
target="_blank" rel="noopener" href="https://insideevs.com/features/759153/car-companies-software-companies/">Link</a>]</p>
</blockquote>
<p>why it's so hard to shift from lagacy automaker to SDV (software
designed vehicle) company?</p>
<ul>
<li>Cultural shift: Legacy automakers treated software as an
afterthought, not a core product. Now, they must adopt a Silicon
Valley-like approach.</li>
<li>Supplier dependence: Traditional automakers rely on suppliers for
ECUs, creating a tangled web of software layers.</li>
<li>Safety vs. agility: They must balance "move fast and break things"
with "zero defects or recalls."</li>
<li>Hybrid challenges: Slowing EV demand means SDV systems must also
work with internal-combustion vehicles, complicating power and update
logistics.</li>
</ul>
<p>Legacy automakers must become software companies to survive, but the
transition is painfully slow and expensive. The winners will be those
who can blend Silicon Valley speed with automotive-grade
reliability—something no traditional automaker has fully achieved
yet.</p>
<blockquote>
<p><strong>8 Reasons Leadership Is Hard And Why Few Are Prepared To Lead
- Forbes</strong> [<a
target="_blank" rel="noopener" href="https://www.forbes.com/sites/glennllopis/2025/05/05/8-reasons-leadership-is-hard-and-why-few-are-prepared-to-lead/">Link</a>]</p>
</blockquote>
<p>The most inspiring leaders today aren’t just adapting—they’re
<strong>rewriting the rules</strong>. Leadership isn’t a pinnacle; it’s
a <strong>daily practice of courage and reinvention</strong>. The world
doesn’t need more bosses; it needs <strong>architects of
possibility</strong>.</p>
<p><strong>Summary</strong>:</p>
<ol type="1">
<li>The Myth of the Omniscient Leader</li>
</ol>
<p><strong>Shift: From "knowing it all" to curiosity-driven
collaboration.</strong></p>
<ul>
<li>Action: Adopt a "Learn It All" mindset (Microsoft’s Satya Nadella
famously replaced "Know It All" with this).<br />
</li>
<li>Tool: Host "No Answers Meetings" where leaders openly discuss
unsolved problems, inviting teams to co-create solutions. Example:
Google’s "20% Time" empowers employees to explore innovations beyond
their core roles, democratizing problem-solving.</li>
</ul>
<ol start="2" type="1">
<li>Embracing the Illusion of Control</li>
</ol>
<p><strong>Shift: From command-and-control to adaptive
stewardship.</strong></p>
<ul>
<li>Action: Practice "Scenario Planning" (like Shell Oil’s famed
strategy) to prepare for multiple futures, not just one.<br />
</li>
<li>Mindset: View volatility as a laboratory for innovation. Spotify’s
"Fail Fast, Learn Fast" approach rewards experimentation.<br />
</li>
<li>Quote: <em>"The art of leadership is not to control, but to
unleash."</em> — Reed Hastings, Netflix.</li>
</ul>
<ol start="3" type="1">
<li>The Leadership Pipeline Crisis</li>
</ol>
<p><strong>Root Cause: Short-term efficiency has gutted long-term talent
development.</strong></p>
<ul>
<li>Fix: Reverse Mentorship Programs (e.g., GE’s junior employees mentor
execs on digital trends).<br />
</li>
<li>Metric: Track "Readiness Ratios"—how many high-potentials are
prepared for next-level roles?<br />
</li>
<li>Warning: Deloitte’s research shows 89% of executives see "weak
leadership benches" as their top threat.</li>
</ul>
<ol start="4" type="1">
<li>The Death of Cookie-Cutter Playbooks</li>
</ol>
<p><strong>New Rule: Context over conformity.</strong></p>
<ul>
<li>Action: Build "Modular Strategies"—flexible frameworks adjusted in
real-time (like Amazon’s "Working Backwards" method).<br />
</li>
<li>Tool: Use "Pre-Mortems" (anticipating failures before launch) to
stress-test strategies. Example: Blockbuster’s rigid playbook failed,
while Netflix’s pivot to streaming embraced uncertainty.</li>
</ul>
<ol start="5" type="1">
<li>Respect as a Daily Earned Currency</li>
</ol>
<p><strong>Key: Authenticity &gt; Authority.</strong></p>
<ul>
<li>Action: Practice "Radical Transparency" (like Bridgewater
Associates’ culture of brutal honesty).<br />
</li>
<li>Tool: Replace "All Hands Meetings" with "All Hearts Meetings"—forums
for empathy and vulnerability. Example: Edelman’s Trust Barometer shows
employees trust "a peer like me" 3x more than CEOs.</li>
</ul>
<ol start="6" type="1">
<li>Rebuilding Trust in Judgment</li>
</ol>
<p><strong>Antidote: Inclusive Decision-Making.</strong></p>
<ul>
<li>Action: Form "Shadow Boards" (e.g., Gucci’s millennial council
advising execs).<br />
</li>
<li>Rule: For major decisions, require "Disagree &amp; Commit" (document
dissent but align once decided). Example: Patagonia’s CEO involves
employees in sustainability bets, building trust through shared
stakes.</li>
</ul>
<ol start="7" type="1">
<li>Titles vs. Influence</li>
</ol>
<p><strong>New Power Model: Fluid Hierarchies.</strong></p>
<ul>
<li>Action: Adopt "Holacracy Lite" (like Zappos’ role-based authority,
not title-based).<br />
</li>
<li>Symbolic Step: Drop "CEO" for "Chief Enabler" (as some startups do
to signal servant leadership).<br />
</li>
<li>Stat: 72% of Gen Z workers prefer "Project Leaders" over "Managers"
(McKinsey, 2024).</li>
</ul>
<ol start="8" type="1">
<li>The Reinvention Imperative</li>
</ol>
<p><strong>Framework: "Learn, Unlearn, Relearn" (Alvin Toffler’s
future-proofing mantra).</strong></p>
<ul>
<li>Tool: "Skills Gap Heatmaps"—quarterly self-assessments on emerging
competencies (e.g., AI literacy).<br />
</li>
<li>Example: Adobe’s "Kickbox" program gives employees $1,000 to test
new ideas, forcing leaders to adapt.</li>
</ul>
<p><strong>The Path Forward: Leadership as a Dynamic
Practice</strong></p>
<p>Your closing question—<em>"Will you be one of them?"</em>—is the call
to action. Leaders who thrive will:<br />
1. Lead with Questions, not answers.<br />
2. Treat Trust as Currency, not a given.<br />
3. Build Antifragile Teams (Nassim Taleb’s concept of growing stronger
through chaos).<br />
4. Measure Success in Learning Cycles, not quarterly profits alone.</p>
<blockquote>
<p><strong>Microsoft Follows Competitors Amazon, Meta, and Google in
Employee Productivity Crackdown</strong> [<a
target="_blank" rel="noopener" href="https://www.inc.com/bruce-crumley/microsoft-follows-competitors-amazon-meta-and-google-in-employee-productivity-crackdown/91186637">Link</a>]</p>
</blockquote>
<p>The pandemic hiring spree, rising interest rates, and the AI arms
race have forced tech giants to abandon the "growth at all costs"
mindset. Instead, they’re:</p>
<ul>
<li><strong>Prioritizing speed</strong> (fewer managers = faster
decisions)</li>
<li><strong>Maximizing output per employee</strong> (via stack ranking
and attrition policies)</li>
<li><strong>Investing savings into AI</strong> (where Microsoft is
battling Google and OpenAI)</li>
</ul>
<blockquote>
<p><strong>Master The Psychology Of Building An Unforgettable Personal
Brand - Forbes</strong> [<a
target="_blank" rel="noopener" href="https://www.forbes.com/sites/jodiecook/2025/05/07/master-the-psychology-of-building-an-unforgettable-personal-brand/">Link</a>]</p>
</blockquote>
<p>When your brand is rooted in internal conviction, it radiates
effortlessly. The right opportunities find you.</p>
<p>"My worth isn’t measured by likes; it’s measured by impact."</p>
<p>"If they don’t buy, it’s not a rejection—it’s a mismatch."</p>
<p>"Outcomes are data, not identity."</p>
<p>"Consistency today compounds into authority tomorrow."</p>
<blockquote>
<p><strong>Zero to One: Learning Agentic Patterns - Philschmid</strong>
[<a
target="_blank" rel="noopener" href="https://www.philschmid.de/agentic-pattern?utm_source=alphasignal">Link</a>]</p>
</blockquote>
<p>This guide explores techniques such as prompt chaining, routing,
parallelization, reflection, tool integration, planning, and multi-agent
collaboration. It features practical code examples for each pattern,
enabling the development of efficient, context-aware workflows with
Google DeepMind Gemini. Emphasis is placed on structured strategies to
enhance task delegation and agent coordination.</p>
<blockquote>
<p><em>Our research shows that by 2030, data centers are projected to
require <span class="math inline">\(\text{\$6.7}\)</span> trillion
worldwide to keep pace with the demand for compute power. Data centers
equipped to handle AI processing loads are projected to require <span
class="math inline">\(\$5.2\)</span> trillion in capital expenditures,
while those powering traditional IT applications are projected to
require <span class="math inline">\(\$1.5\)</span> trillion in capital
expenditures (see sidebar “What about non-AI workloads?”). Overall,
that’s nearly <span class="math inline">\(\$7\)</span> trillion in
capital outlays needed by 2030—a staggering number by any
measure.</em></p>
<p><em>To qualify our <span class="math inline">\(\$5.2\)</span>
trillion investment forecast for AI infrastructure, it’s important to
note that our analysis likely undercounts the total capital investment
needed, as our estimate quantifies capital investment for only three out
of five compute power investor archetypes—<strong>builders, energizers,
and technology developers and designers</strong>—that directly finance
the infrastructure and foundational technologies necessary for AI growth
(see sidebar “Five types of data center investors”). Approximately 15
percent (<span class="math inline">\(\$0.8\)</span> trillion) of
investment will flow to builders for land, materials, and site
development. Another 25 percent (<span
class="math inline">\(\$1.3\)</span> trillion) will be allocated to
energizers for power generation and transmission, cooling, and
electrical equipment. The largest share of investment, 60 percent (<span
class="math inline">\(\$3.1\)</span> trillion), will go to technology
developers and designers, which produce chips and computing hardware for
data centers. The other two investor archetypes,
<strong>operators</strong>, such as hyperscalers and colocation
providers, and <strong>AI architects</strong>, which build AI models and
applications, also invest in compute power, particularly in areas such
as AI-driven automation and data center software. But quantifying their
compute power investment is challenging because it overlaps with their
broader R&amp;D spending.</em></p>
<p><strong>― The cost of compute: A $7 trillion race to scale data
centers - McKinsey</strong> [<a
target="_blank" rel="noopener" href="https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-cost-of-compute-a-7-trillion-dollar-race-to-scale-data-centers#/">Link</a>]</p>
</blockquote>
<figure>
<img
src="/di-blog/2025/05/10/2025-May/est_global_data_center_cap_demand.svg"
alt="est_global_data_center_cap_demand" />
<figcaption
aria-hidden="true">est_global_data_center_cap_demand</figcaption>
</figure>
<blockquote>
<p><strong>The Comfortable Life is Killing You - Poetic Outlaws</strong>
[<a
target="_blank" rel="noopener" href="https://poeticoutlaws.substack.com/p/the-comfortable-life-is-killing-you">Link</a>]</p>
</blockquote>
<p>Meaning is forged in resistance - Meaning is a byproduct of
engagement with resistance. Joy emerges when we meet challenges worthy
of our souls. To paraphrase Camus: <em>The struggle itself is
enough</em>.</p>
<blockquote>
<p><strong>Agentic AI Is Already Changing the Workforce - Harvard
Business Review</strong> [<a
target="_blank" rel="noopener" href="https://hbr.org/2025/05/agentic-ai-is-already-changing-the-workforce">Link</a>]</p>
</blockquote>
<h2 id="papers-and-reports">Papers and Reports</h2>
<blockquote>
<p><strong>The power of one: How standout firms grow national
productivity - McKinsey Global Institute</strong> [<a
target="_blank" rel="noopener" href="https://www.mckinsey.com/mgi/our-research/the-power-of-one-how-standout-firms-grow-national-productivity">Link</a>]</p>
</blockquote>
<p>Productivity growth is crucial for economic prosperity. The report
suggests that instead of waiting for all firms to improve, targeted
support for high-potential firms could accelerate national productivity
gains.</p>
<blockquote>
<p><strong>Identifying and scaling AI   use cases - OpenAI</strong> [<a
target="_blank" rel="noopener" href="https://cdn.openai.com/business-guides-and-resources/identifying-and-scaling-ai-use-cases.pdf">Link</a>]</p>
</blockquote>
<p>OpenAI ads, but useful for pitching GenAI use cases. It offers
guidance on identifying and scaling AI use cases within organizations,
noting that AI adoption is rapidly increasing and demonstrating
significant benefits for early adopters. It emphasizes three key steps
for businesses: understanding where AI can add value by focusing on
repetitive tasks, skill bottlenecks, and navigating ambiguity; teaching
teams fundamental AI use cases like content creation, research, and
automation; and prioritizing opportunities using an impact/effort
framework to determine which projects to pursue and scale.</p>
<blockquote>
<p><strong>ZeroSearch: Incentivize the Search Capability of LLMs without
Searching - Alibaba Group</strong> [<a
target="_blank" rel="noopener" href="https://alibaba-nlp.github.io/ZeroSearch/">Link</a>]</p>
</blockquote>
<p>Traditional RL training requires massive API calls to services like
Google Search, costing hundreds of thousands of dollars. ZeroSearch
replaces this with a <strong>simulated search environment</strong> where
the LLM itself generates both relevant and irrelevant documents in
response to queries.</p>
<p>Real search engines return unpredictable results, complicating
training. While ZeroSearch uses <strong>curriculum-based
rollouts</strong>, gradually degrading document quality to teach the
model to discern useful information.</p>
<p>It has a cost reduction up to 88% and its performance surpasses real
search engines.</p>
<blockquote>
<p><strong>AI Global, Global Sector Trends on Generative AI</strong> [<a
target="_blank" rel="noopener" href="https://www.similarweb.com/corp/wp-content/uploads/2025/04/attachment-Global-AI-Tracker-13.pdf?utm_medium=social&amp;utm_source=twit">Link</a>]</p>
<p><strong>Gen AI Traffic Share update - Similarweb <span
class="citation" data-cites="Twitter">@Twitter</span></strong> [<a
target="_blank" rel="noopener" href="https://x.com/Similarweb/status/1920026287625658628">Link</a>]</p>
</blockquote>
<p>Subdomains and pages only (below).</p>
<figure>
<img src="/di-blog/2025/05/10/2025-May/genai_traffic_share.png"
alt="genai_traffic_share" />
<figcaption aria-hidden="true">genai_traffic_share</figcaption>
</figure>
<h2 id="youtube-and-podcasts">YouTube and Podcasts</h2>
<blockquote>
<p><strong>Fed Hesitates on Tariffs, The New Mag 7, Death of VC,
Google's Value in a Post-Search World - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=g6HSFCQQ6O0">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The Physical Turing Test: Jim Fan on Nvidia's Roadmap for
Embodied AI - Sequoia Capital</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_2NijXqBESI">Link</a>]</p>
</blockquote>
<p>This lecture introduces the <em>Physical Turing Test</em>, a new
benchmark for robotics. Jim Fan from NVIDIA breaks down why solving this
is hard—and what tools researchers are using to make progress.</p>
<blockquote>
<p><strong>5 Types of AI Agents: Autonomous Functions &amp; Real-World
Applications - IBM Technology</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=fXizBc03D7E">Link</a>]</p>
</blockquote>
<p>This lecture covers reflex agents, model-based agents, goal-based
systems, utility-based frameworks, and learning agents.</p>
<blockquote>
<p><strong>Stanford Webinar - Agentic AI: A Progression of Language
Model Usage - Stanford Online</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=kJLiOGle3Lw">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How to connect AI agents to third-party tools using MCP -
Underfitted</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=dgC1FTKDS5k">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Llamacon 2025 - Conversation with Mark Zuckerberg and Satya
Nadella - Meta Developers</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=WaJOONFllLc">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>关税大棒下的苹果：一场全球供应链的迁徙风暴 - 硅谷101</strong>
[<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_f2c0eZ-N7M">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>E191｜小而美的机会来了，聊聊这轮AI Agent进化新范式 -
硅谷101</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=JEXUta6_oaQ">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Sundar Pichai, CEO of Alphabet | The All-In
Interview</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ReGC2GtWFp4&amp;t=226s">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Trump's Big Week: Middle East Trip, China Deal, Pharma EO,
"Big, Beautiful Bill" with Ben Shapiro - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=hyxBMx4w5jg&amp;t=3933s">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>枪声背后的信任危机：“病不起”的美国人 - 硅谷101</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=h8FTJxGu_hA">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Bond crisis looming? GOP abandons DOGE, Google disrupts
Search with AI, OpenAI buys Jony Ive's IO - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=GEZWyC-jJa4">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Microsoft Build 2025 | Satya Nadella Opening Keynote -
Microsoft</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ceV3RsG946s">Link</a>]</p>
</blockquote>
<p>Exciting new products - copilot studio, foundry local, microsoft
discovery, etc!</p>
<blockquote>
<p><strong>Google I/O '25 Keynote - Google</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=o8NiE3XMPrM">Link</a>]</p>
</blockquote>
<p>AI mode finally - Smart move to embrace next-gen search. Android XR
glass is launching, and Gentle Monster + Warby Parker will be the first
eyewear partners. Genimi App has Agent mode is coming. And many
more!</p>
<blockquote>
<p><strong>NVIDIA CEO Jensen Huang Keynote at COMPUTEX 2025 -
NVIDIA</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=TLzna9__DnI">Link</a>]</p>
</blockquote>
<p>NVLink Fusion, DGX Spark AI Computer, DGX Station Super Computer, FTX
Pro Server, AI Robotics, etc.</p>
<blockquote>
<p><em>I'll tell you my hiring experience. We have about 30 people at
8090 and the way that I have found it to work the best is you have
senior people act as mentors and then you have an overwhelming corpus of
young very talented people who are AI native. And if you don't find that
mix, what you have instead are L7s from Google and Amazon and Meta who
come to you with extremely high salary demands and stock demands and
they just don't thrive. And part of why they don't thrive is that they
push back on the tools and how you use them. They push back on all these
things that the tools help you get to it faster. M this is why I think
it's so important for the young folks to just jump in with two feet and
be AI native from the jump because you're much more hirable frankly to
the to the emergent company and the bigger companies you'll have a lot
of these folks that see the writing on the wall may not want to adapt as
fast as otherwise. Another way for example that you can measure this is
if you look inside your company on the productivity lift of some of
these coding assistants for people as a distribution of age. What you'll
see is the younger people leverage it way more and have way more
productivity than older folks. And I'm not saying that as an aegis
comment. I'm saying that it's an actual reflection of how people are
reacting to these tools. What you're describing is a paradigm shift. It
is a big leap. Is you know it's like when I went to college, when I took
computer science, it was object-oriented programming. It was like C++.
It was compiled languages. It was gnarly. It was nasty work. And then
you had these highle abstracted languages. And I used to remember at
Facebook, I would just get so annoyed because I was like, why is
everybody using PHP and Python? This is like not even real. But I was
one of these old lights who didn't understand that I just had to take
the leap. And what it did was it grew the top of the funnel of the
number of developers by 10x. And as a result, what you had were all of
these advancements for the internet. And I think what's happening right
now is akin to the same thing where you're going to grow the number of
developers upstream by 10x. But in order to embrace that, you just have
to jump in with two feet. And if you're very rigid in how you think the
job should be done technically, I think you're just going to get left
behind. - Chamath Palihapitiya</em></p>
<p><strong>― AI Doom vs Boom, EA Cult Returns, BBB Upside, US Steel and
Golden Votes - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=O_AfZ6J0ToE&amp;t=2446s">Link</a>]</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/04/01/2025-April/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/04/01/2025-April/" class="post-title-link" itemprop="url">2025 April - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-04-01 10:27:12" itemprop="dateCreated datePublished" datetime="2025-04-01T10:27:12-04:00">2025-04-01</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="substack">Substack</h2>
<blockquote>
<p><em>A Parquet file is composed of Row Groups, Column Chunk, and
Pages.</em></p>
<p><em>Parquet is a self-described file format that contains all the
information needed for the application that consumes the file. This
allows the software to efficiently understand and process the file
without requiring external information. Thus, the metadata is the
crucial part of Parquet. They include Magic Number, FileMetadata, and
PageHeader.</em></p>
<p><em>Google Dremel (the query engine behind BigQuery) inspired
Parquet’s approach to implementing nested and repeated field storage. In
a <a
target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf">2010
paper</a> introducing Dremel, Google detailed its method for efficiently
handling nested and repeated fields in analytics workloads using
definition level (for nested fields) and repetition level (for
array-like fields). I wrote an article about this approach seven months
ago; you can read it here:</em></p>
<p><strong>― I spent 8 hours learning Parquet. Here’s what I discovered
- Vu Trinh</strong> [<a
target="_blank" rel="noopener" href="https://vutr.substack.com/p/the-overview-of-parquet-file-format">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>The overall BigQuery architecture includes independent components
for query execution, storage, a container management system, and a
shuffler service:</em></p>
<ul>
<li><em><strong>Colossus</strong>: A distributed storage system that
holds and stores data.</em></li>
<li><em><strong>Dremel</strong>: The distributed query engine.</em></li>
<li><em><strong>Borg is</strong> Google’s large-scale cluster management
system that can reliably manage and orchestrate compute resources. (<a
target="_blank" rel="noopener" href="https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/">Borg
is the predecessor of Kubernetes.</a>) We will return to Borg when
discussing the Vortex architecture.</em></li>
<li><em><strong>Dedicate shuffle service</strong>: Dremel was inspired
by the map-reduce paradigm to operate and manage the data shuffle
between stages efficiently; Google built a separate shuffle service on
top of disaggregated distributed memory. This service backs BigQuery and
supports other services, such as <a
target="_blank" rel="noopener" href="https://cloud.google.com/products/dataflow?hl=en">Google
Dataflow</a>.</em></li>
</ul>
<p><strong>― I spent 4 hours learning the architecture of BigQuery's
storage engine - Vu Trinh</strong> [<a
target="_blank" rel="noopener" href="https://vutr.substack.com/p/i-spent-4-hours-learning-the-architecture">Link</a>]</p>
</blockquote>
<blockquote>
<ol type="1">
<li><em><strong>Extract</strong>: The process’s first step is
extraction. The needed data is gathered from various sources, such as
relational databases or third-party APIs</em></li>
<li><em><strong>Transform</strong>: Extracted data undergoes many
potential transformations, including cleaning, filtering, combining from
different sources, and formatting to conform to a target
schema.</em></li>
<li><em><strong>Load</strong>: The transformed data is loaded into the
destination with the predefined schema and constrained.</em></li>
</ol>
<p><em>ELT solves many of the problems associated with ETL.</em></p>
<p><em>Most transformation logic can now be handled within the data
warehouse using SQL, making it more accessible for users such as data
analysts or data scientists. This eliminates the potential performance
bottleneck of ETL pipelines.</em></p>
<p><em>Most importantly, ELT allows you to keep raw data in the
warehouse. This approach offers several advantages. You don’t need to
plan transformation logic in advance; instead, the logic can evolve over
time based on analytical needs—an especially valuable benefit in today’s
agile software development environment.</em></p>
<p><strong>― ETL and ELT - Vu Trinh</strong> [<a
target="_blank" rel="noopener" href="https://vutr.substack.com/p/etl-and-elt">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Apache Airflow Overview - Vu Trinh</strong> [<a
target="_blank" rel="noopener" href="https://vutr.substack.com/p/apache-airflow-overview">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How did Airbnb build their semantic layer? - Vu
Trinh</strong> [<a
target="_blank" rel="noopener" href="https://vutr.substack.com/p/how-did-airbnb-build-their-semantic">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>In a meltdown, discipline beats brilliance.</em></p>
<p><em>Over the years, I’ve found that having a simple rule-based system
helps me stay grounded. Here are the 4 rules I follow to protect my
portfolio:</em></p>
<ul>
<li><em>I invest a fixed amount monthly — rain or shine.</em></li>
<li><em>I don’t add to losers — keeping them relatively small.</em></li>
<li><em>I don’t sell winners — staying the course and being
patient.</em></li>
<li><em>I invest for at least 5 years — to give compounding time to
work.</em></li>
</ul>
<p><strong>― Bear Market Survival Guide - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/bear-market-survival-guide">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Salesforce &amp; AI Strategy - Generative Value</strong> [<a
target="_blank" rel="noopener" href="https://www.generativevalue.com/p/salesforce-and-ai-strategy">Link</a>]</p>
</blockquote>
<p>This article discusses the history of Salesforce, what made it
successful, the state of the business, and the AI opportunity (or
threat) today.</p>
<blockquote>
<p><strong>Everything Wrong with MCP - Shrivu's Substack</strong> [<a
target="_blank" rel="noopener" href="https://blog.sshh.io/p/everything-wrong-with-mcp">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How to future-proof your career in the age of AI - Operator's
Handbook</strong> [<a
target="_blank" rel="noopener" href="https://www.operatorshandbook.com/p/how-to-future-proof-your-career-in">Link</a>]</p>
</blockquote>
<p><strong>Key Takeaways:</strong></p>
<p>The author’s call to "lean into human strengths while actively
engaging with AI" is a compelling middle path. The essay underscores
that the future belongs to those who combine AI literacy with
irreplaceable human skills—judgment, influence, and adaptability.</p>
<p><strong>Human Competitive Advantages</strong>:</p>
<ul>
<li><strong>Judgment &amp; Conviction</strong>: Ability to make
decisions with incomplete/ambiguous data. Distinguishing impactful work
from "interesting but useless" projects. Simplifying complexity into
actionable frameworks.</li>
<li><strong>Influence &amp; Execution</strong>: Navigating
organizational politics and incentives. Building trust and adoption for
AI-driven outputs. Understanding unspoken processes and
relationships.</li>
</ul>
<p><strong>Actionable Skills to Cultivate</strong>:</p>
<ul>
<li>Develop "taste" by studying excellence in your field.</li>
<li>Gain hands-on experience to pressure-test AI outputs.</li>
<li>Learn to align stakeholders and drive consensus.</li>
<li>Build strong interpersonal relationships and reputation.</li>
</ul>
<p><strong>Adaptability as the Ultimate Skill</strong>:</p>
<ul>
<li>AI will keep evolving, so continuous learning and flexibility are
critical.</li>
<li>Focus on areas where humans add unique value (judgment, influence,
creativity).</li>
</ul>
<p>This is a very interesting point: "Develop "taste" by studying
excellence in your field."</p>
<p>Just like any skill, taste sharpens with exposure and effort. The
more you study, critique, and create, the better you’ll get at
recognizing—and producing—excellence. In a world flooded with
AI-generated content, the people who thrive will be those who can
separate the remarkable from the mediocre.</p>
<h2 id="blogs-and-articles">Blogs and Articles</h2>
<blockquote>
<p><strong>How Airbnb Standardized Metric Computation at Scale - Airbnb
Blog</strong> [<a
target="_blank" rel="noopener" href="https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Digital hygiene - Andrej Karpathy</strong> [<a
target="_blank" rel="noopener" href="https://karpathy.bearblog.dev/digital-hygiene/">Link</a>]</p>
</blockquote>
<p>Good tips and tricks for digital hygiene, given the pervasive nature
of internet fraud and the data collection practices of major tech
companies.</p>
<blockquote>
<p><strong>Measuring AI Ability to Complete Long Tasks - METR</strong>
[<a
target="_blank" rel="noopener" href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">Link</a>]</p>
</blockquote>
<figure>
<img src="/di-blog/2025/04/01/2025-April/metr-length-of-tasks-log.png"
alt="metr-length-of-tasks-log" />
<figcaption aria-hidden="true">metr-length-of-tasks-log</figcaption>
</figure>
<blockquote>
<p><strong>The "think" tool: Enabling Claude to stop and think in
complex tool use situations - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/engineering/claude-think-tool">Link</a>]</p>
</blockquote>
<p>Anthropic introduces a "think" tool designed to enhance Claude's
complex problem-solving by providing a dedicated space for structured
reasoning during tasks. This tool differs from extended thinking by
allowing Claude to pause and consider necessary information
mid-response, particularly beneficial for multi-step processes and tool
use. Evaluations on benchmarks like τ-Bench demonstrated significant
performance improvements, especially in policy-heavy domains like
airline customer service, where optimized prompting alongside the
"think" tool proved most effective.</p>
<blockquote>
<p><strong>Tiny Agents: a MCP-powered agent in 50 lines of code -
HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/blog/tiny-agents">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Anthropic CEO wants to open the black box of AI models by
2027 - Techcrunch</strong> [<a
target="_blank" rel="noopener" href="https://techcrunch.com/2025/04/24/anthropic-ceo-wants-to-open-the-black-box-of-ai-models-by-2027/?guccounter=1">Link</a>]</p>
<p><em>Powerful AI will shape humanity’s destiny, and we deserve to
understand our own creations before they radically transform our
economy, our lives, and our future.</em></p>
<p><strong>― The Urgency of Interpretability - Dario Amodei</strong> [<a
target="_blank" rel="noopener" href="https://www.darioamodei.com/post/the-urgency-of-interpretability">Link</a>]</p>
</blockquote>
<p>Interpretability isn’t just academic—it’s a prerequisite for safe,
controllable AI. The window to solve it is narrowing as AI grows more
powerful. By steering resources toward this goal now, we might avoid a
future where humanity builds systems it doesn’t understand but can’t
afford to stop.</p>
<blockquote>
<p><strong>The Jobs That Will Fall First As AI Takes Over The Workplace
- Forbes</strong> [<a
target="_blank" rel="noopener" href="https://www.forbes.com/sites/jackkelly/2025/04/25/the-jobs-that-will-fall-first-as-ai-takes-over-the-workplace/">Link</a>]</p>
</blockquote>
<p><strong>Takeaways:</strong></p>
<ol type="1">
<li>Timeline for Disruption:
<ul>
<li>By 2030: 30% of U.S. jobs could be automated (McKinsey).</li>
<li>By 2035: White-collar restructuring in finance, legal, and media
(Larry Fink, Jamie Dimon).</li>
<li>By 2045: 50% of jobs may be fully automated (Goldman Sachs).</li>
<li>By 2050: AI could dominate 60-80% of jobs, depending on innovation
pace.</li>
</ul></li>
<li>Most Vulnerable Jobs (Near-Term):
<ul>
<li>Administrative: Data entry, scheduling, customer service (60%
automatable, per IPPR).</li>
<li>Finance &amp; Legal: Bookkeeping, contract drafting, paralegal work
(AI tools like Harvey already achieve 90% accuracy).</li>
<li>Creative &amp; Media: Basic graphic design, copywriting, journalism
(30% at risk by 2035, Pew Research).</li>
<li>Routine STEM Tasks: Coding, data analysis (40% automatable by 2040,
WEF).</li>
</ul></li>
<li>More Resilient Jobs (Longer-Term):
<ul>
<li>Healthcare: Nursing, therapy, and patient care (empathy-driven
roles).</li>
<li>Skilled Trades: Construction, repair, maintenance (physical labor is
harder to automate).</li>
<li>Education &amp; Leadership: Teaching, high-level management
(requires emotional intelligence).</li>
</ul></li>
</ol>
<p><strong>To protect career:</strong></p>
<ul>
<li>Focus on critical thinking, creativity, and AI collaboration (e.g.,
prompt engineering, AI-augmented decision-making).</li>
<li>Target Resilient Sectors- Healthcare, education, skilled trades, and
AI-adjacent roles (e.g., cybersecurity, AI ethics).</li>
<li>Push for employer or government-sponsored programs to transition
into hybrid (human + AI) roles.</li>
<li>Embrace Hybrid Roles- Jobs that combine technical skills with human
judgment (e.g., AI-assisted healthcare diagnostics) will thrive.</li>
</ul>
<p>As Ray Dalio warns, the economy faces a "great deleveraging" where AI
disrupts jobs faster than new ones emerge. The key is
<strong>adaptability</strong>—those who proactively reinvent their
skills today will shape the workforce of tomorrow.</p>
<blockquote>
<p><strong>Curation is the new leadership superpower. Here are 3 ways to
adopt a curation mindset - FastCompany</strong> [<a
target="_blank" rel="noopener" href="https://www.fastcompany.com/91322752/curation-is-the-new-leadership-superpower-here-are-3-ways-to-adopt-a-curation-mindset">Link</a>]</p>
</blockquote>
<p>The most transformative leaders of the next decade will be those who
master the art of curation—seeing their role as a conduit for the best
ideas, not the source of them.</p>
<p>The Obsolescence of the "Omniscient Leader": The pace of change,
hyper-specialization, and interconnected challenges (e.g., AI, climate,
global markets) make it impossible for one person to have all the
answers. Leaders must shift from being "the smartest in the room" to
becoming "architects of collective intelligence."</p>
<p><strong>Curation as the Core Leadership Skill:</strong></p>
<ol type="1">
<li>Curating Talent: Prioritize cognitive diversity over homogeneity.
Example: Diverse teams solve problems faster (39% efficiency
boost).</li>
<li>Curating Ideas: Create systems where unconventional thinking
flourishes (e.g., Google’s 20% time → Gmail, Maps). Actively seek
"outliers" (contrarians, outsiders) to challenge groupthink.</li>
<li>Curating Innovation: Design for "structured serendipity" (e.g.,
Pixar’s open office, IDEO’s cross-industry brainstorming). Embrace
cross-disciplinary collisions (e.g., NASA’s tech inspiring sportswear,
biomimicry in architecture).</li>
</ol>
<p><strong>How to Cultivate a Curation Mindset</strong>:</p>
<ul>
<li>Facilitate, don’t dictate: Ask better questions; let solutions
emerge from debate (e.g., Amazon’s "Disagree and Commit").</li>
<li>Optimize for collaboration, not just efficiency: Space matters
(physical or virtual).</li>
</ul>
<blockquote>
<p><strong>Perplexity CEO says its browser will track everything users
do online to sell ‘hyper personalized’ ads - TechCrunch</strong> [<a
target="_blank" rel="noopener" href="https://techcrunch.com/2025/04/24/perplexity-ceo-says-its-browser-will-track-everything-users-do-online-to-sell-hyper-personalized-ads/">Link</a>]</p>
</blockquote>
<p>Perplexity is building a browser (Comet) to track user behavior
across the web—explicitly to fuel targeted advertising. It highlights
the company’s ambition to emulate Google’s surveillance-capitalism
playbook.</p>
<p>Perplexity’s move confirms that the AI search revolution is less
about displacing Google’s model than replicating it—with AI as a smarter
wrapper for the same ads.</p>
<blockquote>
<p><strong>Today’s Most Crucial Leadership Skill Is Systems Thinking -
Forbes</strong> [<a
target="_blank" rel="noopener" href="https://www.forbes.com/sites/maryjohnstone-louis/2025/04/25/todays-most-crucial-leadership-skill-is-systems-thinking/">Link</a>]</p>
</blockquote>
<p>Leaders who master systems thinking don’t just survive
uncertainty—they <strong>thrive in it</strong>, turning complexity into
competitive advantage.</p>
<p><strong>Five Key Tools of Systems Thinking for Strategic
Leaders</strong></p>
<ol type="1">
<li>Problem Statements: Move from surface-level fixes to systemic
solutions. Example: Instead of asking, <em>“How do we get customers to
recycle?”</em>, ask, <em>“How can we redesign products and
infrastructure for circularity?”</em></li>
<li>Stakeholder Mapping: Identify all affected parties—not just obvious
ones. Example: For electric vehicles, consider miners of critical
minerals, urban planners, and regulators, not just automakers and
buyers.</li>
<li>Iceberg Analysis: Look beneath visible events to uncover hidden
structures and mindsets. Example: Employee burnout isn’t just about
workload—it’s shaped by corporate culture, incentive systems, and
societal norms.</li>
<li>Causal Loops: Visualize feedback loops to see how actions create
ripple effects. Example: A cost-cutting measure in one department may
increase inefficiencies elsewhere.</li>
<li>Iteration &amp; Testing: Embrace adaptive strategies, not rigid
plans. Example: Pilot small-scale solutions, measure impact, and refine
before full rollout.</li>
</ol>
<blockquote>
<p><strong>Perplexity CEO shares the Elon Musk–inspired mantra that
helped him build the $9 billion rival to OpenAI - Fortune</strong> [<a
target="_blank" rel="noopener" href="https://fortune.com/article/perplexity-ceo-aravind-srinivas-elon-musk-inspired-mantra-help-build-billion-dollar-openai-google-rival/">Link</a>]</p>
</blockquote>
<p>Srinivas’s journey highlights resilience, speed, and Silicon Valley’s
tight-knit founder network as key drivers of startup success.</p>
<ol type="1">
<li>"It’s Only Over When You Give Up" – Aravind Srinivas, CEO of AI
search startup Perplexity, draws inspiration from Elon Musk’s
perseverance during SpaceX’s early failures. He told Harvard students
that success comes from relentless self-belief, even when others doubt
you.</li>
<li>Rocketing Valuation – Perplexity, competing with Google and OpenAI,
grew from a 1B to 9B and is now in talks to raise funds at an 18B
valuation.</li>
<li>Forget Pitch Decks, Build Fast – Srinivas advises founders to focus
on rapid product iteration rather than lengthy business plans. He admits
he doesn’t even know how to make a pitch deck—Perplexity’s success came
from live demos.</li>
<li>OpenAI Alumni Network – Despite competing with OpenAI, Srinivas
maintains a strong relationship with Sam Altman (his former boss at
OpenAI). This mirrors the "PayPal Mafia" dynamic, where ex-OpenAI
employees now lead major AI firms like Anthropic and Safe
Superintelligence.</li>
</ol>
<blockquote>
<p><strong>Marc Andreessen predicts one of the few jobs that may survive
the rise of AI automation - Fortune</strong> [<a
target="_blank" rel="noopener" href="https://fortune.com/article/mark-andreessen-venture-capitalism-ai-automation-a16z/">Link</a>]</p>
</blockquote>
<p>Andreessen’s logic suggests focusing on roles where trust,
psychology, and networks matter more than data crunching. But don’t
underestimate AI’s ability to creep into those domains too.</p>
<blockquote>
<p><strong>How To Get Noticed Without Self-Promotion By Using Strategic
Visibility - Forbes</strong> [<a
target="_blank" rel="noopener" href="https://www.forbes.com/sites/cynthiayoung/2025/04/27/how-to-get-noticed-without-self-promotion-by-using-strategic-visibility/">Link</a>]</p>
</blockquote>
<p><strong>Core Lessons:</strong></p>
<ol type="1">
<li><strong>Hard Work ≠ Visibility</strong>: Doing great work is
necessary but insufficient. If leaders don’t know what you’re doing,
they can’t reward it. Waiting for annual reviews is too late—visibility
requires consistent, intentional updates.</li>
<li><strong>Humility Has a Hidden Cost</strong>: While modesty is
admirable, staying silent can render you invisible. Gallup’s data on
declining engagement (just 36% in 2020) highlights how disengagement
hurts promotion prospects. Visibility isn’t ego-driven; it’s about
ensuring your impact is recognized.</li>
<li><strong>Visibility ≠ Bragging</strong>: Framing contributions as
useful knowledge (e.g., "Here’s how I solved X") builds trust and
leadership credibility. Sharing wins, failures, and best practices helps
the team and positions you as a problem-solver.</li>
<li><strong>Tactical Ways to Increase Visibility</strong>
<ul>
<li>Share knowledge: Lead "lessons learned" sessions or contribute to
internal newsletters.</li>
<li>Mentor others: Their success reflects your leadership.</li>
<li>Speak up strategically: One substantive insight per meeting &gt;
empty chatter.</li>
<li>Volunteer for high-impact projects: Align with organizational
priorities.</li>
<li>Write internally: Document best practices to showcase thought
leadership.</li>
</ul></li>
<li><strong>Emotional Intelligence (EQ) Matters More Than
Extroversion</strong>
<ul>
<li>Visibility is about meaningful engagement, not being the
loudest.</li>
<li>Avoid self-deprecating language ("I’m sorry, but…")—speak with
conviction.</li>
</ul></li>
<li><strong>What Leaders Actually Notice</strong>
<ul>
<li>Initiative, influence, and alignment with goals matter more than
face-time.</li>
<li>Working smart (not just late) and collaborating effectively signal
leadership potential.</li>
</ul></li>
</ol>
<h2 id="youtube-and-podcast">YouTube and Podcast</h2>
<blockquote>
<p><strong>DOGE updates + Liberation Day Tariff Reactions with Ben
Shapiro and Antonio Gracias - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=OjhA9p3ZXW0&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>2027 Intelligence Explosion: Month-by-Month Model — Scott
Alexander &amp; Daniel Kokotajlo - Dwarkesh Patel</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=htOvH12T7mU&amp;ab_channel=DwarkeshPatel">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Trump vs Harvard, Nvidia export controls, how DEI killed
Hollywood with Tim Dillon - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rCrb4TbHRxc&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>E187 | 关税战难解美国制造业困境，旧秩序正在崩溃 -
硅谷101播客</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=6Fmo_jBc7mA&amp;t=1637s&amp;ab_channel=%E7%A1%85%E8%B0%B7101%E6%92%AD%E5%AE%A2">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How DeepSeek Rewrote the Transformer [MLA] - Welch
Labs</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=0VLAoVGf_74&amp;ab_channel=WelchLabs">Link</a>]</p>
</blockquote>
<p>A lecture explaining the architecture and optimizations behind
DeepSeek R1, a language model that improves Transformer efficiency.</p>
<blockquote>
<p><strong>Live Demo: Reinforcement Fine-Tuning for LLMs — Build Smarter
Models with Less Data l Tutorial - Predibase</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=tHvZypO3le4&amp;ab_channel=Predibase">Link</a>]</p>
</blockquote>
<p>This video was talking about why RFT beats supervised fine-tuning
(SFT) in reasoning tasks, giving live demo of an end-to-end RFT
workflow, and PyTorch-to-Triton case study showing real-world
impact.</p>
<blockquote>
<p><strong>Model Context Protocol (MCP), clearly explained (why it
matters) - Greg Isenberg</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=7j_NE6Pjv-E&amp;ab_channel=GregIsenberg">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Trump Rally or Bessent Put? Elon Back at Tesla, Google's
Gemini Problem, China's Thorium Discovery - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4tcd1EgQrhU&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Suffering is mostly mental anguish and mental pain and it just
means you don't want to do the task at hand.</em></p>
<p><em>The kind of fame that pure actors and celebrities have, I
wouldn't want, but the kind of fame that's earned because you did
something useful, why dodge that.</em></p>
<p><em>People will always want more status uh but I think you can be
satisfied at a certain level of wealth.</em></p>
<p><em>Not the kind of confidence that would say I have the answer but
the kind of confidence that I will figure it out and I know what I want
or only I am a good arbiter of what I want.</em></p>
<p><em>Pride is the enemy of learning, so when I look at my friends and
colleagues, the ones who are still stuck in the past and have grown the
least are the ones who were the proudest, because they sort of feel like
they already had the answers and so they don't want to correct
themselves publicly.</em></p>
<p><em>I think everybody puts themselves first that's just human nature,
you're here because you survive you're a separate organism.</em></p>
<p><em>The happier you are, the more you can sustain doing something,
the more likely you're going to do something that will in turn make you
even happier, and you'll continue to do it, and you'll outwork everybody
else. The more free you are the better you can allocate your
time.</em></p>
<p><em>There are no problems in the real world other than maybe things
that inflict pain on your body. Everything else has to become a problem
in your mind first.</em></p>
<p><em>Your family is broken but you're going to fix the world. People
are running out there to try and fix the world when their own lives are
a mess.</em></p>
<p><em>I think the only true test of intelligence is if you get what you
want out of life and there are two parts to that one is getting what you
want so you know how to get it and the second is wanting the right
things knowing what to want in the first place.</em></p>
<p><em>Usually I think people end up there because they are going on
autopilot with sort of societal expectations or other people's
expectations or out of guilt or out of like mimetic desire.</em></p>
<p><em>Probably the biggest regret will be staying in the relationship
after you knew it was over, exactly you should have left sooner, the
moment you knew it wasn't going to work out, you should have moved
on.</em></p>
<p><em>We are naturally hardwired to be pessimists but modern society is
very different despite whatever problems you may have with modern
society, it is far far safer than living in the jungle and just trying
to survive and the opportunities.</em></p>
<p><em>Leave all those labels alone. It's better just to look at the
problem at hand, look at reality the way it is, try to take yourself out
of the equation in a sense.</em></p>
<p><em>The less you think about yourself the more you can think about a
mission or about God or about a child or something like that.</em></p>
<p><em>I don't think there are any formulas i think it's unique to each
person it's like asking a successful person how did you become
successful each one of them will give you a different story uh you can't
follow anyone else's path.</em></p>
<p><em>A lot of change is more about desire and understanding than it is
about uh forcing yourself or trying to domesticate yourself.</em></p>
<p><em>When your mind is under stress, it's because it has two
conflicting desires at once... and anxiety I think is sort of this
pervasive unidentifiable stress where you're just kind of stressed out
all the time and you're not even sure why and you can't even identify
the underlying problem. I think the reason for that is because you have
so many unresolved problems unresolved stress points that have piled up
in your life that you can no longer identify what the problems
are.</em></p>
<p><em>Life is going to play out the way it's going to play out there
will be some good and some bad most of it is actually just up to your
interpretation.</em></p>
<p><em>The gut is what decides the head, is kind of what rationalizes it
afterwards, the gut is the ultimate decision maker.</em></p>
<p><em>You can't change other people, you can change your reaction to
them.</em></p>
<p><em>If you do want to change someone's behavior, I I think the only
effective way to do it is to compliment them when they do something you
want, not to insult them or be negative or critical when they do
something you don't want.</em></p>
<p>If you can't decide, the answer is no.</p>
<p><em>Almost invariably the advice that you would give yourself 10
years ago is still the advice that you need to hear today.</em></p>
<p><em>On mental things, I think understanding is way more important
once you see the truth of something you cannot unsee it... when we
really do see something clearly, it changes our behavior immediately,
and that is far more efficient than trying to change your behavior
through repetition.</em></p>
<p><em>Truth is often painful, if it wasn't, we'd all be seeing truth
all the time. Reality is always reflecting truth that's all it is why
would you not have accessed it already exactly... wisdom is the set of
things that cannot be transmitted. If they could be transmitted you know
we'd read the same five philosophy books, and we'd all be done, we'd all
be wise. You have to learn it for yourself, it has to be rediscovered
for yourself in your own context.</em></p>
<p><em>You're probably better off only caring about things that are
local or things that you can affect. So if you really care about
something that's in the news, then by all means care about it but make a
difference go do something about it.</em></p>
<p><em>Desire is a contract to be unhappy until you get what you
want.</em></p>
<p><em>The real currency of life is attention it's what you choose to
pay attention to and and what you do about it.</em></p>
<p><strong>― 44 Harsh Truths About Human Nature - Naval Ravikant (4K) -
Chris Williamson</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=KyfUysrNaco">Link</a>]</p>
</blockquote>
<p>Key Learnings:</p>
<ul>
<li>Someone who can do the job peacefully or happily is more effective
than someone with unnecessary emotional turmoil</li>
<li>Fame sought for its own sake is fragile and leads to a constant need
to perform.</li>
<li>People often say things they don't really believe, driven by a
desire to be seen as something they are not.</li>
<li>Status is zero-sum and insatiable, unlike wealth. Status is often
comparative, like leaderboards, where one person's gain can be another's
loss.</li>
<li>Self-esteem comes from aligning actions with internal values,
especially when difficult. Genuine sacrifice, doing something you want
less for something you value more, can build self-esteem.</li>
<li>True confidence is not having all the answers but the self-belief to
figure things out.</li>
<li>Pride is an enemy of learning and can lead to being stuck in past
mistakes.</li>
<li>Everyone puts themselves first; unapologetic self-prioritization is
rare but perhaps more honest. Much of what appears as altruism might be
a waste of time if it goes against one's true desires.</li>
<li>Happiness and freedom are intertwined with efficiency and
productivity.</li>
<li>Many emotional problems arise from the mind creating problems where
none exist in the real world. He advises observing one's thoughts
objectively to realize unnecessary emotional energy expenditure.</li>
<li>People often try to fix the world while their own lives are in
disarray. He questions the credibility of those who cannot manage their
own lives but seek to solve global issues.</li>
<li>True intelligence is getting what you want out of life by wanting
the right things and knowing how to get them.</li>
<li>Many people go through life unconsciously following societal or
mimetic desires. He emphasizes the importance of thinking things through
for oneself rather than blindly following others.</li>
<li>Staying too long in bad situations (relationships, jobs) is a common
regret.</li>
<li>We are naturally hardwired for pessimism due to evolutionary
pressures to avoid ruin.</li>
<li>Humans are dynamic and labels like optimist, pessimist, introvert,
extrovert are self-limiting.</li>
<li>Overthinking about oneself can lead to misery; focusing on something
bigger can bring happiness. Overthinking and rumination do not help with
happiness.</li>
<li>There are no universal formulas for success or happiness; each
person's path is unique.</li>
<li>Lasting change comes from desire and understanding, not forcing
oneself. He suggests aligning actions with genuine wants for maximal
effectiveness.</li>
<li>Anxiety often stems from having many unresolved and conflicting
desires.</li>
<li>Our interpretations of experiences shape our reality. The same
experience can lead to different emotional responses based on individual
interpretation.</li>
<li>The "gut" is the ultimate decision-maker, representing refined
judgment accumulated through evolution and experience. He advises
trusting this instinct once it's developed.</li>
<li>You cannot change other people, only your reaction to them. He adds
that people change through their own insights or trauma, not by being
told to.</li>
<li>Negative reinforcement is less effective than positive reinforcement
in changing behavior.</li>
<li>If faced with a difficult choice and unable to decide, the answer is
often "no." He also suggests that when choosing between two equal
options, take the more painful path in the short term.</li>
<li>Understanding is more important than discipline for mental
change.</li>
<li>Truth, though often painful, is constantly reflected by reality;
wisdom is the personal rediscovery and contextual application of
timeless truths. He also mentions that many important life lessons are
"unteachable" in the sense that they must be experienced firsthand to be
truly understood.</li>
<li>Memorization is becoming less valuable in the age of readily
available information; understanding, judgment, and taste are more
crucial. He links understanding to solving real problems and finding
generalizable truths.</li>
<li>Philosophy evolves with new knowledge and perspectives. He explains
how advancements in science and technology lead to different
philosophical outlooks, and even moral philosophy progresses over
time.</li>
<li>Many philosophical paradoxes can be resolved by considering
different scales and timeframes. Naval suggests that seemingly
contradictory questions like free will and determinism can be understood
by shifting perspectives.</li>
<li>Coordination is essential for societal function; pure libertarianism
is unsustainable.</li>
<li>Modern AI, while powerful, currently lacks true creativity and deep
understanding.</li>
<li>Meaning can be more important than moment-to-moment happiness.</li>
<li>In an age of news saturation, it's a battle to maintain focus on
what truly matters and what one can influence. He emphasizes that
attention is the real currency of life and should be spent consciously.
Attention, not time or money, is the most fundamental resource in
life.</li>
<li>Getting past one's past is a skill achieved by processing it to be
rid of it, not to dwell on it.</li>
</ul>
<blockquote>
<p><em>I think agents are real, but I think that we are far away from
that because we're still at the phase of how do you build reliable
software in production for an enterprise versus the toy apps that you
see on the internet which is like let me vibe code something. I think
these things are worlds apart still. - Chamath Palihapitiya</em></p>
<p><em>I think we have not yet figured out how to move the budgets from
experimentation to mainline production. Meaning where large chunks of
the US economy are comfortable enough with the ways in which
hallucinations are managed such that they will replace legacy
deterministic code with this new probabilistic model generated code
meaning model enabled code. - Chamath Palihapitiya</em></p>
<p><strong>― Trump's First 100 Days, Tariffs Impact Trade, AI Agents,
Amazon Backs Down - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=W960TW79QCI">Link</a>]</p>
</blockquote>
<h2 id="papers-and-reports">Papers and Reports</h2>
<blockquote>
<p><strong>Orchestrating Agents and Data for Enterprise: A Blueprint
Architecture for Compound AI</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.08148">Link</a>]</p>
</blockquote>
<p>This paper contributes to the enterprise AI landscape by offering a
comprehensive architectural blueprint for deploying agentic, modular,
and data-integrated AI systems that can efficiently leverage LLMs and
enterprise assets.</p>
<h2 id="github">Github</h2>
<blockquote>
<p><strong>Google Gemini 2.0 with MCP (Model Context Protocol) Servers -
Gemini Samples</strong> [<a
target="_blank" rel="noopener" href="https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-mcp-example.ipynb">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Maestro - A Framework for Claude Opus, GPT and local LLMs to
Orchestrate Subagents - maestro</strong> [<a
target="_blank" rel="noopener" href="https://github.com/Doriandarko/maestro">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>MCP-Agent</strong> [<a
target="_blank" rel="noopener" href="https://github.com/lastmile-ai/mcp-agent">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Local Deep Researcher</strong> [<a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/local-deep-researcher">Link</a>]</p>
</blockquote>
<h2 id="news">News</h2>
<blockquote>
<p><strong>Accelerate Generalist Humanoid Robot Development with NVIDIA
Isaac GR00T N1 - NVIDIA</strong> [<a
target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/accelerate-generalist-humanoid-robot-development-with-nvidia-isaac-gr00t-n1/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Announcing the Agent2Agent Protocol (A2A) - Google for
Developers</strong> [<a
target="_blank" rel="noopener" href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">Link</a>]</p>
</blockquote>
<p><strong>Key Takeaways:</strong></p>
<ol type="1">
<li>A2A is an open-source protocol backed by 50+ tech giants (e.g.,
Salesforce, SAP, Cohere) and consultancies (e.g., Accenture, Deloitte).
It allows agents from different vendors/frameworks to communicate, share
data, and coordinate tasks without being locked into a single
platform.</li>
<li>Solving Enterprise Pain Points: Breaks down silos by letting agents
interoperate across HR (Workday), CRM (Salesforce), ERP (SAP), and other
systems. Example: A hiring manager’s agent can autonomously source
candidates, schedule interviews, and run background checks by
collaborating with specialized agents.</li>
<li>Design Principles:
<ul>
<li>Agent-Centric: Supports unstructured, multi-agent collaboration
(beyond rigid "tool" roles).</li>
<li>Built on Standards: Uses HTTP, JSON-RPC, and SSE for easy
integration.</li>
<li>Secure by Default: Enterprise-grade auth (aligned with
OpenAPI).</li>
<li>Long-Running Tasks: Handles tasks lasting hours/days with real-time
updates.</li>
<li>Multimodal: Supports text, audio, video, and UI negotiations (e.g.,
web forms, iframes).</li>
</ul></li>
</ol>
<blockquote>
<p><strong>The Chinese goods Americans most rely on, from microwaves to
Barbies - Financial Times</strong> [<a
target="_blank" rel="noopener" href="https://www.ft.com/content/ec96e2ed-5dd6-4c6b-92a0-1b77bf517b36">link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Apple Vision Pro 2 Reportedly Cheaper &amp; Lighter,
Mac-Tethered Headset Coming Too - Upload</strong> [<a
target="_blank" rel="noopener" href="https://www.uploadvr.com/apple-vision-pro-2-reportedly-cheaper-lighter-mac-tethered-headset-coming-too/">Link</a>]</p>
</blockquote>
<h2 id="others">Others</h2>
<p>Machine learning (ML) solutions applied to business problems across
various industries:</p>
<ul>
<li>ML and LLM system design: 500 case studies to learn from [<a
target="_blank" rel="noopener" href="https://www.evidentlyai.com/ml-system-design">Link</a>]</li>
<li>Machine Learning and Data Science Applications in Industry - Firmai
[<a
target="_blank" rel="noopener" href="https://github.com/firmai/industry-machine-learning">Link</a>]</li>
<li>Business Machine Learning - Firmai [<a
target="_blank" rel="noopener" href="https://github.com/firmai/business-machine-learning">Link</a>]</li>
<li>ML System Design Case Studies Repository [<a
target="_blank" rel="noopener" href="https://github.com/Engineer1999/A-Curated-List-of-ML-System-Design-Case-Studies">Link</a>]</li>
<li>500+ Artificial Intelligence Project List with Code [<a
target="_blank" rel="noopener" href="https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code">Link</a>]</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/03/04/2025-March/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/03/04/2025-March/" class="post-title-link" itemprop="url">2025 March - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-03-04 00:15:05" itemprop="dateCreated datePublished" datetime="2025-03-04T00:15:05-05:00">2025-03-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="blogs-and-articles">Blogs and Articles</h2>
<blockquote>
<p><strong>How to Build a Graph RAG App - Steve Hedden</strong> [<a
target="_blank" rel="noopener" href="https://towardsdatascience.com/how-to-build-a-graph-rag-app-b323fc33ba06/">Link</a>]</p>
</blockquote>
<p>Walking through building a graph rag app that improves LLM accuracy
using knowledge graphs. It covers data preparation, search refinement
with MeSH terms, and article summarization.</p>
<blockquote>
<p><em>We believe that, in 2025, we may see the first AI agents “join
the workforce” and materially change the output of companies. We
continue to believe that iteratively putting great tools in the hands of
people leads to great, broadly-distributed outcomes.</em></p>
<p><strong>― Reflections - Sam Altman</strong> [<a
target="_blank" rel="noopener" href="https://blog.samaltman.com/reflections">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Structured Report Generation Blueprint with NVIDIA
AI</strong> [<a
target="_blank" rel="noopener" href="https://blog.langchain.dev/structured-report-generation-blueprint/">Link</a>]
[<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=E04rFNtwFcA">YouTube</a>]</p>
</blockquote>
<blockquote>
<p><strong>Sky-T1: Train your own O1 preview model within $450</strong>
- <a target="_blank" rel="noopener" href="https://novasky-ai.github.io/">NovaSky</a> [<a
target="_blank" rel="noopener" href="https://novasky-ai.github.io/posts/sky-t1/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Agents - Chip Huyen</strong> [<a
target="_blank" rel="noopener" href="https://huyenchip.com//2025/01/07/agents.html">Link</a>]</p>
</blockquote>
<p>This guide provides a comprehensive exploration of AI-powered agents,
focusing on their capabilities, planning, tool selection, and failure
modes. It delves into the factors determining an agent's performance,
how LLMs can plan, and how to augment planning capabilities. It also
provides insights into agent failures and how to evaluate them
effectively.</p>
<blockquote>
<p><strong>The Batch Issue 284 - DeepLearning.AI - Andrew Ng</strong>
[<a
target="_blank" rel="noopener" href="https://www.deeplearning.ai/the-batch/issue-284/">Link</a>]</p>
</blockquote>
<p>Andrew Ng highlights AI Product Management’s growth as software
becomes cheaper to build.</p>
<blockquote>
<p><strong>DeepSeek V3 LLM NVIDIA H200 GPU Inference Benchmarking -
DataCrunch</strong> [<a
target="_blank" rel="noopener" href="https://datacrunch.io/blog/deepseek-v3-llm-nvidia-h200-gpu-inference-benchmarking">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Global-batch load balance almost free lunch to improve your
MoE LLM training - Qwen</strong> [<a
target="_blank" rel="noopener" href="https://qwenlm.github.io/blog/global-load-balance/">Link</a>]</p>
</blockquote>
<p>MoE models struggle with expert underutilization due to
micro-batch-level load balancing, which fails when data within a batch
lacks diversity. This results in poor expert specialization and model
performance.</p>
<p>The paper proposes global-batch load balancing, where expert
selection frequencies are synchronized across all parallel groups,
ensuring more effective domain specialization and improved
performance.</p>
<p>Global-batch load balancing outperforms micro-batch balancing in all
tested configurations. It shows improved performance and expert
specialization, with models achieving better results across various data
sizes and domains.</p>
<blockquote>
<p><strong>How to Evaluate LLM Summarization - Isaac Tham</strong> [<a
target="_blank" rel="noopener" href="https://towardsdatascience.com/how-to-evaluate-llm-summarization-18a040c3905d/">Link</a>]</p>
</blockquote>
<p>A quantitative, research-backed framework for evaluating LLM
summaries, focusing on conciseness and coherence. This guide explores
challenges in summarization evaluation, defines key quality metrics
(conciseness, coherence), and improves the Summarization Metric in the
DeepEval framework. Includes a GitHub notebook for applying these
methods to assess summaries of long-form content systematically.</p>
<blockquote>
<p><strong>We just gave sight to smolagents - HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/blog/smolagents-can-see">Link</a>]</p>
</blockquote>
<p>This tutorial is about how to integrate vision capabilities into
autonomous agents using smolagents. It explains passing images to agents
in two ways: at initialization or dynamically via callbacks. It
demonstrates building a web-browsing agent with vision using the
MultiStepAgent class and helium. The agent performs actions like
navigation, popup handling, and dynamic webpage analysis.</p>
<blockquote>
<p><strong>On DeepSeek and Export Controls - Dario Amodei</strong> [<a
target="_blank" rel="noopener" href="https://darioamodei.com/on-deepseek-and-export-controls">Link</a>]</p>
</blockquote>
<p>Highlighting export controls' impact on AI geopolitics.</p>
<blockquote>
<p><strong>Workflows and Agents - LangGraph</strong> [<a
target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/tutorials/workflows/">Link</a>]</p>
</blockquote>
<p>Review of common patterns for agentic systems.</p>
<blockquote>
<p><strong>Constitutional Classifiers: Defending against universal
jailbreaks - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/research/constitutional-classifiers">Link</a>]
[<a target="_blank" rel="noopener" href="https://youtu.be/BaNXYqcfDyo">YouTube</a>]</p>
</blockquote>
<p>Anthropic invites everyone to test its new safety classifier that
eradicates jailbreaks and further increases Claude's over-refusal
rate.</p>
<blockquote>
<p><strong>Open-source DeepResearch – Freeing our search agents -
HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/blog/open-deep-research">Link</a>]</p>
</blockquote>
<p>Hugging Face challenges OpenAI’s Deep Research with an open-source
alternative, beating previous SOTA by 9 points.</p>
<blockquote>
<p><strong>Choosing the Right AI Agent Framework: LangGraph vs CrewAI vs
OpenAI Swarm - Yi Zhang</strong> [<a
target="_blank" rel="noopener" href="https://blog.relari.ai/choosing-the-right-ai-agent-framework-langgraph-vs-crewai-vs-openai-swarm-56f7931b4249">Link</a>]</p>
</blockquote>
<p>Compare LangGraph, CrewAI, and OpenAI Swarm frameworks for building
agentic applications with hands-on examples. Understand when to use each
framework, and get a preview of debugging and observability topics in
Part II.</p>
<blockquote>
<p><strong>How to Scale Your Model - Google DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://jax-ml.github.io/scaling-book/">Link</a>]</p>
</blockquote>
<p>Learn how to scale LLMs on TPUs by understanding hardware
limitations, parallelism, and efficient training techniques. Explore how
to estimate training costs, memory needs, and optimize performance using
strategies like data, tensor, pipeline, and expert parallelism. Gain
hands-on experience with LLaMA-3, and learn to profile and debug your
code.</p>
<blockquote>
<p><strong>Three Observations - Sam Altman</strong> [<a
target="_blank" rel="noopener" href="https://blog.samaltman.com/three-observations">Link</a>]</p>
</blockquote>
<p>Sam outlines AI trends: AI’s scaling limits, cost reduction, and the
future of autonomous agents.</p>
<blockquote>
<p><strong>How to deploy and fine-tune DeepSeek models on AWS -
HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/blog/deepseek-r1-aws">Link</a>]</p>
</blockquote>
<p>Deploy and fine-tune DeepSeek-R1 models on AWS using Hugging Face
with GPUs, SageMaker, and EC2 Neuron.</p>
<blockquote>
<p><strong>Building a Universal Assistant to connect with any API -
Pranav Dhoolia</strong> [<a
target="_blank" rel="noopener" href="https://medium.com/heurislabs/building-a-universal-assistant-to-connect-with-any-api-89d7c353e524">Link</a>]</p>
</blockquote>
<p>Convert any OpenAPI spec into an MCP-compatible API assistant without
writing custom integration code. Use a generic MCP server to expose API
endpoints dynamically. This approach simplifies integration, expands
compatibility, and makes scaling API support more efficient.</p>
<blockquote>
<p><strong>From PDFs to Insights: Structured Outputs from PDFs with
Gemini 2.0 - Philschmid</strong> [<a
target="_blank" rel="noopener" href="https://www.philschmid.de/gemini-pdf-to-data">Link</a>]</p>
</blockquote>
<p>Learn to convert PDFs into structured JSON using Gemini 2.0. Set up
the SDK, process files, manage tokens, and define JSON schemas with
Pydantic. Covers real-world examples like invoices and forms, best
practices, and cost management, works within the free tier.</p>
<blockquote>
<p><strong>The Hidden Ways We Really Work Together - Microsoft</strong>
[<a
target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/worklab/patterns-hidden-inside-the-org-chart">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Managing LLM implementation projects - Piotr
Jurowiec</strong> [<a
target="_blank" rel="noopener" href="https://piotr-jurowiec.medium.com/managing-llm-implementation-projects-2daa0ef5c67a">Link</a>]</p>
</blockquote>
<p>Discover how to implement LLMs from initial planning to deployment.
Establish project goals, select suitable architectures, preprocess data,
train and evaluate models, optimize hyperparameters, and incorporate
domain expertise. Tackle challenges such as hallucinations, security
risks, regulatory compliance, and scalability limitations. Develop
systematic workflows for building and managing LLM-based
applications.</p>
<blockquote>
<p><strong>How to build a ChatGPT-Powered AI tool to learn technical
things fast - AWS</strong> [<a
target="_blank" rel="noopener" href="https://community.aws/content/2hEttGRiSiGwAn2Bzl0lbQcT7v6/build-ai-with-hf-chatgpt">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>What Problem Does The Model Context Protocol Solve? -
AIhero</strong> [<a
target="_blank" rel="noopener" href="https://www.aihero.dev/what-problem-does-model-context-protocol-solve">Link</a>]</p>
</blockquote>
<p>Learn how the Model Context Protocol (MCP) simplifies integrating
large language models (LLMs) with external APIs.</p>
<p>MCP acts as a connector between LLMs and external data sources,
facilitating interactions with tools without requiring LLMs to
understand intricate APIs. By providing a standardized interface, it
streamlines integrations with platforms like GitHub, enhancing workflow
speed and efficiency.</p>
<blockquote>
<p><strong>Most AI value will come from broad automation, not from
R&amp;D - Epoch AI</strong> [<a
target="_blank" rel="noopener" href="https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d">Link</a>]</p>
</blockquote>
<p>Epoch AI's article argues against the popular notion that the primary
economic benefit of artificial intelligence will stem from its
application in research and development. Instead, the authors posit that
AI's most significant value will arise from its widespread deployment in
automating existing labor across various sectors.</p>
<h2 id="substack">Substack</h2>
<blockquote>
<p><strong>Tencent: Betting Big on AI - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/tencent-betting-big-on-ai">Link</a>]</p>
</blockquote>
<figure>
<img src="/di-blog/2025/03/04/2025-March/tencent_corporate_overview.png"
alt="tencent_corporate_overview" />
<figcaption aria-hidden="true">tencent_corporate_overview</figcaption>
</figure>
<p>Tencent's proprietary HunYuan framework has developed into a central
AI platform catering to both consumers and enterprises. Originally
centered on text and conversational AI, HunYuan has expanded to support
multimodal capabilities, including image, video, and 3D generation,
where it has attained top rankings in industry benchmarks.</p>
<figure>
<img src="/di-blog/2025/03/04/2025-March/HunYuan_Thesis.png"
alt="HunYuan_Thesis" />
<figcaption aria-hidden="true">HunYuan_Thesis</figcaption>
</figure>
<p>Tencent has a Dual-Core AI strategy: It combines its proprietary T1
model with external AI, such as DeepSeek’s R1, in a “double-core”
approach. Yuanbao chatbot utilizes both—T1 for deep reasoning and R1 for
quick responses—while WeChat Search enhances accuracy by integrating T1
with DeepSeek.</p>
<figure>
<img
src="/di-blog/2025/03/04/2025-March/tencent_multi_model_strategy.png"
alt="tencent_multi_model_strategy" />
<figcaption aria-hidden="true">tencent_multi_model_strategy</figcaption>
</figure>
<blockquote>
<p><strong>Google: Biggest Deal Ever - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/google-biggest-deal-ever">Link</a>]</p>
</blockquote>
<p>Alphabet has completed its largest acquisition to date with a $32
billion deal to acquire cloud security startup Wiz. If successful, this
move could redefine GCP’s security portfolio, strengthening its stance
as AI-driven cloud computing becomes the focal point.</p>
<figure>
<img src="/di-blog/2025/03/04/2025-March/google_biggest_acquisition.png"
alt="google_biggest_acquisition" />
<figcaption aria-hidden="true">google_biggest_acquisition</figcaption>
</figure>
<h2 id="papers-and-reports">Papers and Reports</h2>
<blockquote>
<p><strong>Whitepaper Agents - Authors: Julia Wiesinger, Patrick Marlow
and Vladimir Vuskovic</strong> [<a
target="_blank" rel="noopener" href="https://www.kaggle.com/whitepaper-agents">Link</a>]</p>
</blockquote>
<p>Google’s whitepaper explains how AI agents use reasoning, tools, and
external data to automate tasks, turning large language models (LLMs)
into workflow automation systems. Google suggests using
<strong>LangChain</strong> for prototyping and <strong>Vertex
AI</strong> for scaling production-ready agents. its framework provides
a standardized approach to ensure reliable AI agent execution.</p>
<p><strong>Key Components</strong></p>
<ol type="1">
<li><strong>Decision Engine</strong> – The LLM plans and executes tasks
using reasoning methods like ReAct or Chain-of-Thought.</li>
<li><strong>Tool Integration</strong> – Agents interact with APIs,
databases, and real-time data.</li>
<li><strong>Orchestration Layer</strong> – Manages task execution and
decision-making.</li>
</ol>
<p><strong>Tool Types</strong></p>
<ol type="1">
<li><strong>Extensions</strong> – Directly call APIs for
automation.</li>
<li><strong>Functions</strong> – Allow developers to control
execution.</li>
<li><strong>Data Stores</strong> – Use retrieval-augmented generation
(RAG) for external data access.</li>
</ol>
<p><strong>Use Cases</strong></p>
<p>Agents handle tasks like personalized recommendations, workflow
automation, and database queries. For example, they can fetch a user’s
purchase history and generate tailored responses.</p>
<blockquote>
<p><strong>Introducing <em>smolagents</em>, a simple library to build
agents - HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/blog/smolagents">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Memory Layers at Scale - Meta</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.09764">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>2 OLMo 2 Furious - Alien AI</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.00656">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>HuatuoGPT-o1, Towards Medical Complex Reasoning with
LLMs</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.18925">Link</a>]</p>
</blockquote>
<p>This paper shows how to build domain-specific reasoning models using
a two-stage training process. HuatuoGPT-o1, a medical LLM, enhances
complex reasoning using this two-stage approach: (1) supervised
fine-tuning (SFT) with complex Chain-of-Thought (CoT) and (2)
reinforcement learning (RL) using a verifier to refine reasoning.</p>
<figure>
<img src="/di-blog/2025/03/04/2025-March/huatuogpto1.png"
alt="huatuogpto1" />
<figcaption aria-hidden="true">huatuogpto1</figcaption>
</figure>
<blockquote>
<p><strong>Inference-Time Scaling for Diffusion Models beyond Scaling
Denoising Steps - Google DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.09732">Link</a>]</p>
</blockquote>
<p>Google DeepMind introduces noise search method, outperforming
traditional denoising in diffusion models.</p>
<blockquote>
<p><strong>Chain of Agents: Large language models collaborating on
long-context tasks - Google Research</strong> [<a
target="_blank" rel="noopener" href="https://research.google/blog/chain-of-agents-large-language-models-collaborating-on-long-context-tasks/">Link</a>]
[<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=LuCLf4BJsr">Paper</a>]</p>
</blockquote>
<blockquote>
<p><strong>SFT Memorizes, RL Generalizes: A Comparative Study of
Foundation Model Post-training - Google DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://tianzhechu.com/SFTvsRL/">Link</a>] [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.17161v1">Link</a>]</p>
</blockquote>
<p>Explaining why reinforcement learning outperforms supervised
fine-tuning for model generalization.</p>
<blockquote>
<p><strong>Learning to Plan &amp; Reason for Evaluation with
Thinking-LLM-as-a-Judge</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.18099">Link</a>]</p>
</blockquote>
<p>A new preference optimization algorithm for LLM-as-a-Judge
models.</p>
<blockquote>
<p><strong>Hallucination Mitigation using Agentic AI Natural
Language-Based Frameworks</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.13946v1">Link</a>]</p>
</blockquote>
<p>Generative AI models often produce hallucinations, making them less
reliable and reducing trust in AI systems. In this work, a multi-agent
system is designed using over 300 prompts to induce hallucinations. AI
agents at different levels review and refine outputs using distinct
language models, structured JSON communication, and the OVON framework
for seamless interaction. New KPIs are introduced to measure
hallucination levels.</p>
<blockquote>
<p><strong>ELEGNT: Expressive and Functional Movement Design for
Non-Anthropomorphic Robot - Apple</strong> [<a
target="_blank" rel="noopener" href="https://machinelearning.apple.com/research/elegnt-expressive-functional-movement">Link</a>]
[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.12493">Link</a>]</p>
</blockquote>
<p>This is very cool.</p>
<blockquote>
<p><strong>π0 and π0-FAST: Vision-Language-Action Models for General
Robot Control - Hugging Face</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/blog/pi0">Link</a>]</p>
</blockquote>
<p>Hugging Face publishes the first open-source robotics foundation
models for real-world applications.</p>
<blockquote>
<p><strong>Claude’s extended thinking - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/research/visible-extended-thinking">Link</a>]</p>
</blockquote>
<p>Claude 3.7 Sonnet introduces extended thinking, visible reasoning,
and improved agentic capabilities for complex tasks.</p>
<blockquote>
<p><strong>Can LLMs Generate Novel Research Ideas? A Large-Scale Human
Study with 100+ NLP Researchers</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.04109">Link</a>]</p>
</blockquote>
<p>This study evaluated the ability of LLMs to generate novel,
expert-level research ideas compared to human experts by recruiting over
100 NLP researchers for idea generation and blind reviews. Results
showed that LLM-generated ideas were rated as more novel than human
ideas (p &lt; 0.05) but slightly less feasible. While LLMs demonstrated
promising ideation capabilities, challenges such as limited idea
diversity and unreliable self-evaluation were identified, highlighting
areas for improvement in developing effective research agents.</p>
<blockquote>
<p><strong>Transformers without Normalization</strong> [<a
target="_blank" rel="noopener" href="https://jiachenzhu.github.io/DyT/?utm_source=alphasignal">Link</a>]</p>
</blockquote>
<p>Yann LeCun and his team have proposed Dynamic Tanh (DyT) as an
alternative to conventional normalization layers in deep learning
models. This innovative method, leveraging the scaled tanh function,
delivers performance on par with or superior to techniques like
LayerNorm and RMSNorm. Notably, its ability to lower computational costs
while preserving model efficiency makes it particularly compelling.</p>
<blockquote>
<p><strong>Energy</strong> [<a
target="_blank" rel="noopener" href="https://infrastructurereportcard.org/wp-content/uploads/2025/03/Energy.pdf">Link</a>]</p>
</blockquote>
<p>Most serious issues:</p>
<ul>
<li>Aging and overburdened energy infrastructure is the most serious
issue.</li>
<li>Energy demand is at its highest growth rate in 20 years. EV adoption
and AI workloads are accelerating the strain on the grid.</li>
<li>There is increased frequency of extreme weather events causing
outages.</li>
<li>The U.S. experienced twice as many weather-related power outages
from 2014–2023 compared to 2000–2009.</li>
</ul>
<p>Some key trends:</p>
<ul>
<li>Energy demand is rising rapidly, especially due to data centers, AI,
and EV adoption.</li>
<li>Extreme weather is causing more frequent and severe power
outages.</li>
<li>The transition to renewables is accelerating, but grid
interconnection delays are slowing progress.</li>
<li>Grid infrastructure is aging and requires massive investment, but
funding gaps remain.</li>
<li>Transformer shortages and long lead times are hindering grid
expansion and maintenance.</li>
<li>Cybersecurity threats and physical attacks on substations are
emerging risks.</li>
</ul>
<h2 id="youtube-and-podcasts">YouTube and Podcasts</h2>
<blockquote>
<p><strong>Fixing the American Dream with Andrew Schulz - All-In
Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=IjU-Nd6iiQ4&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>E179｜DeepSeek技术解析：为何引发英伟达股价下跌？-
硅谷101播客</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=f9Wm1ayTKlQ&amp;ab_channel=%E7%A1%85%E8%B0%B7101%E6%92%AD%E5%AE%A2">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>This Year in Uber’s AI-Driven Developer Productivity
Revolution - Gradle</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=jp-fBw07r7c&amp;t=2s">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>GraphRAG: The Marriage of Knowledge Graphs and RAG: Emil
Eifrem - AI Engineer</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=knDDGYHnnSI">Link</a>]</p>
</blockquote>
<p>Great intro to Graph RAG from Prof Emil Eifrem. Check out <a
target="_blank" rel="noopener" href="https://neo4j.com/labs/genai-ecosystem/">neo4j genai
ecosystem</a>.</p>
<p>Some articles mentioned: "Retrieval-Augmented Generation with
Knowledge Graphs for Customer Service Question Answering" [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.17723v1">Link</a>], and "GraphRAG:
Unlocking LLM discovery on narrative private data" [<a
target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/">Link</a>].</p>
<blockquote>
<p><strong>Building a fully local "deep researcher" with DeepSeek-R1 -
LangChain</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=sGUjmyfof4Q">Link</a>]</p>
</blockquote>
<p>This tutorial reviews DeepSeek R1's training methods, explains
downloading the model via Ollama, and demonstrates JSON-mode testing.
Test its local "deep research" assistant, which performs web research
and iterative summarization with reflection for improved results.</p>
<blockquote>
<p><strong>Building Effective Agents with LangGraph - LangChain</strong>
[<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=aHCDrAbH_go">Link</a>]</p>
</blockquote>
<p>This video shows the difference between agents and workflows and when
to use each. You'll Implement patterns like prompt chaining,
parallelization, and routing using LangGraph. The session covers
building agents, applying advanced patterns, and understanding how
LangGraph enhances automation and optimization in AI systems.</p>
<blockquote>
<p><strong>Nvidia's GTC 2025 Keynote: Everything Announced in 30 Minutes
- Amrit Talks</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=DT5IMxzfdL0&amp;ab_channel=AmritTalks">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How I use LLMs - Andrej Karpathy</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=EWvNQjAaOHw&amp;ab_channel=AndrejKarpathy">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>White House BTS, Google buys Wiz, Treasury vs Fed, Space
Rescue - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=p1MAA8y4CgU&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Satya Nadella – Microsoft’s AGI Plan &amp; Quantum
Breakthrough - Dwarkesh Partel</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4GLSzuYXh6w&amp;ab_channel=DwarkeshPatel">Link</a>]</p>
</blockquote>
<p>Satya Nadella discusses AI, AGI skepticism, economic growth, quantum
computing, AI pricing, gaming models, and legal challenges. Notes of
insights impressed me:</p>
<ol type="1">
<li>Nadella believes hyperscalers (like Microsoft Azure, AWS, and Google
Cloud) will be major beneficiaries of AI advancements. The exponential
growth in compute demand for AI workloads—both for training and
inference—will drive massive infrastructure needs. Hyperscalers are
well-positioned to meet this demand due to their ability to scale
compute, storage, and AI accelerators efficiently.</li>
<li>He argues that hyperscale infrastructure is not a winner-takes-all
market. Enterprises and corporations prefer multiple suppliers to avoid
dependency on a single vendor. This structural dynamic ensures
competition and prevents monopolization.</li>
<li>While there may be a few dominant closed-source AI models, Nadella
predicts that open-source alternatives will act as a check, preventing
any single entity from monopolizing the AI model space. He draws
parallels to the coexistence of closed-source (e.g., Windows) and
open-source systems in the past.</li>
<li>He highlights that governments worldwide are unlikely to allow
private companies to dominate AI entirely. Regulatory and state
involvement will likely shape the landscape, further preventing a
winner-takes-all scenario.</li>
<li>In consumer markets, network effects can lead to winner-takes-all
dynamics (e.g., ChatGPT's early success). However, in enterprise
markets, multiple players will thrive across different categories.</li>
<li>He disagrees with the notion that AI models or cloud infrastructure
will become commoditized. At scale, the complexity of managing
hyperscale infrastructure and the know-how required to optimize it
create significant barriers to entry and sustain profitability.</li>
<li>Microsoft aims to build a versatile hyperscale fleet capable of
handling large training jobs, inference workloads, and specialized tasks
like reinforcement learning (RL). The company focuses on distributed
computing, global data center placement, and high utilization of
resources to meet diverse AI demands.</li>
<li>Nadella envisions a future where AI agents and specialized models
will drive even greater compute demand. He emphasizes the importance of
building infrastructure that can support both training and inference at
scale, while also accommodating evolving AI research and
development.</li>
<li>Microsoft Research (MSR) has a history of investing in fundamental,
curiosity-driven research, often with no immediate payoff. Nadella
emphasizes the importance of maintaining this culture, even if the
benefits may only materialize decades later. Nadella highlights the
difficulty of transitioning from research breakthroughs to scalable
products. The role of leadership is to ensure that innovations are not
only technically sound but also commercially viable.</li>
<li>Nadella envisions quantum computing being accessed via APIs, similar
to how cloud services are used today. This could democratize access to
quantum capabilities for research and industry.</li>
</ol>
<blockquote>
<p><strong>Instrumenting &amp; Evaluating LLMs - Hamei Husain</strong>
[<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=SnbGD677_u0&amp;ab_channel=HamelHusain">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How to Get Your Data Ready for AI Agents (Docs, PDFs,
Websites) - Dave Ebbelaar</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9lBTS5dM27c&amp;ab_channel=DaveEbbelaar">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The AI Cold War, Signalgate, CoreWeave IPO, Tariff Endgames,
El Salvador Deportations - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Iazo7g40VbQ&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<h2 id="github">Github</h2>
<blockquote>
<p><strong>a smol course - HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://github.com/huggingface/smol-course">Link</a>]</p>
</blockquote>
<p>Good course to help you to align LLM with specific use cases. It
includes instruction tuning, preference alignment using DPO/ORPO, LoRA,
prompt tuning, and multimodal model adaptation, and it covers creating
synthetic datasets, evaluation, and efficient inference.</p>
<blockquote>
<p><strong>AI Hedge Fund</strong> [<a
target="_blank" rel="noopener" href="https://github.com/virattt/ai-hedge-fund">Link</a>]</p>
</blockquote>
<p>POC for educational purposes.</p>
<blockquote>
<p><strong>build-your-own-x</strong> [<a
target="_blank" rel="noopener" href="https://github.com/codecrafters-io/build-your-own-x">Link</a>]</p>
</blockquote>
<p>Interesting resources / tutorials for building technologies from
scratch to deepen practical understanding.</p>
<blockquote>
<p><strong>Open R1- HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://github.com/huggingface/open-r1">Link</a>]</p>
</blockquote>
<p>A fully open reproduction of DeepSeek-R1.</p>
<blockquote>
<p><strong>AI Tutor Using RAG, Vector DB, and Groq</strong> [<a
target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1ZdVuxLTEkQJrV6EdZwEqtvoRkGF4FOY0">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>GenAI Agents: Comprehensive Repository for Development and
Implementation</strong> [<a
target="_blank" rel="noopener" href="https://github.com/NirDiamant/GenAI_Agents">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>DeepSeek Open Infra</strong> [<a
target="_blank" rel="noopener" href="https://github.com/deepseek-ai/open-infra-index/tree/main">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Awesome MCP Servers</strong> [<a
target="_blank" rel="noopener" href="https://github.com/punkpeye/awesome-mcp-servers">Link</a>]</p>
</blockquote>
<p>This repository provides access to a selection of curated Model
Context Protocol (MCP) servers designed for seamless AI model-resource
interaction. It features both production-ready and experimental servers,
offering capabilities like file access, database connections, and API
integrations. There are frameworks, tutorials, and practical tips to
enhance model deployment and maximize resource efficiency in real-world
applications.</p>
<h2 id="news">News</h2>
<blockquote>
<p><strong>Replit Integrates xAI</strong> [<a
target="_blank" rel="noopener" href="https://docs.replit.com/updates/2025/03/07/changelog#xai-integration">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Expand your research with proprietary financial data -
Perplexity</strong> [<a
target="_blank" rel="noopener" href="https://www.perplexity.ai/enterprise/crunchbase-factset-integration">Link</a>]</p>
</blockquote>
<p>Crunchbase, FactSet, and DeepSeek R1 now power enterprise
insights.</p>
<blockquote>
<p><strong>Google to acquire cloud security startup Wiz for $32 billion
after deal fell apart last year - CNBC</strong> [<a
target="_blank" rel="noopener" href="https://www.cnbc.com/2025/03/18/google-to-acquire-cloud-security-startup-wiz-for-32-billion.html">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Leave it to Manus - Manus</strong> [<a
target="_blank" rel="noopener" href="https://manus.im/">Link</a>]</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/02/09/Daring-Greatly/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/02/09/Daring-Greatly/" class="post-title-link" itemprop="url">Daring Greatly</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-02-09 21:53:40" itemprop="dateCreated datePublished" datetime="2025-02-09T21:53:40-05:00">2025-02-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>I just finished reading "Daring Greatly: How the Courage to Be
Vulnerable Transforms the Way We Live, Love, Parent, and Lead" by Brené
Brown. This is the second book of hers I have read. Her words feel like
whispers from God.</p>
<p>What she says about wholeheartedness:</p>
<blockquote>
<p>“Wholehearted living is about engaging in our lives from a place of
worthiness. It means cultivating the courage, compassion, and connection
to wake up in the morning and think, No matter what gets done and how
much is left undone, I am enough. It's going to bed at night thinking,
Yes, I am imperfect and vulnerable and sometimes afraid, but that
doesn't change the truth that I am also brave and worthy of love and
belonging.”</p>
</blockquote>
<p>What she says about vulnerability:</p>
<blockquote>
<p>“Vulnerability is based on mutuality and requires boundaries and
trust. It's not oversharing, it's not purging, it's not indiscriminate
disclosure, and it's not celebrity-style social media information dumps.
Vulnerability is about sharing our feelings and our experiences with
people who have earned the right to hear them. Being vulnerable and open
is mutual and an integral part of the trust-building process.”</p>
</blockquote>
<blockquote>
<p>“If we're going to find our way out of shame and back to each other,
vulnerability is the path and courage is the light. To set down those
lists of what we're supposed to be is brave. To love ourselves and
support each other in the process of becoming real is perhaps the
greatest single act of daring greatly.”</p>
</blockquote>
<p>What she says about perfectionism: (I love this part!)</p>
<blockquote>
<p>“The problem was thankfully never fixed, and in time the box
overflowed as more and more art piled up. I think the dilemma exists
because art, among all the other tidy categories, most closely resembles
what it is like to be human. To be alive. It is our nature to be
imperfect. To have uncategorized feelings and emotions. To make or do
things that don't sometimes necessarily make sense.</p>
<p>Art is all just perfectly imperfect.</p>
<p>My fixation with these words from Leonard Cohen's song "Anthem" comes
from how much comfort and hope they give me as I put "enough" into
practice: "There's a crack in everything. That's how the light gets
in."”</p>
</blockquote>
<p>What she says about oversharing:</p>
<blockquote>
<p>“It's an important question, and the answer is that I don't tell
stories or share vulnerabilities with the public until I've worked
through them with the people I love. I have my own boundaries around
what I share and what I don't share and I stay mindful of my
intentions.</p>
<p>First, I only share stories or experiences that I've worked through
and feel that I can share from solid ground. I don't share what I define
as "intimate" stories, nor do I share stories that are fresh wounds.</p>
<p>Second, I follow the rule that I learned in my graduate social work
training. Sharing yourself to teach or move a process forward can be
healthy and effective, but disclosing information as a way to work
through your personal stuff is inappropriate and unethical.</p>
<p>Last, I only share when I have no unmet needs that I'm trying to
fill. I firmly believe that being vulnerable with a larger audience is
only a good idea if the healing is tied to the sharing, not to the
expectations I might have for the response I get.</p>
</blockquote>
<p>What she says about disengagement:</p>
<blockquote>
<p>“Disengagement is the issue underlying the majority of problems I see
in families, schools, communities, and organizations and it takes many
forms, including the ones we discussed in the "Armory" chapter. We
disengage to protect ourselves from vulnerability, shame, and feeling
lost and without purpose. We also disengage when we feel like the people
who are leading us—our boss, our teachers, our principal, our clergy,
our parents, our politicians-aren't living up to their end of the social
contract.”</p>
</blockquote>
<blockquote>
<p>“The gap starts here: We can't give people what we don't have. Who we
are matters immeasurably more than what we know or who we want to be.
The space between our practiced values (what we're actually doing,
thinking, and feeling) and our aspirational values (what we want to do,
think, and feel) is the value gap, or what I call "the disengagement
divide." It's where we lose our employees, our clients, our students,
our teachers, our congregations, and even our own children.”</p>
</blockquote>
<p>What she says about vulnerabilities in Sales:</p>
<blockquote>
<p>“My answer was no. And yes. In that scenario vulnerability is
recognizing and owning that you don't know something; it's looking the
customer in the eye and saying, "I don't know the answer to that, but
I'll find out. I want to make sure you have the correct information." I
explained that the unwillingness to engage with the vulnerability of not
knowing often leads to making excuses, dodging the question,
or-worst-case scenario-bullshitting. That's the deathblow in any
relationship, and the one thing I've learned from talking to people who
sell for a living is that sales is all about relationships.”</p>
</blockquote>
<p>And her Daring Greatly Leadership Manifesto:</p>
<blockquote>
<p>“To the CEOs and teachers. To the principals and the managers. To the
politicians, community leaders, and decision makers:</p>
<p>We want to show up, we want to learn, and we want to inspire.</p>
<p>We are hardwired for connection, curiosity, and engagement.</p>
<p>We crave purpose, and we have a deep desire to create and
contribute.</p>
<p>We want to take risks, embrace our vulnerabilities, and be
courageous.</p>
<p>When learning and working are dehumanized, when you no longer see us
and no longer encourage our daring, or when you only see what we produce
or how we perform, we disengage and turn away from the very things that
the world needs from us: our talent, our ideas, and our passion.</p>
<p>What we ask is that you engage with us, show up beside us, and learn
from us.</p>
<p>Feedback is a function of respect; when you don't have honest
conversations with us about our strengths and our opportunities for
growth, we question our contributions and your commitment.</p>
<p>Above all else, we ask that you show up, let yourself be seen, and be
courageous. Dare Greatly with us.”</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/02/02/2025-February/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/02/02/2025-February/" class="post-title-link" itemprop="url">2025 February - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-02-02 13:20:26" itemprop="dateCreated datePublished" datetime="2025-02-02T13:20:26-05:00">2025-02-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="substack">Substack</h2>
<blockquote>
<p><em>The key aspect of managing up is to learn to speak the language
of your counterpart. If you can speak their language you can understand
their goals and fears, and you can communicate at the level they are.
You'll be in a better position to be an effective report.</em> <em>— <a
target="_blank" rel="noopener" href="https://www.linkedin.com/in/umberto-nicoletti/">Umberto
Nicoletti</a>, Head of R&amp;D at Proemion</em></p>
<p><em>The better we understand the goals that our managers have, the
less surprising their actions will be. […] Some of the situations where
managers act in ways that most dismay or surprise us are when they are
acting on their fears and worries. - <a
target="_blank" rel="noopener" href="https://monkeysthumb.co.uk/">Joe Chippindale</a>, CTO
Coach</em></p>
<p><strong>― Frameworks for Managing Up as a Software Engineer - High
Growth Engineer</strong> [<a
target="_blank" rel="noopener" href="https://read.highgrowthengineer.com/p/frameworks-for-managing-up-as-an-engineer">Link</a>]</p>
</blockquote>
<p><strong>Building Trust</strong>:</p>
<ol type="1">
<li>Sincerity — you are honest and transparent, even when it’s
uncomfortable. This includes admitting mistakes early, being upfront
with challenges, and sharing both good and bad news, without
sugar-coating the latter.</li>
<li>Reliability — this is about consistency and following through. You
do what you say you'll do, you set realistic expectations, and
communicate proactively through regular update habits. More on this
later in the <em>updates</em> section.</li>
<li>Care — you have their best interests in mind. This means
understanding their goals and challenges, being proactive in helping
them succeed, and showing empathy when things get tough.</li>
<li>Competence — finally, you deliver results. This goes beyond
technical skills: it's about delivering business value, learning and
growing from feedback, and understanding the big picture.</li>
</ol>
<p><strong>Speaking their language</strong>:</p>
<ol type="1">
<li><p><strong>Map their context</strong></p>
<ol type="1">
<li>What makes <em>you</em> successful? — What are your goals and
concerns?</li>
<li>What makes <em>me</em> successful? — How can I help you reach your
goals?</li>
</ol>
<p>The only way for <em>you</em> to be successful is to make <em>your
manager</em> successful. To do that, you need to be able to map your
goals and concerns into their own.</p></li>
<li><p><strong>Translate impact across altitudes</strong></p>
<p>For any item you report to your manager, the question you should ask
yourself is: <em>why</em> should my manager care about this? And, more
subtly: <em>what</em> does my manager care about this?</p></li>
<li><p><strong>Create explicit agreements</strong></p>
<ol type="1">
<li>Scope of ownership — do you know what decisions you can make
autonomously vs when you need to involve your manager?</li>
<li>Success criteria — how do you know if what you do is successful? Do
you know how impact will be measured?</li>
<li>Mutual expectations — do you know what your manager needs from you?
And do they know what you need from them?</li>
</ol></li>
</ol>
<p><strong>Creating effective updates</strong></p>
<ol type="1">
<li><p><strong>Define your update stack</strong></p>
<ol type="1">
<li>Async messages (daily) — about significant progress or
blockers.</li>
<li>Written reports (weekly) — structured updates about key results and
next steps.</li>
<li>1:1s (weekly or biweekly) — deeper conversations about growth,
wellbeing, and strategy.</li>
</ol></li>
<li><p><strong>Make every update count</strong></p>
<ol type="1">
<li>Why does this matter to my manager?</li>
<li>What should they do with this information?</li>
</ol></li>
<li><p><strong>Build a feedback loop</strong></p>
<p>Use 1:1s, retrospectives, and feedback moments to inspect your update
process: what's working? What feels like noise? What critical
information is missing?</p></li>
</ol>
<blockquote>
<p><strong>JD Vance's AI Summit Paris Speech - Artificial Intelligence
Survey</strong> [<a
target="_blank" rel="noopener" href="https://futuresin.substack.com/p/jd-vance-speech-ai-summit-paris">Link</a>]
[<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=64E9O1Gv99o">YouTube</a>]</p>
</blockquote>
<p>Here are some of JD Vance's main points regarding AI, on behalf of
the Trump Administration:</p>
<ul>
<li>Vance emphasizes AI's potential for revolutionary applications and
economic innovation and advocates against being too risk-averse. This is
the main stance of this optimistic speech - more of AI opportunity, less
of AI safety.</li>
<li>He states the administration aims to ensure American AI technology
remains the gold standard and the U.S. is the preferred partner for AI
expansion. The U.S. wants to partner with other countries in the AI
revolution with openness and collaboration, but this requires
international regulatory regimes that foster creation rather than
strangling it.</li>
<li>He expresses concern that excessive regulation could stifle the AI
industry and supports a deregulatory approach. He mentions the
development of an AI action plan that avoids overly precautionary
regulatory regimes, while ensuring that all Americans benefit from the
technology and its transformative potential. The administration is
troubled by foreign governments tightening regulations on U.S. tech
companies with international footprints. Vance states that preserving an
open regulatory environment has encouraged American innovators to
experiment.</li>
<li>He stresses that American AI should not be co-opted for
authoritarian censorship and should be free from ideological bias.</li>
<li>He notes the importance of building the most powerful AI systems in
the U.S. with American-designed and manufactured chips.</li>
<li>He believes AI should be a tool for job creation and making workers
more productive, prosperous, and free. The administration will always
center American workers in its AI policy and ensure that AI makes
workers more productive. For all major AI policy decisions coming from
the federal government, the Trump Administration will guarantee American
workers a seat at the table.</li>
</ul>
<figure>
<img
src="/di-blog/2025/02/02/2025-February/ai_action_submit_jd_tweets.png"
alt="ai_action_submit_jd_tweets" />
<figcaption aria-hidden="true">ai_action_submit_jd_tweets</figcaption>
</figure>
<blockquote>
<p><strong>Elon Musk Blocked a Bill to Stop Amazon from Helping Kids
Kill Themselves - BIG by Matt Stoller</strong> [<a
target="_blank" rel="noopener" href="https://www.thebignewsletter.com/p/elon-musk-blocked-a-bill-to-stop">Link</a>]</p>
</blockquote>
<p>In December, Elon Musk pushed for the reduction of government funding
legislation, which led to the removal of several provisions. One
provision removed due to Musk's intervention was the <strong>Youth
Poisoning Prevention Act</strong>, which would have prevented consumers
from buying concentrated sodium nitrite, a chemical often used in
teenage suicides. This chemical, while used in low concentrations as a
food preservative, is lethal in high concentrations and has no household
uses.</p>
<p>The article highlights that Musk, who has significant political
power, can make harmful mistakes, sometimes unknowingly. The author
notes that the removal of the provision was considered a mistake that
could be fixed. Despite bipartisan support for the priorities, there has
been no action taken to reinstate them. He questions whether anyone will
address and rectify the issues that arise from actions taken by figures
like Musk and Trump.</p>
<blockquote>
<p><strong>Deep Research, information vs. insight, and the nature of
science - Interconnects</strong> [<a
target="_blank" rel="noopener" href="https://www.interconnects.ai/p/deep-research-information-vs-insight-in-science">Link</a>]</p>
</blockquote>
<p>This is a very interesting point: the article considers how AI might
challenge Thomas Kuhn's theories of scientific revolutions. Kuhn's
<em>The Structure of Scientific Revolutions</em> describes how science
evolves, with scientists forming <strong>paradigms</strong> around
theories and using them to gain knowledge until limitations necessitate
a new paradigm. Here's how AI might challenge Kuhn's theories:</p>
<ul>
<li>AI is accelerating scientific progress, potentially faster than
paradigms can be established. The fundamental unit of scientific
progress is reducing so quickly that it redefines experimentation
methods.</li>
<li>Kuhn emphasizes that scientific knowledge is a process, not a set of
fixed ideas. AI's emergence challenges this.</li>
<li>Kuhn suggests science is done by a community that slowly builds out
the frontier of knowledge, rather than filling in a known space. The
article questions how the dynamics of science will change with AI
systems.</li>
<li>Kuhn states that to reject one paradigm requires the simultaneous
substitution of another. The article implies that AI's rapid
advancements may disrupt this process.</li>
</ul>
<blockquote>
<p><em>Check out <a target="_blank" rel="noopener" href="https://www.wired.com/tag/elon-musk/">this
impressive list</a> of stories they’ve broken since Trump took
office:</em></p>
<ul>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/plaintext-trump-musk-us-digital-service-doge/">Elon
Musk Plays DOGE Ball—and Hits America’s Geek Squad</a> (Steven Levy,
1/22/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/doge-elon-musk/?_sp=8deced32-408d-4fd0-8b2f-aacda4ff2a0a.1738769208605">DOGE
Will Allow Elon Musk to Surveil the US Government From the Inside</a>
(Vittoria Elliott, 1/24/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-lackeys-office-personnel-management-opm-neuralink-x-boring-stalin/?_sp=246a8187-8e09-45b8-bc25-ec0a6c86df8a.1738769551792">Elon
Musk Lackeys Have Taken Over the Office of Personnel Management</a>
(Vittoria Elliott, 1/28/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-twitter-playbook-federal-government/">Elon
Musk Is Running the Twitter Playbook on the Federal Government</a> (Zoë
Schiffer, 1/28/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-sleeping-doge-office/">Elon
Musk Tells Friends He’s Sleeping at the DOGE Offices in DC</a> (Zoë
Schiffer, 1/29/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-government-tech-workers-gsa-tts/">Government
Tech Workers Forced to Defend Projects to Random Elon Musk Bros</a>
(Makena Kelly, Vittoria Elliott, 1/30/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-lackeys-general-services-administration/">Elon
Musk’s Friends Have Infiltrated Another Government Agency</a> (Makena
Kelly, Zoë Schiffer, 1/31/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/us-government-websites-are-disappearing-in-real-time/">US
Government Websites Are Disappearing in Real Time</a> (Vittoria Elliott,
Dhruv Mehrotra, Dell Cameron, 2/1/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/doge-hr-elon-musk-resignation-fork-road-leaked-staff-meeting/">DOGE
Staff Had Questions About the ‘Resign’ Email. Their New HR Chief Dodged
Them</a> (Makena Kelly, 2/1/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-government-young-engineers/">The
Young, Inexperienced Engineers Aiding Elon Musk’s Government
Takeover</a> (Vittoria Elliott, 2/2/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/pronouns-removed-government-email-signatures/">Pronouns
Are Being Forcibly Removed From Government Signatures</a> (Dell Cameron,
2/3/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-lieutenant-gsa-ai-agency/">Elon
Musk Ally Tells Staff ‘AI-First’ Is the Future of Key Government
Agency</a> (Makena Kelly, 2/3/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/usaid-researchers-email-access/">Elon
Musk’s DOGE Is Still Blocking HIV/AIDS Relief Exempted From Foreign Aid
Cuts</a> (Kate Knibbs, 2/3/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-associate-bfs-federal-payment-system/">A
25-Year-Old With Elon Musk Ties Has Direct Access to the Federal Payment
System</a> (Vittoria Elliott, Dhruv Mehrotra, Leah Feiger, Tim Marchman,
2/4/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/federal-workers-sue-over-doge-server/">Federal
Workers Sue to Disconnect DOGE Server</a> (Dell Cameron,
2/4/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/chaos-usaid-staffers-sent-home/">State
Department Puts ‘All Direct Hire’ USAID Personnel on Administrative
Leave</a> (Kate Knibbs, 2/4/25)</em></li>
<li><em><a
target="_blank" rel="noopener" href="https://www.wired.com/story/trump-aides-concern-musk-takeover/">Elon
Musk’s Takeover Is Causing Rifts in Donald Trump’s Inner Circle</a>
(Jake Lahut, 2/5/25)</em></li>
</ul>
<p><strong>When It Comes to Covering Musk's Government Takeover, WIRED
Is Showing Everyone How It's Done - The Present Age</strong> [<a
target="_blank" rel="noopener" href="https://www.readtpa.com/p/when-it-comes-to-covering-musks-government">Link</a>]</p>
<p><strong>The Media Is Missing the Story: Elon Musk Is Staging a Coup -
The Present Age</strong> [<a
target="_blank" rel="noopener" href="https://www.readtpa.com/p/the-media-is-missing-the-story-elon">Link</a>]</p>
</blockquote>
<p>Mainstream media is refusing to tell the truth. WIRED deserves
support.</p>
<p>"<a
target="_blank" rel="noopener" href="https://bookshop.org/p/books/character-limit-how-elon-musk-destroyed-twitter-kate-conger/21224329"><em>Character
Limit - How Elon Musk Destroyed Twitter</em></a>" by Kate Conger and
Ryan Mac, as a reference.</p>
<blockquote>
<p><strong>2025: the Year of Datacenter Mania - AI Supremacy</strong>
[<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/2025-year-of-datacenter-mania">Link</a>]</p>
</blockquote>
<p>This is an overview of what's happening and going to happen around
Data Center Construction, covering a wide range of areas.</p>
<p><strong>AI Expansion and Energy Demand:</strong></p>
<p>The AI race is intensifying, leading to significant capital
expenditure by Big Tech and raising concerns about potential harmful
consequences. AI data centers' power demands are rapidly increasing,
with estimates of needing 10 gigawatts of additional capacity in 2025
alone.</p>
<p>Goldman Sachs Research projects a 165% increase in data center power
demand by 2030. By 2027, global AI data center power demand could reach
68 GW and 327 GW by 2030, compared to a total global data center
capacity of 88 GW in 2022. Training AI models could require up to 1 GW
in a single location by 2028 and 8 GW by 2030.</p>
<p><strong>Infrastructure and Logistical Challenges:</strong></p>
<p>Power infrastructure delays are increasing wait times for grid
connections, which can take four to seven years in key regions. Data
centers face struggles with local and state permits, especially for
backup generators and environmental impact assessments.</p>
<p>A lack of data center infrastructure in the U.S. could cause a shift
of construction to other countries. Countries with greater compute
access may gain economic and military advantages.</p>
<p><strong>Environmental and Health Concerns:</strong></p>
<p>There are growing concerns that the impact of data centers on human
health is being overlooked, and one of President Biden's executive
orders acknowledges that data centers are harmful to health.</p>
<p>The environmental cost of AI includes concerns about water
consumption, air pollution, electronic waste, and critical materials, in
addition to public health concerns around pollution.</p>
<p><strong>Energy Solutions and the Nuclear Option:</strong></p>
<p>To meet AI’s growing power needs, some experts advocate for nuclear
energy as the most viable long-term solution. Nuclear energy produces no
carbon emissions during operation and offers a reliable, constant energy
supply. Tech giants like Microsoft and Google are recognizing nuclear
energy’s potential, with Microsoft exploring small modular reactors
(SMRs). The adoption of nuclear energy faces obstacles such as high
upfront costs, regulatory hurdles, and public skepticism.</p>
<p><strong>Global AI Race and Investments:</strong></p>
<p>The EU is mobilizing $200 billion in AI investments, signifying a
global race for AI leadership. The UAE is investing billions in AI data
centers in France and is implicated, along with SoftBank and Oracle, in
OpenAI's data center project in Abilene, Texas.</p>
<p><strong>The Question of Sustainability:</strong></p>
<p>AI's rapid expansion is testing the limits of power infrastructure,
natural resources, and sustainability efforts. If AI continues to expand
at its current rate, there is a risk of a gridlocked future limited by
energy availability. The future of AI depends on sustainability and the
willingness to sacrifice energy for intelligence.</p>
<blockquote>
<p><strong>Amazon: Outspending Everyone - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/amazon-outspending-everyone">Link</a>]</p>
</blockquote>
<p>Some highlights:</p>
<ol type="1">
<li>Amazon is planning to invest over $100 billion in 2025, primarily in
AI-related infrastructure. This is more than any other company, and a
20% increase from 2024.</li>
<li>AWS revenue grew 19% Y/Y, and roughly half of the growth is
attributed to AI. AWS has a 30% market share in cloud infrastructure.
Amazon is focused on custom silicon (Trainium and Inferentia) to improve
AI efficiency.</li>
<li>The <strong>de minimis exemption</strong>, which allows imports
under $800 to avoid US tariffs, gives companies like Shein and Temu a
competitive edge. <strong>Amazon Haul</strong> was launched last year to
compete directly with these companies. Should the de minimis loophole be
eliminated, Amazon's superior logistics network could give it an
advantage in fulfillment and reliability.</li>
<li><strong>Amazon Prime's multi-faceted membership</strong> is highly
effective at reducing churn. A 2022 <a
target="_blank" rel="noopener" href="https://downloads.ctfassets.net/0o6s67aqvwnu/2cEHaMphwzN7d2L5PtnUZF/05818aacde67374a914e04e7dc024ac7/NRG_ThoughtLeadership-Subscriptions-Final_Version.pdf">study</a>
by the National Research Group found that <strong>Prime has one of the
lowest churn rates</strong>, second only to cloud storage and music
streaming services. Amazon's detailed purchase data provides advertisers
with a valuable advantage, enabling highly targeted CTV ads with
industry-leading returns on ad spend (ROAS).</li>
</ol>
<blockquote>
<p><strong><em>Uber’s Three-Pronged AV Strategy:</em></strong></p>
<ol type="1">
<li><em><strong>Fleet partnerships:</strong> Uber isn’t building its own
AVs. Instead, it partners with companies like Waymo, Motional, and
Aurora, integrating their fleets into Uber’s network.</em></li>
<li><em><strong>Hybrid model:</strong> AVs can’t handle all trips—human
drivers will fill gaps, handling extreme weather, complex routes, and
peak hours for decades.</em></li>
<li><em><strong>Fleet infrastructure:</strong> Uber is investing in
charging depots and fleet management to maximize AV asset
utilization.</em></li>
</ol>
<p><em>While Tesla is vertically integrated, its rideshare strategy may
take a different path. If Tesla adopts an asset-light model, Tesla
owners—not Tesla itself—would decide whether to list their AVs on Uber.
If maximum utilization is the goal, Uber could be the logical
choice.</em></p>
<p><em>When it comes to demand aggregation, Uber remains the undisputed
leader—its network effects ensure that as long as it aggregates supply,
demand will follow, and gross profit will scale.</em></p>
<p><em>While the rideshare market will become more fragmented, Uber
could still be the biggest fish in a much larger pond. After all, Uber
is already the Airbnb for cars.</em></p>
<p><em>Tesla has a massive opportunity once the pieces fall into place.
But with auto sales under pressure and market share declining, it still
faces a long road ahead before claiming the top spot in any
market.</em></p>
<p><strong>― Tesla vs. Uber: Collision Course? - App Economy
Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/tesla-vs-uber-collision-course">Link</a>]</p>
</blockquote>
<p>Uber's business model is one of my favorite business models. Not only
because it's asset-light, its network effect, etc, but also because it
created millions of jobs.</p>
<p>Uber's AV strategy is designed to balance innovation with
practicality, ensuring that the company remains competitive while
minimizing risks and costs. By leveraging partnerships, maintaining a
hybrid model, and investing in infrastructure, Uber is well-positioned
to lead the transition to autonomous mobility.</p>
<p>While a partnership between Uber and Tesla is possible and could
offer significant synergies, it is not guaranteed. The decision would
depend on whether both companies can align their goals and overcome
competitive tensions. If Tesla decides to prioritize its own
ride-hailing network (Tesla Network), it may choose to compete rather
than collaborate with Uber. However, if Tesla sees more value in
leveraging Uber’s platform and customer base, a partnership could be a
strategic move for both companies.</p>
<figure>
<img src="/di-blog/2025/02/02/2025-February/uber_investor_pre.png"
alt="uber_investor_pre" />
<figcaption aria-hidden="true">uber_investor_pre</figcaption>
</figure>
<blockquote>
<p><strong>Microsoft: AI Efficiency Paradox - App Economy
Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/microsoft-ai-demand-paradox">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Google: Capex Arms Race - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/google-capex-arms-race">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The End of Search, The Beginning of Research - One Useful
Thing</strong> [<a
target="_blank" rel="noopener" href="https://www.oneusefulthing.org/p/the-end-of-search-the-beginning-of">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong><em>Huang’s take:</em></strong> <em>“We've really only tapped
consumer AI and search and some amount of consumer generative AI,
advertising, recommenders, kind of the early days of software. […]
Future reasoning models can consume much more compute.”</em></p>
<p><em>DeepSeek-R1, he said, has “ignited global enthusiasm” and will
push reasoning AI into even more compute-intensive
applications.</em></p>
<p><strong>― NVIDIA: AI's 3 Scaling Laws - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/nvidia-ais-3-scaling-laws">Link</a>]</p>
</blockquote>
<p>Huang introduced a framework for AI’s evolving compute demands,
outlining three scaling laws:</p>
<ul>
<li><strong>Pre-training scaling:</strong> Traditional model growth
through data consumption, now enhanced by multimodal learning and
reasoning-based data.</li>
<li><strong>Post-training scaling:</strong> The fastest-growing compute
demand, driven by reinforcement learning from human and AI feedback.
This phase now exceeds pre-training in compute usage due to the
generation of synthetic data.</li>
<li><strong>Inference &amp; reasoning scaling:</strong> The next major
shift, where AI engages in complex reasoning (e.g., chain-of-thought,
search). Inference already requires 100x more compute than early LLMs
and could scale to millions of times more.</li>
</ul>
<p>Jensen Huang outlined a three-layer AI transformation across
industries:</p>
<ul>
<li><strong>Agentic AI (Enterprise AI):</strong> AI copilots and
automation tools boosting productivity in sectors like automotive,
finance, and healthcare.</li>
<li><strong>Physical AI (AI for Machines):</strong> AI-driven training
systems for robotics, warehouses, and autonomous vehicles.</li>
<li><strong>Robotic AI (AI in the Real World):</strong> AI enabling
real-world interaction and navigation, from self-driving cars to
industrial robots.</li>
</ul>
<blockquote>
<p><strong>Grab: The Uber Slayer - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/grab-the-uber-slayer">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>DeepSeek isn’t a threat—it’s <strong>validation</strong>. If AI
inference costs are falling, Meta stands to benefit more than almost any
other company. Instead of challenging its strategy, DeepSeek reinforces
that heavy AI investments will pay off—not the other way
around.</em></p>
<p><strong>― Meta: DeepSeek Tailwinds - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/meta-deepseek-tailwinds">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Elon Musk and spiky intelligence - Silver Bulletin</strong>
[<a
target="_blank" rel="noopener" href="https://www.natesilver.net/p/elon-musk-and-spiky-intelligence">Link</a>]</p>
</blockquote>
<p>Interesting study on spiky intelligence, using Elon as a case study.
Concepts highlights:</p>
<p><strong>Spiky Intelligence</strong>: This refers to individuals who
exhibit exceptional abilities in certain areas while being deficient in
others. It contrasts with the idea of general intelligence (the "g
factor"), where most cognitive abilities are positively correlated.
Spiky intelligence is often seen in people who excel in abstract,
analytical reasoning but may lack emotional intelligence, empathy, or
practical judgment.</p>
<p><strong>Berkson’s Paradox</strong>: This statistical phenomenon
explains why successful individuals often appear to have significant
weaknesses. In highly competitive fields, it’s rare to find people who
excel in all dimensions, so success often goes to those with a few
standout traits.</p>
<figure>
<img
src="/di-blog/2025/02/02/2025-February/beerkson_paradox_with_selection_effects.png"
alt="beerkson_paradox_with_selection_effects" />
<figcaption
aria-hidden="true">beerkson_paradox_with_selection_effects</figcaption>
</figure>
<h2 id="youtube-and-podcasts">YouTube and Podcasts</h2>
<blockquote>
<p><strong>DOGE vs USAID, Crypto Framework, Google's $75B AI Spend, US
Sovereign Wealth Fund, GLP-1s - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=R3q5TrwSek0&amp;list=PLn5MTSAqaf8peDZQ57QkJBzewJU1aUokl&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI
Megaclusters - Lex Fridman Podcast #459</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/hashtag/459">Link</a>] [<a
target="_blank" rel="noopener" href="https://lexfridman.com/deepseek-dylan-patel-nathan-lambert-transcript">Transcript</a>]</p>
</blockquote>
<p>This is a very good one. 5 hours intro and overview of current AI
landscape.</p>
<blockquote>
<p><em>Those driver jobs weren't even there 10 years ago. Uber came
along and created all these driver jobs. DoorDash created all these
driver jobs. So what technology does—yes, technology destroys jobs—but
it replaces them with opportunities that are even better. And then,
either you can go capture that opportunity yourself, or an entrepreneur
will come along and create something that allows you to capture those
opportunities. AI is a productivity tool. It increases the productivity
of a worker; it allows them to do more creative work and less repetitive
work. As such, it makes them more valuable. Yes, there is some
retraining involved, but not a lot. These are natural language
computers—you can talk to them in plain English, and they talk back to
you in plain English. But I think David is absolutely right. I think we
will see job creation by AI that will be as fast or faster than job
destruction. You saw this even with the internet. Like, YouTube came
along—look at all these YouTube streamers and influencers. That didn’t
used to be a job. New jobs—really, opportunities—because 'job' is the
wrong word. 'Job' implies someone else has to give it to me, like
they're handed out, as if it's a zero-sum game. Forget all that—it's
opportunities. After COVID, look at how many people are making money by
working from home in mysterious little ways on the internet that you
can't even quite grasp. - Naval Ravikant</em></p>
<p><em>you know as long as you remain adaptive and you keep learning and
you learn how to take advantage of these tools you should do better and
if you wall yourself off from the technology and don't take advantage of
it that's when you put yourself at risk - David Sacks</em></p>
<p><em>If you trained on the open web, your model should be open
source.</em> – Naval Ravikant.</p>
<p><em>To keep the conversation moving, let me segue a point that came
up that was really important into tariffs. And the point is, even though
the internet was open, the U.S. won a lot of the internet—a lot of U.S.
companies won the internet. And they won that because we got there "the
firstest with the mostest," as they say in the military. And that
matters because a lot of technology businesses have scale economies and
network effects underneath, even basic brand-based network effects. If
you go back to the late '90s and early 2000s, very few people would have
predicted that we would have ended up with Amazon basically owning all
of e-commerce. You would have thought it would have been perfect
competition and very spread out. And that applies to how we end up with
Uber as basically one taxi service or how we end up with Airbnb.
Meta—Airbnb—it's just network effects, network effects, network effects
rule the world around me. But when it comes to tariffs and when it comes
to trade, we act like network effects don't exist. The classic Ricardian
comparative advantage dogma says that you should produce what you're
best at, I produce what I'm best at, and we trade. And then, even if you
want to charge me more for it—if you want to impose tariffs for me to
ship to you—I should still keep tariffs down because I'm better off.
You're just selling me stuff cheaply—great. Or if you want to subsidize
your guys—great, you're selling me stuff cheaply. The problem is, that
is not how most modern businesses work. Most modern businesses have
network effects. As a simple thought experiment, suppose that we have
two countries, right? I'm China, you're the U.S. I start out by
subsidizing all of my companies and industries that have network
effects. So I'll subsidize TikTok, I'll ban your social media but push
mine. I will subsidize my semiconductors, which tend to have
winner-take-all dynamics in certain categories. Or I'll subsidize my
drones and then, exactly—BYD, self-driving, whatever. And then, when I
win, I own the whole market and I can raise prices. And if you try to
start up a competitor, it's too late—I've got network effects. Or if
I've got scale economies, I can lower my price to zero, crash you out of
business, no one in their right mind will invest, and then I'll raise
prices right back up. So you have to understand that certain industries
have hysteresis, or they have network effects, or they have economies of
scale—and these are all the interesting ones. These are all the
high-margin businesses. So in those, if somebody is subsidizing or
they're raising tariffs against you to protect their industries and let
them develop, you do have to do something. You can't just completely
back down. - Naval Ravikant</em></p>
<p><em>I think Sam and his team would do better to leave the nonprofit
part alone, leave an actual independent nonprofit board in charge, and
then have a strong incentive plan and a strong fundraising plan for the
investors and the employees. So I think this is workable. It's just that
trying to grab it all seems way off, especially when it was built on
open algorithms from Google, open data from the web, and nonprofit
funding from Elon and others. - Naval Ravikant</em></p>
<p><strong>― JD Vance's AI Speech, Techno-Optimists vs Doomers, Tariffs,
AI Court Cases with Naval Ravikant - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=AI5qI6ej-yM">Link</a>]</p>
</blockquote>
<p>"AI won't take your job; it's someone using AI that will take your
job." – Richard Baldwin. The discussion around AI's impact on jobs is
often framed as a zero-sum game, but the reality is more nuanced. While
AI will displace certain jobs (e.g., self-driving cars replacing
drivers), it will also create new opportunities and industries that we
can't yet fully envision. The key is adaptability—those who learn to use
AI tools will thrive, while those who resist will fall behind.</p>
<blockquote>
<p><strong>The Stablecoin Future, Milei's Memecoin, DOGE for the DoD,
Grok 3, Why Stripe Stays Private - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=OxP55dZjqZs">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How to build full-stack apps with OpenAI o1 pro - Part 1 -
Mckay Wrigley</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Y4n_p9w8pGY&amp;ab_channel=MckayWrigley">Link</a>]</p>
</blockquote>
<p>Learn app development using OpenAI o1-Pro with a structured
six-prompt workflow.</p>
<blockquote>
<p><strong>Open Deep Research - LangChain</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2mSNIX-l_Zc&amp;ab_channel=LangChain">Link</a>]</p>
</blockquote>
<p>Build and run a deep research agent with LangGraph Studio, customize
configurations, compare architectures, and analyze costs.</p>
<h2 id="paper-and-reports">Paper and Reports</h2>
<blockquote>
<p><strong>Probabilistic weather forecasting with machine
learning</strong> [<a
target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-024-08252-9">Link</a>]</p>
</blockquote>
<p>GenCast's success stems from its ability to generate ensembles of
sharp, realistic weather trajectories and well-calibrated probability
distributions.</p>
<p>The methodology of GenCast involves several key components:</p>
<ul>
<li>GenCast employs a <strong>second-order Markov assumption</strong>,
meaning it conditions its predictions on the two previous weather
states, rather than just one. This is done because conditioning on two
previous time steps works better than one.</li>
<li>GenCast is implemented as a <strong>conditional diffusion
model</strong>. Diffusion models are generative machine learning methods
that can model the probability distribution of complex data and generate
new samples by iteratively refining a noisy initial state. The model
predicts a residual with respect to the most recent weather state. The
sampling process begins with random noise, which is then refined over a
series of steps.</li>
<li>At each step of the iterative refinement process, GenCast uses a
<strong>denoiser neural network</strong>. This network is trained to
remove noise that has been artificially added to atmospheric states. The
architecture of the denoiser includes an encoder, a processor, and a
decoder. The encoder maps the noisy target state to an internal
representation on a refined icosahedral mesh, the processor is a
<strong>graph transformer</strong>, and the decoder maps the internal
mesh representation back to a denoised target state.</li>
<li>GenCast uses a noise distribution that respects the spherical
geometry of global weather variables. Rather than using independent and
identically distributed (i.i.d.) Gaussian noise on the
latitude-longitude grid, it samples <strong>isotropic Gaussian white
noise</strong> on the sphere and projects it onto the grid.</li>
<li>GenCast's performance is evaluated using various metrics, including:
<ul>
<li><strong>CRPS (Continuous Ranked Probability Score):</strong>
Measures the skill of a probabilistic forecast.</li>
<li><strong>RMSE (Root Mean Squared Error):</strong> Measures how
closely the mean of an ensemble of forecasts matches the ground
truth.</li>
<li><strong>Spread/Skill Ratios and Rank Histograms:</strong> Used to
evaluate the calibration of the forecast distributions.</li>
<li><strong>Brier Skill Score:</strong> Evaluates probabilistic
forecasts of binary events, specifically the prediction of extreme
weather events.</li>
<li><strong>Relative Economic Value (REV):</strong> Characterizes the
potential value of a forecast over a range of probability decision
thresholds.</li>
<li><strong>Spatially Pooled CRPS:</strong> Evaluates forecasts
aggregated over circular spatial regions of varying sizes to assess the
model's ability to capture spatial dependencies.</li>
<li><strong>Regional Wind Power Forecasting:</strong> Evaluates the
model's ability to predict wind power generation at wind farm locations
using a standard idealized power curve.</li>
<li><strong>Tropical Cyclone Track Prediction:</strong> Uses the
TempestExtremes tropical cyclone tracker to extract cyclone trajectories
from the forecast and analysis data. The model's ability to forecast
cyclone tracks is evaluated using position error and track
probability.</li>
</ul></li>
</ul>
<blockquote>
<p><em>The United States currently leads the world in data centers and
AI compute, but unprecedented demand leaves the industry struggling to
find the power capacity needed for rapidly building new data centers.
Failure to address current bottlenecks may compel U.S. companies to
relocate AI infrastructure abroad, potentially compromising the U.S.
competitive advantage in compute and AI and increasing the risk of
intellectual property theft.</em></p>
<p><strong>― AI's Power Requirements Under Exponential Growth -
RAND</strong> [<a
target="_blank" rel="noopener" href="https://www.rand.org/pubs/research_reports/RRA3572-1.html">Link</a>]
[<a
target="_blank" rel="noopener" href="https://www.rand.org/content/dam/rand/pubs/research_reports/RRA3500/RRA3572-1/RAND_RRA3572-1.pdf">pdf</a>]</p>
</blockquote>
<blockquote>
<p><strong>Genome modeling and design across all domains of life with
Evo 2 - Arc Institute</strong> [<a
target="_blank" rel="noopener" href="https://arcinstitute.org/manuscripts/Evo2">Link</a>]</p>
</blockquote>
<p>Evo 2 is a powerful genome modeling and design tool that operates
across all domains of life. It can analyze and generate genetic
sequences from molecular to genome scale. It accurately assigns
likelihood scores to human disease variants, distinguishing between
pathogenic and benign mutations in both coding and noncoding regions. It
can predict whether genes are essential or nonessential using mutational
likelihoods, helping in bacterial and phage gene essentiality studies.
It can generate large-scale DNA sequences with structured features like
tRNAs, promoters, and genes with intronic structures. It provides
zero-shot fitness predictions for protein and non-coding RNA sequences,
correlating well with experimental fitness measurements. It robustly
predicts the pathogenicity of various mutation types, achieving
state-of-the-art performance for noncoding and splice variants.</p>
<blockquote>
<p><strong>Large Action Models: From Inception to
Implementation</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.10047v1">Link</a>]</p>
</blockquote>
<p>Microsoft Research published one of the most complete papers in this
area, outlining a complete framework for large action models (LAMs)
models. The core idea is to bridge the gap between the language
understanding capability of LLMs and the need for real-world action
execution.</p>
<figure>
<img src="/di-blog/2025/02/02/2025-February/microsoft_lam.png"
alt="microsoft_lam" />
<figcaption aria-hidden="true">microsoft_lam</figcaption>
</figure>
<blockquote>
<p><strong>Towards an AI co-scientist</strong> [<a
target="_blank" rel="noopener" href="https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Uncovering the Impact of Chain-of-Thought Reasoning for
Direct Preference Optimization: Lessons from Text-to-SQL</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.11656">Link</a>]</p>
</blockquote>
<p>Direct Preference Optimization (DPO) does not consistently improve
performance in the Text-to-SQL task and sometimes even degrades it.
Existing Standard Fine-Tuning (SFT) methods are limited by the lack of
high-quality training data, and prompting-based methods are expensive,
slow, and raise data privacy concerns.</p>
<p>To solve the problems, they generate <strong>synthetic CoT
solutions</strong> to improve training datasets, leading to <strong>more
stable and significant performance improvements</strong> in DPO. They
integrate <strong>execution-based feedback</strong> to refine the
model’s SQL generation process, making the optimization process more
reliable. And they create a <strong>quadruple-based preference
dataset</strong> to help the model learn to distinguish between correct
and incorrect SQL responses more effectively.</p>
<blockquote>
<p><strong>MONA: Myopic Optimization with Non-myopic Approval Can
Mitigate Multi-step Reward Hacking</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.13011">Link</a>]</p>
</blockquote>
<p>Google DeepMind developed an innovative approach - Myopic
Optimization with Non-myopic Approval (MONA), to mitigate multi-step
reward hacking. This MONA methodology is built on two key principles.
The first is <strong>myopic optimization</strong>, where agents focus on
maximizing rewards for immediate actions rather than planning multi-step
strategies. This ensures that agents do not develop complex,
unintelligible tactics. The second principle is <strong>non-myopic
approval</strong>, where human overseers assess the agent's actions
based on their expected long-term utility. These evaluations serve as
the primary mechanism for guiding agents toward behavior aligned with
human-defined objectives, without relying on direct feedback from
outcomes.</p>
<figure>
<img src="/di-blog/2025/02/02/2025-February/google_deepmind_mona.png"
alt="google_deepmind_mona" />
<figcaption aria-hidden="true">google_deepmind_mona</figcaption>
</figure>
<blockquote>
<p><strong>The Ultra-Scale Playbook: Training LLMs on GPU Clusters -
Hugging Face</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high_level_overview">Link</a>]</p>
</blockquote>
<p>This book from Hugging Face explains 5D parallelism, ZeRO, CUDA
kernel optimizations, and compute-communication overlap in large-scale
AI training. It breaks down scaling bottlenecks, PyTorch internals, and
parallelism techniques like ZeRO-3, pipeline, sequence, and context
parallelism.</p>
<h2 id="articles-and-blogs">Articles and Blogs</h2>
<blockquote>
<p><em>The research found six distinct leadership styles, each springing
from different components of emotional intelligence. The styles, taken
individually, appear to have a direct and unique impact on the working
atmosphere of a company, division, or team, and in turn, on its
financial performance. And perhaps most important, the research
indicates that leaders with the best results do not rely on only one
leadership style; they use most of them in a given week—seamlessly and
in different measure—depending on the business situation. Imagine the
styles, then, as the array of clubs in a golf pro’s bag. Over the course
of a game, the pro picks and chooses clubs based on the demands of the
shot. Sometimes he has to ponder his selection, but usually it is
automatic. The pro senses the challenge ahead, swiftly pulls out the
right tool, and elegantly puts it to work. That’s how high-impact
leaders operate, too.</em></p>
<p><em>Leaders who have mastered four or more—especially the
authoritative, democratic, affiliative, and coaching styles—have the
very best climate and business performance.</em></p>
<p><em>The leader can build a team with members who employ styles she
lacks.</em></p>
<p><strong>― Leadership That Gets Results - Harvard Business
Review</strong> [<a
target="_blank" rel="noopener" href="https://hbr.org/2000/03/leadership-that-gets-results">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>WeatherNext - Google DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://deepmind.google/technologies/weathernext/#access-weathernext">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>GenCast predicts weather and the risks of extreme conditions
with state-of-the-art accuracy - Google DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://deepmind.google/discover/blog/gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-sota-accuracy/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Morgan Stanley stated that ASICs perform exceptionally well in
certain specific application scenarios, but are highly dependent on the
custom needs of particular clients; the development cost of ASICs is
usually lower, but their system costs and Software deployment costs may
be much higher than GPUs that can be commercially scaled, leading to a
higher total cost of ownership. In addition, NVIDIA's CUDA ecosystem is
very mature and widely used in Global Cloud Computing Services, with a
market position that remains as solid as ever.</em></p>
<p><strong>― Morgan Stanley: ASICs are overheated, and NVIDIA's position
is difficult to shake. - moomoo</strong> [<a
target="_blank" rel="noopener" href="https://www.moomoo.com/news/post/49238491/morgan-stanley-asics-are-overheated-and-nvidia-s-position-is?level=1&amp;data_ticket=1739666344971762">Link</a>]</p>
</blockquote>
<p>NVIDIA possesses a robust competitive advantage in the AI chip market
due to its mature ecosystem, continuous R&amp;D investments, and strong
technical capabilities.</p>
<ul>
<li><strong>NVIDIA's CUDA ecosystem</strong> is well-established,
enabling clients to easily deploy and run various workloads. The
maturity of this ecosystem means that customers may find it easier to
use NVIDIA products compared to adapting software for ASICs or other
alternatives.</li>
<li>NVIDIA has a <strong>leading position in the AI chip
market</strong>, which is reinforced by its <strong>presence on every
cloud platform across the globe</strong>. Investments within NVIDIA's
ecosystem benefit from global dissemination, further solidifying its
market dominance.</li>
<li>NVIDIA <strong>invests significantly in R&amp;D</strong>. The
company is expected to invest approximately <span
class="math inline">\(\$16\)</span> billion in R&amp;D this year. This
level of investment allows NVIDIA to <strong>maintain a 4-5 year
development cycle</strong> and continuously introduce leading
high-performance chips. Custom ASIC development budgets are typically
smaller (less than <span class="math inline">\(\$1\)</span> billion),
giving NVIDIA an edge in innovation.</li>
<li>NVIDIA is difficult to surpass in providing <strong>high-end
training capabilities</strong>. The company focuses on training
multi-modal AGI models.</li>
</ul>
<blockquote>
<p><em>This means DeepResearch can identify cross-domain links or
examples that might otherwise be overlooked, offering fresh
perspectives. In professional settings, this can support more
well-rounded decision-making – for example, a product manager can
quickly gather insights from scientific research, market data, and
consumer opinions in one place, rather than relying on multiple teams or
lengthy research processes. It makes you multifaceted!</em></p>
<p><strong>― #87: Why DeepResearch Should Be Your New Hire - Turing
Post</strong> [<a
target="_blank" rel="noopener" href="https://www.turingpost.com/p/fod87?_bhlid=b5475ffe865a70cea5fc76bc1589c7fbada04657&amp;utm_campaign=87-why-deepresearch-should-be-your-new-hire&amp;utm_medium=newsletter&amp;utm_source=www.turingpost.com">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Deep Research and Knowledge Value - Stratechery</strong> [<a
target="_blank" rel="noopener" href="https://stratechery.com/2025/deep-research-and-knowledge-value/">Link</a>]</p>
</blockquote>
<p>OpenAI launched Deep Research in ChatGPT, which is an agentic
capability that conducts multi-step research on the internet for complex
tasks. It synthesizes knowledge in an economically valuable way but does
not create new knowledge.</p>
<p>As demonstrated in the article, it can be useful for researching
people and companies before conducting interviews. However, it can also
produce reports that are completely wrong by missing major entities in
an industry.</p>
<p>This is a good point - <strong>The Internet revealed that news was
worthless in terms of economic value because the societal value does not
translate to economic value. Deep Research reveals how much more could
be known, but the increasing amount of "slop" makes it more difficult to
find the right information. Information that matters and is not on the
Internet has future economic value wrapped up in it.</strong></p>
<p>Proprietary data is valuable, and AI tools like Deep Research make it
more difficult to harvest alpha from reading financial filings.
Prediction markets may become more important as AI increases the
incentive to keep things secret.</p>
<p>As a summary of the impact - Deep Research is a good value, but
<strong>it is limited by the quality of information on the Internet and
the quality of the prompt.</strong> There is value in the search for and
sifting of information, and this may be lost with on-demand reports. AI
will replace knowledge work. Secrecy is a form of friction that imposes
scarcity on valuable knowledge. Deep Research is not yet good at
understanding some things.</p>
<blockquote>
<p><strong>Massive Foundation Model for Biomolecular Sciences Now
Available via NVIDIA BioNeMo - NVIDIA Blog</strong> [<a
target="_blank" rel="noopener" href="https://blogs.nvidia.com/blog/evo-2-biomolecular-ai/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Grok 3: Intelligence, Performance &amp; Price Analysis -
Artificial Analysis</strong> [<a
target="_blank" rel="noopener" href="https://artificialanalysis.ai/models/grok-3">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Grok resets the AI race - The Verge</strong> [<a
target="_blank" rel="noopener" href="https://www.theverge.com/command-line-newsletter/617780/grok-3-elon-musk-ai-race-chatgpt">link</a>]</p>
</blockquote>
<p>Grok-3 (chocolate) is the first-ever model to break 1400 score and is
now #1 in <a
target="_blank" rel="noopener" href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard">Arena</a>.</p>
<blockquote>
<p><strong>Grok 3 Beta — The Age of Reasoning Agents - Grok
Blog</strong> [<a target="_blank" rel="noopener" href="https://x.ai/blog/grok-3">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Motivated by unmet needs in the modern scientific discovery
process and building on <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.05530">recent AI advances</a>,
including the ability to synthesize across complex subjects and to
perform <a
target="_blank" rel="noopener" href="https://deepmind.google/technologies/gemini/flash-thinking/">long-term
planning and reasoning</a>, we developed an <a
target="_blank" rel="noopener" href="https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf">AI
co-scientist system</a>. The AI co-scientist is a multi-agent AI system
that is intended to function as a collaborative tool for scientists.
Built on <a
target="_blank" rel="noopener" href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">Gemini
2.0, AI co-scientist is</a> designed to mirror the reasoning process
underpinning the scientific method. Beyond standard literature review,
summarization and “deep research” tools, the AI co-scientist system is
intended to uncover new, original knowledge and to formulate
demonstrably novel research hypotheses and proposals, building upon
prior evidence and tailored to specific research objectives.</em></p>
<p><strong>― Accelerating scientific breakthroughs with an AI
co-scientist - Google Blog</strong> [<a
target="_blank" rel="noopener" href="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/">Link</a>]</p>
</blockquote>
<figure>
<img
src="/di-blog/2025/02/02/2025-February/AICoScientist-1-Components.png"
alt="AICoScientist-1-Components" />
<figcaption aria-hidden="true">AICoScientist-1-Components</figcaption>
</figure>
<blockquote>
<p><strong>An Interview with Uber CEO Dara Khosrowshahi About
Aggregation and Autonomy - Stratechery</strong> [<a
target="_blank" rel="noopener" href="https://stratechery.com/2025/an-interview-with-uber-ceo-dara-khosrowshahi-about-aggregation-and-autonomy/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>OpenAI Deep Research Guide - DAIR.AI</strong> [<a
target="_blank" rel="noopener" href="https://docs.google.com/document/d/1vLaEMu5jirQT5RK0cW8RUXNFQyszMQ-xrjxUZF2wOg4/edit?pli=1&amp;tab=t.0#heading=h.2y9eo2rdwxv2">Link</a>]</p>
</blockquote>
<p>This is a super helpful guide to Deep Research Tools.</p>
<p>You provide use cases and examples, very inspiring, and tips,
etc.</p>
<figure>
<img src="/di-blog/2025/02/02/2025-February/usecases_examples.png"
alt="usecases_examples" />
<figcaption aria-hidden="true">usecases_examples</figcaption>
</figure>
<figure>
<img
src="/di-blog/2025/02/02/2025-February/claude_deepresearch_usecases_chart.png"
alt="claude_deepresearch_usecases_chart" />
<figcaption
aria-hidden="true">claude_deepresearch_usecases_chart</figcaption>
</figure>
<blockquote>
<p><em>Studies on the brain affirm the benefits of Tom’s visualization
technique: Imagining something in vivid detail can fire the same brain
cells actually involved in doing that activity. The new brain circuitry
appears to go through its paces, strengthening connections, even when we
merely repeat the sequence in our minds. So to alleviate the fears
associated with trying out riskier ways of leading, we should first
visualize some likely scenarios. Doing so will make us feel less awkward
when we actually put the new skills into practice.</em></p>
<p><strong>― Primal Leadership: The Hidden Driver of Great Performance -
Harvard Business Review</strong> [<a
target="_blank" rel="noopener" href="https://hbr.org/2001/12/primal-leadership-the-hidden-driver-of-great-performance">Link</a>]</p>
</blockquote>
<p>Imagine it, fake it, and make it.</p>
<blockquote>
<p><em>Our research tells us that three conditions are essential to a
group’s effectiveness: trust among members, a sense of group identity,
and a sense of group efficacy.</em></p>
<p><strong>― Building the Emotional Intelligence of Groups - Harvard
Business Review</strong> [<a
target="_blank" rel="noopener" href="https://hbr.org/2001/03/building-the-emotional-intelligence-of-groups">Link</a>]</p>
</blockquote>
<p>Team is so important to leaders.</p>
<blockquote>
<p><em>Interrupt the ascent.</em></p>
<p><em>When people are continually promoted within their areas of
expertise, they don’t have to stray far from their comfort zones, so
they seldom need to ask for help, especially if they’re good problem
solvers. Accordingly, they may become overly independent and fail to
cultivate relationships with people who could be useful to them in the
future. What’s more, they may rely on the authority that comes with rank
rather than learning how to influence people. A command-and-control
mentality may work in certain situations, particularly in lower to
middle management, but it’s usually insufficient in more senior
positions, when peer relationships are critical and success depends more
on the ability to move hearts and minds than on the ability to develop
business solutions.</em></p>
<p><strong>― The Young and the Clueless - Harvard Business
Review</strong> [<a
target="_blank" rel="noopener" href="https://hbr.org/2002/12/the-young-and-the-clueless">Link</a>]</p>
</blockquote>
<p>Don't fall into the independence trap.</p>
<blockquote>
<p><strong>Accelerating scientific breakthroughs with an AI co-scientist
- Google Research</strong> [<a
target="_blank" rel="noopener" href="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/">Link</a>]</p>
</blockquote>
<h2 id="news-and-comments">News and Comments</h2>
<blockquote>
<p><strong>Introducing deep research - Open AI</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/introducing-deep-research/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Introducing Perplexity Deep Research - Perplexity</strong>
[<a
target="_blank" rel="noopener" href="https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research">Link</a>]</p>
</blockquote>
<p>Trend is on deep research.</p>
<blockquote>
<p><strong>Shopify Tells Employees to Just Say No to Meetings -
Bloomberg</strong> [<a
target="_blank" rel="noopener" href="https://www.bloomberg.com/news/articles/2023-01-03/shopify-ceo-tobi-lutke-tells-employees-to-just-say-no-to-meetings">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Who will control the future of AI? - The Washington
Post</strong> [<a
target="_blank" rel="noopener" href="https://www.washingtonpost.com/opinions/2024/07/25/sam-altman-ai-democracy-authoritarianism-future/">Link</a>]</p>
</blockquote>
<p>Sam promotes a U.S.-led strategy to ensure AI development aligns with
democratic values and remains under the leadership of the U.S. and its
allies.</p>
<blockquote>
<p><em>This new architecture used to develop the Majorana 1 processor
offers a clear path to fit a million qubits on a single chip that can
fit in the palm of one’s hand.</em></p>
<p><em>Microsoft is now one of two companies to be <a
target="_blank" rel="noopener" href="https://www.darpa.mil/news/2025/quantum-computing-approaches">invited
to move to the final phase</a> of DARPA’s Underexplored Systems for
Utility-Scale Quantum Computing (US2QC) program – one of the programs
that makes up DARPA’s larger <a
target="_blank" rel="noopener" href="https://www.darpa.mil/research/programs/quantum-benchmarking-initiative">Quantum
Benchmarking Initiative</a> – which aims to deliver the industry’s first
utility-scale fault-tolerant quantum computer, or one whose
computational value exceeds its costs.</em></p>
<p><strong>― Microsoft’s Majorana 1 chip carves new path for quantum
computing - Microsoft</strong> [<a
target="_blank" rel="noopener" href="https://news.microsoft.com/source/features/ai/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/">Link</a>]</p>
</blockquote>
<p>In the near term, Google’s approach with superconducting qubits (like
Willow) is more mature. This technology has already demonstrated
impressive benchmarks and is backed by years of incremental
improvements. Its error correction techniques, while still challenging,
are well‑studied, and scaling up using transmon qubits is an area where
significant progress has been made.</p>
<p>On the other hand, Microsoft’s topological approach with Majorana 1
aims to use a completely new type of qubit—one that is “protected by
design” thanks to its topological nature. In theory, this means lower
error rates and potentially a much more scalable architecture with fewer
physical qubits needed per logical qubit. However, this method is still
very experimental, and questions remain over whether true Majorana zero
modes have been reliably created and controlled.</p>
<p>In summary, for near‑term practical applications, Google’s path
appears to be the safer bet. But if Microsoft’s topological qubit
platform can overcome its technical hurdles, it may ultimately provide a
more efficient and scalable route to fault‑tolerant quantum
computing.</p>
<blockquote>
<p><strong>Tencent's Weixin app, Baidu launch DeepSeek search testing -
Reuters</strong> [<a
target="_blank" rel="noopener" href="https://www.reuters.com/technology/artificial-intelligence/tencents-messaging-app-weixin-launches-beta-testing-with-deepseek-2025-02-16/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>OpenAI tries to ‘uncensor’ ChatGPT - Techcrunch</strong> [<a
target="_blank" rel="noopener" href="https://techcrunch.com/2025/02/16/openai-tries-to-uncensor-chatgpt/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Elon Musk Ally Tells Staff ‘AI-First’ Is the Future of Key
Government Agency - WIRED</strong> [<a
target="_blank" rel="noopener" href="https://www.wired.com/story/elon-musk-lieutenant-gsa-ai-agency/">Link</a>]</p>
</blockquote>
<p>Thomas Shedd, a former Tesla engineer and ally of Elon Musk, is
implementing an "AI-first strategy" at the General Services
Administration's Technology Transformation Services (TTS). Shedd
envisions the agency operating like a software startup, automating tasks
and centralizing federal data. This shift is causing concern among GSA
staff, who report being thrown into unexpected meetings and facing
potential workforce cuts. Shedd is promoting collaboration between TTS
and the United States Digital Services (DOGE), though specifics about
the new AI-driven projects and data repository remain unclear. A
cybersecurity expert expressed concern that automating government tasks
is difficult and the attempt is raising red flags. Employees also voiced
concerns regarding working hours and potential job losses.</p>
<blockquote>
<p><strong>OpenAI o3-mini - OpenAI</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/openai-o3-mini/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Introducing deep research - OpenAI</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/introducing-deep-research/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Gemini 2.0 is now available to everyone - Google
DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The all new le Chat: Your AI assistant for life and work -
Mistral AI</strong> [<a
target="_blank" rel="noopener" href="https://mistral.ai/news/all-new-le-chat">Link</a>]</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/02/01/DeepSeek-R1-Pivotal-Moment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/02/01/DeepSeek-R1-Pivotal-Moment/" class="post-title-link" itemprop="url">DeepSeek-R1: Pivotal Moment</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-02-01 21:21:58" itemprop="dateCreated datePublished" datetime="2025-02-01T21:21:58-05:00">2025-02-01</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>My thoughts regarding the AI landscape at the current stage:</p>
<p>As open-source AI becomes more affordable, it is poised to become as
ubiquitous and accessible as electricity—financially viable for
everyone. The AI and AGI arms race, whether between nations, open- and
closed-source models, or competing companies, is effectively over or
should be over, and the outcome is clear. Compute power still remains
essential, but semiconductor giants like NVIDIA should look beyond
language model training and inference, shifting their focus to the next
frontiers, such as robotics and world models. Now is the time for
developers and startups to concentrate on the vertical integration of
AI, where real economic value can be realized.</p>
<h2 id="deepseek---background">DeepSeek - Background</h2>
<p>DeepSeek began as a research offshoot of
<strong>High-Flyer</strong>—a hedge fund that had already amassed a
large GPU inventory (reportedly 10,000 Nvidia A100s in 2021). Over time,
this resource base appears to have grown, with estimates suggesting
that—when you account for research, ablation experiments, and shared
infrastructure with trading—the effective pool might be closer to 50,000
GPUs. This expansive compute power enables DeepSeek to run many
experiments simultaneously and quickly iterate on new architectures.</p>
<p>By leveraging a shared infrastructure with its hedge fund operations,
DeepSeek can reinvest profits from quant trading into AI research. This
model of “doing more with less” not only challenges the notion that
massive, multibillion-dollar compute expenditures are necessary to build
world-class AI models but also has broader implications for the
industry. It raises questions about the future economics of AI
development and the potential for more cost-efficient, research-driven
models to shift market dynamics, as seen by the notable impact on
Nvidia’s stock and market sentiment.</p>
<h2 id="export-controls-on-gpus-to-china">Export Controls on GPUs to
China</h2>
<p>In essence, the U.S. government originally imposed limits on chips
that exceed certain thresholds in both <strong>interconnect
bandwidth</strong> and <strong>compute (FLOPs)</strong> to restrict
China’s ability to train massive AI models. Early on, chips that
combined high interconnect speeds with high FLOPs were off‐limits.</p>
<p>For example, the H100—one of Nvidia’s top GPUs—was deemed too
powerful. In response, Nvidia developed the H800, which maintained the
same floating point performance (FLOPs) as the H100 but had its
interconnect bandwidth intentionally reduced to meet U.S. export
criteria. However, when the government later decided to tighten controls
further (targeting chips solely on FLOPs), even the H800 was banned.
This led Nvidia to innovate once again with the H20, a chip that now
offers full interconnect bandwidth (and even improved memory
characteristics over the H100) but with a deliberate cut in overall
FLOPs to satisfy export rules.</p>
<p>The strategic rationale behind these controls is to “decap” China’s
compute—especially for large-scale AI training—by limiting how many of
the most advanced GPUs (and thus the overall density of compute) can be
legally acquired. While Chinese companies can still purchase GPUs to
train models, the overall capacity available for training (which is
critical for developing super-powerful AI) is being capped. This is seen
as a way to maintain U.S. and allied leadership in AI, particularly in a
world where super-powerful AI may soon offer decisive military and
economic advantages.</p>
<h3 id="sidenote---gpus-for-ai">Sidenote - GPUs for AI</h3>
<p><strong>Keys GPU Specifications:</strong></p>
<ul>
<li><strong>FLOPS (Compute Power)</strong>: Critical for training large
models (e.g., GPT-4) but less critical for inference tasks like
reasoning.</li>
<li><strong>Memory Bandwidth/Capacity</strong>: Determines how much data
(e.g., KV cache in transformers) can be stored and accessed quickly,
crucial for long-sequence tasks.</li>
<li><strong>Interconnect Speed</strong>: Affects communication between
GPUs in clusters, important for distributed training but less regulated
now.</li>
</ul>
<p><strong>H20 vs. H100: Tradeoffs for AI Workloads:</strong></p>
<ul>
<li><p><strong>H20 (China-Specific):</strong> has its strength in higher
memory bandwidth and capacity than H100, making it better suited for
<strong>reasoning tasks</strong> (e.g., long-context inference,
chain-of-thought). However, FLOPS (≈1/3 of H100 on paper, ≈50-60% in
practice) is reduced, limiting its utility for training.</p>
<p><strong>Regulatory Context</strong>: Designed to comply with U.S.
export controls that focus on FLOPS, allowing Nvidia to ship 1M units to
China in 2023 (20-25% of total GPUs).</p></li>
<li><p><strong>H100</strong>: Optimized for FLOPS-heavy training but
less efficient for memory-bound inference tasks</p></li>
</ul>
<p><strong>Why Memory Matters for Reasoning:</strong></p>
<ul>
<li><strong>KV Cache in Transformers</strong> stores keys/values of all
tokens in a sequence for attention mechanisms. Memory demands grow
<strong>quadratically</strong> with sequence length (e.g., 10K+ tokens
in reasoning tasks).</li>
<li><strong>Autoregressive Generation</strong>: Output tokens require
sequential processing, forcing repeated KV cache access. This limits
parallelism and increases memory pressure. Tasks like agentic AI or
chain-of-thought involve generating long outputs (10K+ tokens),
stressing memory bandwidth/capacity.</li>
</ul>
<h2 id="deepseek---technical-comments">DeepSeek - Technical
Comments</h2>
<p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.19437">DeepSeek-V3
Technical Report</a></p>
<p><strong>GPU Infrastructure with Nvidia Hardware</strong></p>
<ul>
<li>DeepSeek trains on <strong>Nvidia GPUs</strong>. These are equipped
with many cores (organized into streaming multiprocessors, or SMs) that
perform the heavy lifting during both training and inference.</li>
<li>The GPUs they used were those <strong>legally available in
China</strong>, which imposed certain limitations—especially on
interconnect bandwidth between units. This meant that DeepSeek needed to
overcome hardware constraints that might not be present with the very
latest high-end GPUs elsewhere.</li>
</ul>
<p><strong>Custom Low-Level Optimization</strong></p>
<ul>
<li>Instead of relying solely on Nvidia’s standard NCCL (Nvidia
Communications Collectives Library) for handling inter-GPU
communications, DeepSeek’s engineers developed custom scheduling
techniques. They even <strong>scheduled communications at the SM
level</strong>, which is more granular than the typical approach.</li>
<li>Their implementation involved programming approaches that went deep
into the hardware—down to using <strong>PTX</strong> (an intermediate
assembly-like language for CUDA). This allowed them to squeeze extra
efficiency from each GPU by reducing the overhead in communication
between layers of the model.</li>
</ul>
<p><strong>Efficiency via Architectural Choices</strong></p>
<ul>
<li>One of the key innovations was using a <strong>sparse Mixture of
Experts (MoE) architecture</strong>. With a model that can have hundreds
of billions of parameters overall but only activates a fraction (e.g.,
around 37 billion at a time), the compute and memory demands are
dramatically reduced. This architectural choice means that even if the
hardware isn’t the absolute latest, it can still be very cost-effective
by not needing to run every parameter for every token.</li>
<li>DeepSeek's novel attention mechanism <strong>MLA (Multi-Head Latent
Attention)</strong> reduces memory usage by 80–90% compared to
traditional transformer attention. This optimization lowers
computational costs, especially for long-context processing, without
sacrificing performance.</li>
<li>By optimizing both the hardware usage (through custom scheduling and
low-level programming) and the model architecture (via MoE and MLA),
DeepSeek manages to cut down on the cost of training. This is crucial
given the significant compute expense associated with large-scale
language models.</li>
</ul>
<p><strong>Pre-Training and Context Window Extension</strong></p>
<ul>
<li>Pre-trained on 14.8 trillion tokens drawn from a multilingual corpus
(primarily English and Chinese) with a higher proportion of math and
programming content compared to previous iterations.</li>
<li>Utilizes a two-phase extension (via the YaRN framework) to expand
the context length from 4K tokens to 32K and finally to 128K
tokens.</li>
<li>Reported training cost for V3 is approximately $5.58 million,
consuming about 2.788 million GPU-hours on Nvidia H800 GPUs. This figure
is significantly lower than the hundreds of millions typically reported
by US rivals.</li>
</ul>
<p><strong>Post-Training: Supervised Fine-Tuning &amp; Reinforcement
Learning</strong></p>
<ul>
<li>V3 is fine-tuned on a carefully curated dataset of approximately 1.5
million examples (both reasoning and non-reasoning tasks) to improve
instruction-following and output formatting.</li>
<li>DeepSeek employs <strong>GRPO</strong>—a <strong>group relative
policy optimization method</strong>—to reward outputs based on
correctness (accuracy rewards) and presentation (format rewards).</li>
<li>R1 leverages RL to fine-tune the reasoning process, rewarding
chain-of-thought quality and encouraging the model to generate
self-reflective “aha moments.”</li>
</ul>
<p><strong>Speed-to-Market and Safety Tradeoffs</strong></p>
<ul>
<li><p>DeepSeek prioritizes rapid deployment over extensive safety
testing, avoiding delays and costs associated with ethical reviews
(common in Western firms like Anthropic). This "ship-first" approach
reduces development cycle expenses.</p></li>
<li><p>Releasing model weights publicly attracts third-party hosting and
innovation, indirectly expanding reach without bearing full
infrastructure costs.</p></li>
</ul>
<h2 id="the-tech-and-business-perspective"><strong>The Tech and Business
Perspective</strong></h2>
<p>The release of DeepSeek-R1 marks a pivotal moment in the AI industry,
igniting discussions about open-source dominance, market disruption, and
geopolitical implications.</p>
<p><strong>Industry Leaders Weigh In</strong>:</p>
<p><strong>Yann LeCun (Meta’s Chief AI Scientist)</strong></p>
<p>LeCun emphasized the growing power of open-source models over
proprietary approaches:</p>
<blockquote>
<p><em>"To people who see the performance of DeepSeek and think China is
surpassing the US in AI. You are reading this wrong. The correct reading
is: Open source models are surpassing proprietary ones."</em></p>
</blockquote>
<p><strong>Andrej Karpathy (OpenAI Co-founder)</strong></p>
<p>Karpathy pointed out the continued need for large-scale computing
while praising DeepSeek’s efficiency:</p>
<blockquote>
<p><em>"Does this mean you don't need large GPU clusters for frontier
LLMs? No, but you have to ensure that you're not wasteful with what you
have, and this looks like a nice demonstration that there's still a lot
to get through with both data and algorithms."</em></p>
</blockquote>
<p><strong>Satya Nadella (Microsoft CEO)</strong></p>
<p>Nadella underscored the significance of DeepSeek, highlighting its
role in making AI reasoning more accessible:</p>
<blockquote>
<p><em>"We should take the developments out of China very, very
seriously."</em> <em>"DeepSeek has had some real innovations. …
Obviously, now all that gets commoditized."</em> <em>"When token prices
fall, inference computing prices fall, that means people can consume
more, and there will be more apps written."</em></p>
</blockquote>
<p><strong>Mark Zuckerberg (Meta CEO)</strong></p>
<p>Zuckerberg acknowledged DeepSeek's novel infrastructure
optimizations:</p>
<blockquote>
<p><em>"DeepSeek had a few pretty novel infrastructure optimization
advances, which, fortunately, they published them, so we can not only
observe what they did, but we can read about it and implement it, so
that'll benefit us."</em> <em>"Always interesting when there's someone
who does something better than you. Let's make sure we are on
it."</em></p>
</blockquote>
<p><strong>Aravind Srinivas (Perplexity AI CEO)</strong></p>
<p>Srinivas stressed the importance of foundational innovation:</p>
<blockquote>
<p><em>"We need to build, not just wrap existing AI."</em></p>
</blockquote>
<p><strong>Marc Andreessen (Andreessen Horowitz Co-founder)</strong></p>
<p>He likened DeepSeek-R1 to a historic milestone:</p>
<blockquote>
<p><em>"DeepSeek R1 is AI's Sputnik moment."</em></p>
</blockquote>
<p><strong>Tim Cook (Apple CEO)</strong></p>
<p>Cook gave a measured response during an earnings call:</p>
<blockquote>
<p><em>"In general, I think innovation that drives efficiency is a good
thing."</em></p>
</blockquote>
<h2 id="academic-and-research-perspectives"><strong>Academic and
Research Perspectives</strong></h2>
<p><strong>AI Researchers on DeepSeek-R1</strong>:</p>
<p><strong>Timnit Gebru (AI Ethics Researcher)</strong></p>
<p>Gebru reflected on past AI development priorities:</p>
<blockquote>
<p><em>"At Google, I asked why they were fixated on building THE LARGEST
model. Why are you going for size? What function are you trying to
achieve? They responded by firing me."</em></p>
</blockquote>
<p><strong>Ethan Mollick (Wharton AI Professor)</strong></p>
<p>Mollick focused on accessibility rather than capabilities:</p>
<blockquote>
<p><em>"DeepSeek is a really good model, but it is not generally a
better model than o1 or Claude. But since it is both free and getting a
ton of attention, I think a lot of people who were using free 'mini'
models are being exposed to what an early 2025 reasoner AI can do and
are surprised."</em></p>
</blockquote>
<p><strong>Andrew Ng (AI Researcher and Entrepreneur)</strong></p>
<p>Ng saw the market reaction as an opportunity for developers:</p>
<blockquote>
<p><em>"Today's 'DeepSeek selloff' in the stock market—attributed to
DeepSeek V3/R1 disrupting the tech ecosystem—is another sign that the
application layer is a great place to be. The foundation model layer
being hyper-competitive is great for people building
applications."</em></p>
</blockquote>
<p><strong>Global Academic Community Response</strong>:</p>
<p>Huan Sun from Ohio State University noted that DeepSeek's
affordability is expanding LLM adoption in research. Cong Lu from the
University of British Columbia highlighted R1’s rapid adoption,
surpassing 3 million downloads on Hugging Face in a week. Meanwhile,
safety concerns emerged as studies revealed R1 is 11 times more likely
to generate harmful content compared to OpenAI models, prompting calls
for better safeguards.</p>
<h2 id="impact-discussion">Impact Discussion</h2>
<p><strong>Market and Industry Impact</strong></p>
<p>The release of DeepSeek-R1 caused massive shifts in financial
markets. U.S. tech stocks collectively lost <span
class="math inline">\(\$1\)</span> trillion, with Nvidia suffering
record losses due to the rising competition from this cost-efficient
model. Investors are recalibrating AI development strategies as DeepSeek
achieved comparable performance to OpenAI’s models at just <span
class="math inline">\(\$6\)</span> million versus OpenAI’s <span
class="math inline">\(\$100\)</span> million.</p>
<p><strong>Integration into Cloud Ecosystems</strong></p>
<p>AWS and Microsoft Azure have incorporated DeepSeek-R1, enabling
developers to explore its capabilities securely and cost-effectively.
The emergence of cost-effective models like DeepSeek R1 is forcing a
shift in AI economics, emphasizing efficiency over massive capital
investments. As a result, competition in the AI sector is intensifying,
ushering in a “warring states era” where companies are scrambling for
innovation in cost-effective models.</p>
<p><strong>Geopolitical and National Security Implications</strong></p>
<p>The success of DeepSeek R1 has intensified concerns that the U.S. is
losing its technological edge to China. Policymakers are reassessing
export controls on advanced chips in light of DeepSeek's ability to
innovate using restricted hardware. Security concerns have also prompted
the U.S. Navy to ban the use of DeepSeek R1 due to potential security
and ethical risks, fueling debates over the implications of adopting
foreign-developed AI systems.</p>
<p><strong>Open-Source vs Proprietary Models</strong></p>
<p>DeepSeek R1 is accelerating the democratization of AI by lowering
barriers for smaller developers and researchers, fostering innovation.
However, transparency concerns remain as DeepSeek has not disclosed its
training data, raising ethical and bias-related questions.</p>
<p><strong>Ethical and Technical Questions</strong></p>
<p>Concerns have emerged regarding potential censorship, as some
versions of DeepSeek R1 appear to align with Chinese narratives.
Additionally, skepticism exists over whether DeepSeek’s reported costs
and capabilities are fully accurate, with some experts questioning the
factors that contributed to its success.</p>
<p><strong>Public Sentiment and the Future of AI</strong></p>
<p>Public reaction to DeepSeek-R1 has been mixed. Some view this as a
“Sputnik moment,” encouraging U.S. firms to accelerate AI innovation
while leveraging open-source models to stay competitive. Others see it
as a wake-up call, with former President Donald Trump urging U.S.
industries to adapt quickly to maintain leadership in AI
development.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2025/01/24/Persisting-Agent-State/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2025/01/24/Persisting-Agent-State/" class="post-title-link" itemprop="url">Persisting Agent State</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-01-24 22:25:22" itemprop="dateCreated datePublished" datetime="2025-01-24T22:25:22-05:00">2025-01-24</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="persistence-in-langgraph">Persistence in LangGraph</h2>
<p><a
target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/concepts/persistence/">Persistence</a>
is a cornerstone for building robust and production-grade applications.
LandGraph introduces a game-changing feature that ensures application
states are stored and retrievable at any point. This redefines
reliability and scalability in workflow management. This capability is
especially vital when executing workflows involving interruptions, user
inputs, or debugging. Whether you're building a simple app or an
enterprise-grade system, persistence ensures your application is always
ready to handle interruptions and user interactions gracefully.</p>
<p>The "Persisting Agent Stage" enables seamless workflows, especially
in user-facing applications. Here’s why this feature is critical:</p>
<ol type="1">
<li><strong>Human-in-the-Loop Workflows:</strong> Many applications rely
on user input to make decisions or advance processes. With persistence,
LandGraph allows the graph execution to pause, checkpoint the state into
persistent storage, and resume later. This means the application can
wait for user input and continue without losing context.</li>
<li><strong>Debugging and History</strong>: Persistence creates a robust
mechanism for saving the application state after every step. This makes
debugging easier and enables the creation of detailed execution
histories.</li>
<li><strong>Support for Multi-Session Scenarios</strong>: Applications
often require users to switch between sessions while maintaining their
progress. Persistence ensures continuity by saving states into
persistent storage.</li>
</ol>
<p>At the heart of this feature is the <strong><a
target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/checkpoints/">CheckPointer</a></strong>
object, a persistence layer implemented by LandGraph. Here’s how it
works:</p>
<ul>
<li><p><strong>Integration with Databases</strong> The CheckPointer can
save states into various database types, including:</p>
<ul>
<li><strong>Document databases</strong>: Firestore, MongoDB</li>
<li><strong>Relational databases</strong>: PostgreSQL, SQLite,
MySQL</li>
<li><strong>Graph databases</strong>: Neo4j, AWS Neptune</li>
</ul>
<p>For example, the following section will focus on persisting states
into an SQLite database, a popular choice for local environments. The
process can also be extended to managed cloud databases like Google
Cloud SQL or AWS RDS.</p></li>
<li><p><strong>State Management</strong> As each node in the graph
executes, the CheckPointer saves the updated state into the database.
This ensures that states are recoverable after interruptions, enabling
the graph to resume execution from exactly where it left off.</p></li>
</ul>
<p>To implement persistence, follow these simple steps:</p>
<ol type="1">
<li>Import the <strong>CheckPointer</strong> object from LandGraph.</li>
<li>Create an instance of CheckPointer and configure it with a
connection string (local or cloud-based database).</li>
<li>Pass the CheckPointer instance to your graph during creation.
LandGraph will handle state persistence automatically after each node
execution.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.checkpoint.sqlite <span class="keyword">import</span> SqliteSaver</span><br><span class="line"></span><br><span class="line">memory = SqliteSaver.from_conn_string(<span class="string">&quot;:checkpoints.sqlite:&quot;</span>)</span><br><span class="line">graph = workflow.complie(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<p>The result is that you can pause the graph, fetch user input, and
continue execution seamlessly, all while ensuring states are securely
stored in your chosen database.</p>
<h2 id="memorysaver-interrupts-human-in-the-loop">MemorySaver +
Interrupts = Human In The Loop</h2>
<p>Human-in-the-loop systems are essential to modern applications,
allowing seamless integration of human feedback into automated
workflows. With the help of the <strong>MemorySaver</strong> feature,
you can build applications using LangGraph that pause, capture user
input, and resume execution effortlessly.</p>
<p>In workflows involving human interaction, there are moments where the
application needs to pause, gather feedback from the user, and then
continue processing. For instance, consider a sequence of tasks
where:</p>
<ol type="1">
<li>A process executes its initial steps.</li>
<li>The system pauses to collect human input.</li>
<li>The workflow resumes, incorporating the user’s feedback.</li>
</ol>
<p>This type of flow requires <strong>interrupts</strong> to halt the
execution and <strong>persistence</strong> to save the current state of
the workflow. Langraph provides the tools to manage both seamlessly.</p>
<h3 id="implementation">Implementation</h3>
<p>To illustrate, let’s build a straightforward graph with the following
steps:</p>
<ol type="1">
<li>Start with a simple initial node.</li>
<li>Execute a task and pause for human feedback.</li>
<li>Resume execution with the updated state and complete the
workflow.</li>
</ol>
<p>We use Langraph's <strong>MemorySaver</strong>, a checkpointing tool
that saves the workflow’s state in memory after each node’s execution.
This ephemeral storage method is perfect for local testing and
prototyping. Here’s a simplified version of the setup:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START, END</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    <span class="built_in">input</span>: <span class="built_in">str</span></span><br><span class="line">    user_feedback: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_1</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;---Step 1---&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">human_feedback</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;---human_feedback---&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_3</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;---Step 3--&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">builder = StateGraph(State)</span><br><span class="line">builder.add_node(<span class="string">&quot;step_1&quot;</span>, step_1)</span><br><span class="line">builder.add_node(<span class="string">&quot;human_feedback&quot;</span>, human_feedback)</span><br><span class="line">builder.add_node(<span class="string">&quot;step_3&quot;</span>, step_3)</span><br><span class="line">builder.add_edge(START, <span class="string">&quot;step_1&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;step_1&quot;</span>, <span class="string">&quot;human_feedback&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;human_feedback&quot;</span>, <span class="string">&quot;step_3&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;step_3&quot;</span>, END)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">memory = MemorySaver()</span><br><span class="line"></span><br><span class="line">graph = builder.<span class="built_in">compile</span>(checkpointer=memory, interrupt_before=[<span class="string">&quot;human_feedback&quot;</span>])</span><br><span class="line"></span><br><span class="line">graph.get_graph().draw_mermaid_png(output_file_path=<span class="string">&quot;graph.png&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>The graph visualization by using <a
target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.graph.MermaidDrawMethod.html">Mermaid.ink</a>
is here:</p>
<figure>
<img src="/di-blog/2025/01/24/Persisting-Agent-State/hitl-graph.png"
alt="hitl-graph" />
<figcaption aria-hidden="true">hitl-graph</figcaption>
</figure>
<h2 id="memorysaver-implementations">MemorySaver Implementations</h2>
<p>Integrating human feedback into automated systems is a growing trend
in AI development. It bridges the gap between machine automation and
human judgment, enabling better decision-making, improved accuracy, and
adaptability. In this section, we explore how to incorporate
human-in-the-loop functionality into a graph-based system while
leveraging memory storage to track execution states. This walkthrough
showcases the process from initialization to final execution.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START, END</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    <span class="built_in">input</span>: <span class="built_in">str</span></span><br><span class="line">    user_feedback: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_1</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;### Step 1 ###&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">human_feedback</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;### Human Feedback ###&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_3</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;### Step 3 ###&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">builder = StateGraph(State)</span><br><span class="line">builder.add_node(<span class="string">&quot;step_1&quot;</span>, step_1)</span><br><span class="line">builder.add_node(<span class="string">&quot;human_feedback&quot;</span>, human_feedback)</span><br><span class="line">builder.add_node(<span class="string">&quot;step_3&quot;</span>, step_3)</span><br><span class="line">builder.add_edge(START, <span class="string">&quot;step_1&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;step_1&quot;</span>, <span class="string">&quot;human_feedback&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;human_feedback&quot;</span>, <span class="string">&quot;step_3&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;step_3&quot;</span>, END)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">memory = MemorySaver()</span><br><span class="line"></span><br><span class="line">graph = builder.<span class="built_in">compile</span>(checkpointer=memory, interrupt_before=[<span class="string">&quot;human_feedback&quot;</span>])</span><br><span class="line"></span><br><span class="line">graph.get_graph().draw_mermaid_png(output_file_path=<span class="string">&quot;graph.png&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    thread = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: <span class="string">&quot;1&quot;</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">    initial_input = &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;hello world&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> event <span class="keyword">in</span> graph.stream(initial_input, thread, stream_mode=<span class="string">&quot;values&quot;</span>):</span><br><span class="line">        <span class="built_in">print</span>(event)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(graph.get_state(thread).<span class="built_in">next</span>)</span><br><span class="line"></span><br><span class="line">    user_input = <span class="built_in">input</span>(<span class="string">&quot;How do you want to update the state? &quot;</span>)</span><br><span class="line"></span><br><span class="line">    graph.update_state(thread, &#123;<span class="string">&quot;user_feedback&quot;</span>: user_input&#125;, as_node=<span class="string">&quot;human_feedback&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;### State after update ###&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(graph.get_state(thread))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(graph.get_state(thread).<span class="built_in">next</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> event <span class="keyword">in</span> graph.stream(<span class="literal">None</span>, thread, stream_mode=<span class="string">&quot;values&quot;</span>):</span><br><span class="line">        <span class="built_in">print</span>(event)</span><br></pre></td></tr></table></figure>
<p>The graph’s execution is tied to a <code>thread</code> variable, a
dictionary initialized with a <code>thread_id</code>. This serves as a
session or conversation identifier, distinguishing various graph runs.
For simplicity, the <code>thread_id</code> is set to <code>1</code>,
though a more robust implementation would use a UUID. The graph
processes events using <code>graph.stream()</code>, which accepts the
initial input and thread details. Events are streamed in value mode, and
each event is printed for transparency.</p>
<p>During execution:</p>
<ul>
<li>Input is processed.</li>
<li>Node executions are logged.</li>
<li>Interruptions allow for dynamic human input.</li>
</ul>
<p>Running the graph in debug mode provides insights into:</p>
<ul>
<li>Memory storage (<code>memory.storage</code>) containing nested
objects that log the graph state.</li>
<li>Transition logs for each node, showing updates or lack thereof.</li>
</ul>
<p>At an interrupt, human feedback is solicited using Python's built-in
<code>input()</code> function. This input updates the state dynamically.
Once human input is integrated, the graph resumes execution. Subsequent
steps process the updated state, leading to the graph’s completion.</p>
<h2 id="sqlitesaver">SqliteSaver</h2>
<p>Switching from an ephemeral memory-based state saver to a persistent
database saver can significantly enhance the durability and traceability
of your graph’s execution. In this section, we’ll explore how to replace
the in-memory <code>MemorySaver</code> with an <code>SQLiteSaver</code>
for long-term storage and easy debugging.</p>
<p>The <code>MemorySaver</code> is transient, meaning all state
information vanishes after the program stops. By using an SQLite
database, you can:</p>
<ul>
<li>Persist graph states across runs.</li>
<li>Debug and troubleshoot using a structured database.</li>
<li>Resume executions exactly where they were interrupted.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.sqlite <span class="keyword">import</span> SqliteSaver</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START, END</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    <span class="built_in">input</span>: <span class="built_in">str</span></span><br><span class="line">    user_feedback: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_1</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;### Step 1 ###&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">human_feedback</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;### Human Feedback ###&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_3</span>(<span class="params">state: State</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;### Step 3 ###&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">builder = StateGraph(State)</span><br><span class="line">builder.add_node(<span class="string">&quot;step_1&quot;</span>, step_1)</span><br><span class="line">builder.add_node(<span class="string">&quot;human_feedback&quot;</span>, human_feedback)</span><br><span class="line">builder.add_node(<span class="string">&quot;step_3&quot;</span>, step_3)</span><br><span class="line">builder.add_edge(START, <span class="string">&quot;step_1&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;step_1&quot;</span>, <span class="string">&quot;human_feedback&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;human_feedback&quot;</span>, <span class="string">&quot;step_3&quot;</span>)</span><br><span class="line">builder.add_edge(<span class="string">&quot;step_3&quot;</span>, END)</span><br><span class="line"></span><br><span class="line">conn = sqlite3.connect(<span class="string">&quot;checkpoints.sqlite&quot;</span>, check_same_thread=<span class="literal">False</span>)</span><br><span class="line">memory = SqliteSaver(conn)</span><br><span class="line">graph = builder.<span class="built_in">compile</span>(checkpointer=memory, interrupt_before=[<span class="string">&quot;human_feedback&quot;</span>])</span><br><span class="line"></span><br><span class="line">graph.get_graph().draw_mermaid_png(output_file_path=<span class="string">&quot;graph.png&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    thread = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: <span class="string">&quot;1&quot;</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">    initial_input = &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;hello world&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> event <span class="keyword">in</span> graph.stream(initial_input, thread, stream_mode=<span class="string">&quot;values&quot;</span>):</span><br><span class="line">        <span class="built_in">print</span>(event)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(graph.get_state(thread).<span class="built_in">next</span>)</span><br><span class="line"></span><br><span class="line">    user_input = <span class="built_in">input</span>(<span class="string">&quot;How do you want to update the state: &quot;</span>)</span><br><span class="line"></span><br><span class="line">    graph.update_state(thread, &#123;<span class="string">&quot;user_feedback&quot;</span>: user_input&#125;, as_node=<span class="string">&quot;human_feedback&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;### State after update ###&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(graph.get_state(thread))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(graph.get_state(thread).<span class="built_in">next</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> event <span class="keyword">in</span> graph.stream(<span class="literal">None</span>, thread, stream_mode=<span class="string">&quot;values&quot;</span>):</span><br><span class="line">        <span class="built_in">print</span>(event)</span><br></pre></td></tr></table></figure>
<p>We start by importing the required modules. Then Initialize a
connection to your SQLite database. The
<code>check_same_thread=False</code> flag ensures thread-safe database
operations, essential for stopping and restarting execution across
different threads. After that we create an instance of
<code>SQLiteSaver</code> and pass it the SQLite connection. This saver
integrates seamlessly with the graph execution pipeline, persisting
states to the SQLite database.</p>
<ol type="1">
<li><strong>Initial Execution</strong>: Run the graph with the
<code>SQLiteSaver</code>. After execution, you’ll see a new file,
<code>checkpoints.sqlite</code>, created in your project directory.</li>
<li><strong>Inspect the Database</strong>: Use your IDE’s database tools
(e.g. SQLite3 Editor for VS Code) to load and inspect the
<code>checkpoints.sqlite</code> file. You’ll find a table storing graph
states, similar to what you’d see with <code>MemorySaver</code>, but now
it’s persistent.</li>
</ol>
<figure>
<img
src="/di-blog/2025/01/24/Persisting-Agent-State/screenshot_sqlite_ide.png"
alt="screenshot_sqlite_ide" />
<figcaption aria-hidden="true">screenshot_sqlite_ide</figcaption>
</figure>
<p>Changing the <code>thread_id</code> allows you to simulate a new
session while retaining access to previous runs. When resuming, the
graph starts from the last recorded state. You can verify this by
inspecting the database entries for the new <code>thread_id</code>.</p>
<p>For enhanced traceability, integrate Langsmith for tracking and
debugging. Langsmith provides detailed insights, including thread
metadata and execution traces.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/di-blog/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/di-blog/">1</a><span class="page-number current">2</span><a class="page-number" href="/di-blog/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/di-blog/page/5/">5</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/di-blog/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Di Zhen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/di-blog/js/comments.js"></script><script src="/di-blog/js/utils.js"></script><script src="/di-blog/js/motion.js"></script><script src="/di-blog/js/sidebar.js"></script><script src="/di-blog/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/di-blog/js/third-party/math/mathjax.js"></script>



</body>
</html>
