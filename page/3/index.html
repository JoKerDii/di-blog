<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/di-blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/di-blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/di-blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/di-blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/di-blog/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jokerdii.github.io","root":"/di-blog/","images":"/di-blog/images","scheme":"Muse","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/di-blog/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Di&#39;s Blog">
<meta property="og:url" content="https://jokerdii.github.io/di-blog/page/3/index.html">
<meta property="og:site_name" content="Di&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Di Zhen">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jokerdii.github.io/di-blog/page/3/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Di's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/di-blog/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/di-blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Di's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Di Zhen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/di-blog/archives/">
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/12/25/Understanding-LoRA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/12/25/Understanding-LoRA/" class="post-title-link" itemprop="url">Understanding LoRA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-25 15:11:11" itemprop="dateCreated datePublished" datetime="2024-12-25T15:11:11-05:00">2024-12-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>I finally got time to have some deep dives. Happy Christmas!</p>
<h2 id="ram-usage-during-training">RAM Usage During Training</h2>
<p>Training large-scale machine learning models e.g. LLMs, requires
significant compute resources. Here’s a breakdown of the possible memory
usage (RAM) at various stages of the classic training process, based on
the pseudocode below:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = Model()</span><br><span class="line">optimizer = Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">    <span class="comment"># Compute prediction and loss</span></span><br><span class="line">    pred = model(X)</span><br><span class="line">    loss = loss_fn(pred, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h3 id="key-components-of-memory-usage">Key Components of Memory
Usage</h3>
<ol type="1">
<li><p><strong>Model Parameters</strong>: These are the trainable
weights of the model, which need to be stored in memory throughout the
training process. The size is proportional to the number of parameters
in the model.</p></li>
<li><p><strong>Model Gradients</strong>: Gradients for each parameter
are computed during backpropagation and stored temporarily for the
optimizer to update the weights.</p></li>
<li><p><strong>Optimizer States</strong>: Optimizers like Adam maintain
additional states, including:</p>
<ul>
<li><p><strong>First-order momentum</strong>: Tracks the moving average
of gradients.</p></li>
<li><p><strong>Second-order momentum</strong>: Tracks the moving average
of squared gradients.</p></li>
<li><p>Both momentum terms have the same size as the model
gradients.</p></li>
</ul></li>
<li><p><strong>Activations</strong>: Activation outputs from the forward
pass are stored for use during backpropagation, where the Hessian matrix
is multiplied with the activations. The memory required for activations
can be substantial, especially as batch size increases. While the size
of parameters, gradients, and optimizer states remains constant,
activation memory scales directly with batch size.</p></li>
<li><p><strong>Other Overheads</strong>: Temporary buffers and memory
fragmentation during computation also contribute to RAM usage.</p></li>
</ol>
<h3 id="memory-calculation-examples">Memory Calculation Examples</h3>
<ol type="1">
<li><p><strong>Gradients and Parameters</strong>:</p>
<p>For 70B model, using 32-bit floating-point precision (FP32): <span
class="math display">\[
70\times10^9\times4 \text{ bytes}\times2 =521.5\text{GM}
\]</span> This accounts for the weights and their corresponding
gradients.</p></li>
<li><p><strong>Optimizer State</strong>:</p>
<p>Adam optimizer requires two additional states (first and second-order
momentum), each the same size as the gradients: <span
class="math display">\[
70\times10^9\times4 \text{ byte}\times2 =521.5\text{GM}
\]</span></p></li>
<li><p><strong>Activations</strong>:</p>
<p>For 70B model with a hidden size of 8192, 80 layers, and FP32
precision, each token’s activation memory: <span class="math display">\[
8192\times80\times4\times12 \text{ bytes/token}=30\text{ MB/token}
\]</span></p></li>
</ol>
<h3 id="simple-strategies-for-reducing-memory-usage">Simple Strategies
for Reducing Memory Usage</h3>
<ol type="1">
<li><strong>Activation Checkpointing</strong>: Instead of storing all
activation outputs, recompute activations during backpropagation as
needed. This significantly reduces activation memory at the cost of
additional compute time.</li>
<li><strong>Mixed Precision Training (FP16)</strong>: Use 16-bit
floating-point precision (FP16) instead of FP32 for model weights,
gradients, and activations. This halves the memory requirements without
substantial accuracy loss when done correctly.</li>
</ol>
<h2 id="lora">LoRA</h2>
<h3 id="adapters">Adapters</h3>
<p>The original adapter was introduced in 2019 in the paper "<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.00751">Parameter-Efficient Transfer
Learning for NLP</a>". It's a small, additional module added to a
pre-trained model to adapt it to a new task without significantly
changing the original model parameters.</p>
<figure>
<img
src="/di-blog/2024/12/25/Understanding-LoRA/adapter_architecture.png"
alt="adapter_architecture" />
<figcaption aria-hidden="true">adapter_architecture</figcaption>
</figure>
<p>Adapters generally <strong>reduce training latency</strong> compared
to full fine-tuning because only a small number of parameters (those
within the adapter modules) are updated during training. This reduction
in trainable parameters leads to lower computational overhead and faster
convergence in many cases. Additionally, adapters allow for larger batch
sizes due to reduced memory usage, which can further accelerate
training</p>
<p>However, adapter layers <strong>increase inference latency</strong>
because they are added sequentially and cannot be parallelized. This
issue becomes more pronounced with small batch sizes or when using
sharded models, such as GPT-2. Techniques like layer pruning or
multi-task settings can mitigate but not completely eliminate this
latency.</p>
<p>As shown in the experiment results below, inference latent can be
significant (Source: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">LoRA
paper</a>):</p>
<figure>
<img
src="/di-blog/2024/12/25/Understanding-LoRA/adapter_experiment_results.png"
alt="adapter_experiment_results" />
<figcaption aria-hidden="true">adapter_experiment_results</figcaption>
</figure>
<h3 id="lora-basics">LoRA Basics</h3>
<p>LoRA (Low-Rank Adaptation) was introduced by a Microsoft team in 2021
in the paper <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank
Adaptation of Large Language Models</a>. The main idea of LoRA is to
enable efficient fine-tuning of large pre-trained models by introducing
low-rank trainable matrices into the model’s architecture, while keeping
the original model weights frozen. This approach significantly reduces
the number of trainable parameters and computational requirements
compared to full fine-tuning, without compromising performance.</p>
<figure>
<img src="/di-blog/2024/12/25/Understanding-LoRA/lora_diagram.png"
alt="lora_diagram" />
<figcaption aria-hidden="true">lora_diagram</figcaption>
</figure>
<p>LoRA approximates weight updates in neural networks using
<strong>low-rank matrix factorization</strong>. Instead of updating the
full weight matrix <span class="math inline">\(W\)</span> , it
introduces two smaller trainable matrices <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> with size <span
class="math inline">\((r \times d)\)</span> and <span
class="math inline">\((d \times r)\)</span>. These matrices have much
fewer parameters, as their rank <span class="math inline">\(r\)</span>
is much smaller than the dimensions of <span
class="math inline">\(W\)</span>. Instead of training <span
class="math inline">\(\Delta W\)</span>, LoRA trains the parameters in
<span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span>. This can be written in formula: <span
class="math display">\[
h=W_0x + \Delta Wx = W_0x + BAx
\]</span> where <span class="math inline">\(W_0\)</span> is original
prerained weight matrix in size <span class="math inline">\((d\times
d)\)</span> which is frozen during training; <span
class="math inline">\(\Delta W\)</span> is in <span
class="math inline">\((d \times d)\)</span> as well computed by <span
class="math inline">\(BA\)</span>. <span
class="math inline">\(x\)</span> is a new input with size <span
class="math inline">\((1 \times d)\)</span>.</p>
<p>At the start of the training process, the matrix $ A $ is randomly
initialized following a normal distribution <span
class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span>, while the
matrix $ B $ is initialized as a zero matrix. In the initial round, this
setup results in $ BA = 0 $, leading to $ h = W_0x $. This
initialization strategy ensures stability by preventing significant
deviations of $ W_0 $ from its original state.</p>
<p>LoRA is a groundbreaking method with a lot of
<strong>benefits</strong>:</p>
<ul>
<li><strong>Parameter Efficiency</strong>: By training only the low-rank
matrices, LoRA reduces the number of updated parameters resulting in
lower memory usage and faster training.</li>
<li><strong>Frozen Pre-trained Weights</strong>: The original
pre-trained weights remain unchanged, preserving the model’s
general-purpose knowledge and avoiding catastrophic forgetting.</li>
<li><strong>No Inference Latency Overhead</strong>: Unlike adapters,
LoRA does not add additional layers to the model. The low-rank matrices
can be merged back into the original weight matrix after fine-tuning,
ensuring no additional inference latency.</li>
<li><strong>Versatility</strong>: LoRA can be applied to various
architectures (e.g. transformers) and tasks, making it a flexible
solution for adapting large models like GPT-3 or RoBERTa to specific use
cases.</li>
</ul>
<h3 id="lora-usage">LoRA Usage</h3>
<p>The Microsoft developers of LoRA created a Python package called <a
target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA"><code>loralib</code></a> to
facilitate the use of LoRA. With this library, any linear layer
implemented as <code>nn.Linear()</code> can be replaced by
<code>lora.Linear()</code>. This is possible because LoRA is designed to
work with any layer involving matrix multiplication. The
<code>lora.Linear()</code> module introduces a pair of low-rank
adaptation matrices, which are used to modify the original weight matrix
by applying a low-rank decomposition.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ===== Before =====</span></span><br><span class="line"><span class="comment"># layer = nn.Linear(in_features, out_features)</span></span><br><span class="line"><span class="comment"># ===== After ======</span></span><br><span class="line"><span class="keyword">import</span> loralib <span class="keyword">as</span> lora</span><br><span class="line"><span class="comment"># Add a pair of low-rank adaptation matrices with rank r=16</span></span><br><span class="line">layer = lora.Linear(in_features, out_features, r=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>
<p>Before training the model, all non-lora matrix should be fixed and
only LoRA matrices should be set as trainable. Training loops can run as
usual.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> loralib <span class="keyword">as</span> lora</span><br><span class="line">model = BigModel()</span><br><span class="line"><span class="comment"># This sets requires_grad to False for all parameters without the string &quot;lora_&quot; in their names</span></span><br><span class="line">lora.mark_only_lora_as_trainable(model)</span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> dataloader:</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure>
<p>When saving model checkpoints during LoRA fine-tuning, only the
LoRA-specific parameters need to be saved, not the entire large
pre-trained model. This results in significantly smaller checkpoint
files and more efficient storage.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ===== Before =====</span></span><br><span class="line"><span class="comment"># torch.save(model.state_dict(), checkpoint_path)</span></span><br><span class="line"><span class="comment"># ===== After =====</span></span><br><span class="line">torch.save(lora.lora_state_dict(model), checkpoint_path)</span><br></pre></td></tr></table></figure>
<h3 id="implementation-of-lora---lora.linear">Implementation of LoRA -
lora.Linear()</h3>
<p>Let's take a deep dive into the <code>lora.Linear()</code> <a
target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA/blob/main/loralib/layers.py">source
code</a>:</p>
<p>The <code>lora.Linear</code> class builds upon
<code>torch.nn.Linear()</code>. It retains the original weight matrix $
W $ as initialized in
<code>nn.Linear.__init__(self, in_features, out_features)</code>, and
introduces two additional LoRA matrices: <code>self.lora_A</code> and
<code>self.lora_B</code>. The matrix <code>self.lora_A</code> has
dimensions of $ (r, ) $, while <code>self.lora_B</code> has dimensions
of $ (, r) $. These matrices are used to adapt the original weight
matrix through low-rank decomposition.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Linear, LoRALayer):</span><br><span class="line">    <span class="comment"># LoRA implemented in a dense layer</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, </span></span><br><span class="line"><span class="params">        in_features: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        out_features: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        r: <span class="built_in">int</span> = <span class="number">0</span>, </span></span><br><span class="line"><span class="params">        lora_alpha: <span class="built_in">int</span> = <span class="number">1</span>, </span></span><br><span class="line"><span class="params">        lora_dropout: <span class="built_in">float</span> = <span class="number">0.</span>,</span></span><br><span class="line"><span class="params">        fan_in_fan_out: <span class="built_in">bool</span> = <span class="literal">False</span>, <span class="comment"># Set this to True if the layer to replace stores weight like (fan_in, fan_out)</span></span></span><br><span class="line"><span class="params">        merge_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        **kwargs</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        nn.Linear.__init__(<span class="variable language_">self</span>, in_features, out_features, **kwargs)</span><br><span class="line">        LoRALayer.__init__(<span class="variable language_">self</span>, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,</span><br><span class="line">                           merge_weights=merge_weights)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.fan_in_fan_out = fan_in_fan_out</span><br><span class="line">        <span class="comment"># Actual trainable parameters</span></span><br><span class="line">        <span class="keyword">if</span> r &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="variable language_">self</span>.lora_A = nn.Parameter(<span class="variable language_">self</span>.weight.new_zeros((r, in_features)))</span><br><span class="line">            <span class="variable language_">self</span>.lora_B = nn.Parameter(<span class="variable language_">self</span>.weight.new_zeros((out_features, r)))</span><br><span class="line">            <span class="variable language_">self</span>.scaling = <span class="variable language_">self</span>.lora_alpha / <span class="variable language_">self</span>.r</span><br><span class="line">            <span class="comment"># Freezing the pre-trained weight matrix</span></span><br><span class="line">            <span class="variable language_">self</span>.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="variable language_">self</span>.reset_parameters()</span><br><span class="line">        <span class="keyword">if</span> fan_in_fan_out:</span><br><span class="line">            <span class="variable language_">self</span>.weight.data = <span class="variable language_">self</span>.weight.data.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>In the <code>forward()</code> function, it implements <span
class="math inline">\(h=W_0x + \Delta Wx = W_0x+ BAx\)</span>.</p>
<p>There is a flag variable called <code>self.merge</code> which is use
to flag whether it's doing inference or training. Recall that the
original weight matrix remaining unchanged during LoRA training is a key
feature of the LoRA - pre-trained weights are freezed and instead small,
low-rank matrices are trained to approximate updates.</p>
<ul>
<li>During inference, if <code>merge_weights</code> is set to
<code>True</code>, the low-rank updates
<code>self.lora_B @ self.lora_A</code> are added directly to the frozen
pre-trained weights (<code>self.weight</code>). This avoids the need for
separate computations of LoRA updates during forward passes, improving
efficiency.</li>
<li>During training, if <code>merge_weights</code> is enabled and
weights were previously merged, the updates are subtracted from
<code>self.weight</code> to revert it to its original frozen state. This
ensures that gradients are not incorrectly computed on the merged
weights.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Linear, LoRALayer):</span><br><span class="line">    <span class="comment"># LoRA implemented in a dense layer</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, </span></span><br><span class="line"><span class="params">        in_features: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        out_features: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        r: <span class="built_in">int</span> = <span class="number">0</span>, </span></span><br><span class="line"><span class="params">        lora_alpha: <span class="built_in">int</span> = <span class="number">1</span>, </span></span><br><span class="line"><span class="params">        lora_dropout: <span class="built_in">float</span> = <span class="number">0.</span>,</span></span><br><span class="line"><span class="params">        fan_in_fan_out: <span class="built_in">bool</span> = <span class="literal">False</span>, <span class="comment"># Set this to True if the layer to replace stores weight like (fan_in, fan_out)</span></span></span><br><span class="line"><span class="params">        merge_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        **kwargs</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line"></span><br><span class="line">      ......</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, mode: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">T</span>(<span class="params">w</span>):</span><br><span class="line">            <span class="keyword">return</span> w.transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="keyword">if</span> <span class="variable language_">self</span>.fan_in_fan_out <span class="keyword">else</span> w</span><br><span class="line">        nn.Linear.train(<span class="variable language_">self</span>, mode)</span><br><span class="line">        <span class="keyword">if</span> mode:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.merge_weights <span class="keyword">and</span> <span class="variable language_">self</span>.merged:</span><br><span class="line">                <span class="comment"># Make sure that the weights are not merged</span></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.r &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="variable language_">self</span>.weight.data -= T(<span class="variable language_">self</span>.lora_B @ <span class="variable language_">self</span>.lora_A) * <span class="variable language_">self</span>.scaling</span><br><span class="line">                <span class="variable language_">self</span>.merged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.merge_weights <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>.merged:</span><br><span class="line">                <span class="comment"># Merge the weights and mark it</span></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.r &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="variable language_">self</span>.weight.data += T(<span class="variable language_">self</span>.lora_B @ <span class="variable language_">self</span>.lora_A) * <span class="variable language_">self</span>.scaling</span><br><span class="line">                <span class="variable language_">self</span>.merged = <span class="literal">True</span>    </span><br><span class="line">                </span><br><span class="line">			<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">T</span>(<span class="params">w</span>):</span><br><span class="line">            <span class="keyword">return</span> w.transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="keyword">if</span> <span class="variable language_">self</span>.fan_in_fan_out <span class="keyword">else</span> w</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>.merged:</span><br><span class="line">            result = F.linear(x, T(<span class="variable language_">self</span>.weight), bias=<span class="variable language_">self</span>.bias)            </span><br><span class="line">            result += (<span class="variable language_">self</span>.lora_dropout(x) @ <span class="variable language_">self</span>.lora_A.transpose(<span class="number">0</span>, <span class="number">1</span>) @ <span class="variable language_">self</span>.lora_B.transpose(<span class="number">0</span>, <span class="number">1</span>)) * <span class="variable language_">self</span>.scaling</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> F.linear(x, T(<span class="variable language_">self</span>.weight), bias=<span class="variable language_">self</span>.bias)</span><br><span class="line">          </span><br><span class="line">      ......</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/12/25/Langchain-Common-Practices/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/12/25/Langchain-Common-Practices/" class="post-title-link" itemprop="url">LangChain Common Practices</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-25 05:04:50" itemprop="dateCreated datePublished" datetime="2024-12-25T05:04:50-05:00">2024-12-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>This is a collection of some common useful LangChain (v.0.3.3)
practices based on my coding experience so far.</p>
<h2 id="llm-application-development-landscape">LLM Application
Development Landscape</h2>
<p>Nowadays, LLM applications can be classified into the following
categories.</p>
<ol type="1">
<li><p><strong>Simple LLM Calls</strong></p>
<p>Applications where LLMs are used directly to answer questions or
perform tasks without additional layers of complexity. The focus is on
generating responses to prompts or queries. These are straightforward
implementations, often used for tasks like content generation, question
answering, or summarization.</p>
<p>Real-world examples:</p>
<ul>
<li>ChatGPT for Q&amp;A: Users input questions, and the model directly
generates answers.</li>
<li>Copywriting Tools: Applications like Jasper AI create marketing
content, blogs, or product descriptions based on user inputs.</li>
</ul></li>
<li><p><strong>Vectorstores (RAG)</strong></p>
<p>Vectorstores are used in Retrieval-Augmented Generation (RAG)
applications, where relevant information is retrieved from a database of
embeddings (vectorized representations of text) to enhance the LLM's
responses. This allows the LLM to work with domain-specific or
proprietary knowledge not contained in its training data.</p>
<p>Real-world examples:</p>
<ul>
<li>Chatbots for Enterprises: A customer support chatbot retrieves
relevant product documentation or FAQs stored in a vectorstore to
provide accurate responses.</li>
<li>Search-Augmented Systems: Google Bard integrates real-time
information retrieval to provide up-to-date and contextually relevant
responses.</li>
</ul></li>
<li><p><strong>Agents</strong></p>
<p>Agents are LLM-driven systems that execute tasks autonomously or
semi-autonomously based on input instructions. They can make decisions,
interact with APIs, and manage workflows. Agents often use reasoning
frameworks like ReAct (Reasoning and Acting) to decide what steps to
take next.</p>
<p>Real-world examples:</p>
<ul>
<li>Zapier AI Assistant: Automates workflows by taking instructions,
analyzing data, and executing API calls or actions across
platforms.</li>
<li>LangChain Agents: Used for multi-step tasks such as filling out
forms, managing databases, or performing calculations.</li>
</ul></li>
<li><p><strong>Agents + Vectorstores</strong></p>
<p>This combines the reasoning and decision-making capabilities of
agents with the data retrieval abilities of vectorstores. These systems
can autonomously fetch relevant knowledge from vectorstores and execute
tasks, enabling advanced applications like AutoGPT. The integration
provides both reasoning depth and domain-specific accuracy.</p>
<p>Real-world examples:</p>
<ul>
<li><strong>AutoGPT:</strong> An open-source agent that can generate
business plans by researching topics, retrieving relevant information,
and autonomously completing subtasks.</li>
<li><strong>GPT Engineer:</strong> Helps developers by retrieving
relevant programming resources and autonomously generating code,
debugging, or improving software projects.</li>
</ul></li>
</ol>
<h2 id="chaining-a-simple-prompt">Chaining a Simple Prompt</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain.prompts.prompt <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_ollama <span class="keyword">import</span> ChatOllama</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define information to be incorporated to the prompt template</span></span><br><span class="line">information = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Elon Reeve Musk (/ˈiːlɒn/; EE-lon; born June 28, 1971) is a businessman and investor. </span></span><br><span class="line"><span class="string">    He is the founder, chairman, CEO, and CTO of SpaceX; angel investor, CEO, product architect and former chairman of Tesla, Inc.; owner, chairman and CTO of X Corp.; founder of the Boring Company and xAI; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. </span></span><br><span class="line"><span class="string">    He is the wealthiest person in the world, with an estimated net worth of US$232 billion as of December 2023, according to the Bloomberg Billionaires Index, and $254 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.</span></span><br><span class="line"><span class="string">    A member of the wealthy South African Musk family, Elon was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. </span></span><br><span class="line"><span class="string">    Two years later, he matriculated at Queen&#x27;s University at Kingston in Canada. Musk later transferred to the University of Pennsylvania, and received bachelor&#x27;s degrees in economics and physics. </span></span><br><span class="line"><span class="string">    He moved to California in 1995 to attend Stanford University. However, Musk dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. </span></span><br><span class="line"><span class="string">    The startup was acquired by Compaq for $307 million in 1999, and, that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal.</span></span><br><span class="line"><span class="string">    In October 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. </span></span><br><span class="line"><span class="string">    In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. </span></span><br><span class="line"><span class="string">    In 2006, Musk helped create SolarCity, a solar-energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. </span></span><br><span class="line"><span class="string">    In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. </span></span><br><span class="line"><span class="string">    The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. </span></span><br><span class="line"><span class="string">    In 2022, he acquired Twitter for $44 billion. He subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. </span></span><br><span class="line"><span class="string">    In March 2023, he founded xAI, an artificial intelligence company.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create a prompt template</span></span><br><span class="line">template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Given the information &#123;information&#125; about a person, please create:</span></span><br><span class="line"><span class="string">1. A short summary</span></span><br><span class="line"><span class="string">2. Two interesting facts about the person.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># incorporate information into prompt</span></span><br><span class="line">summary_prompt_template = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;information&quot;</span>], template=template</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create an llm</span></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model_name=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line"><span class="comment"># llm = ChatOllama(model=&quot;llama3&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create a chain</span></span><br><span class="line">chain = summary_prompt_template | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># prompt the model</span></span><br><span class="line">response = chain.invoke(<span class="built_in">input</span>=&#123;<span class="string">&quot;information&quot;</span>: information&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="parsing-the-output-with-a-customized-format">Parsing the Output
with a Customized Format</h2>
<p>Using <code>PydanticOutputParser</code> and user defined output data
structure.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.output_parsers <span class="keyword">import</span> PydanticOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.pydantic_v1 <span class="keyword">import</span> BaseModel, Field, validator</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model_name=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define your desired data structure</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Joke</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    setup: <span class="built_in">str</span> = Field(description=<span class="string">&quot;question to set up a joke&quot;</span>)</span><br><span class="line">    punchline: <span class="built_in">str</span> = Field(description=<span class="string">&quot;answer to resolve the joke&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add custom validation logic easily with Pydantic</span></span><br><span class="line"><span class="meta">    @validator(<span class="params"><span class="string">&quot;setup&quot;</span></span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">question_ends_with_question_mark</span>(<span class="params">cls, field</span>):</span><br><span class="line">        <span class="keyword">if</span> field[-<span class="number">1</span>] != <span class="string">&quot;?&quot;</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Badly formed question!&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># set up a parser + inject instructions into the prompt template.</span></span><br><span class="line">parser = PydanticOutputParser(pydantic_object=Joke)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a prompt with query and instruction</span></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    template=<span class="string">&quot;Answer the user query.\n&#123;format_instructions&#125;\n&#123;query&#125;\n&quot;</span>,</span><br><span class="line">    input_variables=[<span class="string">&quot;query&quot;</span>],</span><br><span class="line">    partial_variables=&#123;<span class="string">&quot;format_instructions&quot;</span>: parser.get_format_instructions()&#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#a query intended to prompt a language model to populate the data structure</span></span><br><span class="line">prompt_and_model = prompt | llm</span><br><span class="line">output = prompt_and_model.invoke(&#123;<span class="string">&quot;query&quot;</span>: <span class="string">&quot;Tell me a joke.&quot;</span>&#125;)</span><br><span class="line">parser.invoke(output)</span><br></pre></td></tr></table></figure>
<h2 id="data-ingestion-to-pinecone-vectorstore-rag">Data Ingestion to
Pinecone Vectorstore (RAG)</h2>
<p>Using <code>TextLoader</code>, <code>CharacterTextSplitter</code>,
<code>OpenAIEmbeddings</code>, and <a
target="_blank" rel="noopener" href="https://app.pinecone.io/">Pinecone</a> vector database.</p>
<p>Please refer to <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/#text-splitters">LangChain
text splitter techniques</a> ;<a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/character_text_splitter/">text
Split by character</a>; <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/embed_text/">text
embedding models</a> for more details.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_pinecone <span class="keyword">import</span> PineconeVectorStore</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;doc1.txt&quot;</span>)</span><br><span class="line">document = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># split data</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">texts = text_splitter.split_documents(document)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create embedding</span></span><br><span class="line">embeddings = OpenAIEmbeddings(openai_api_key=os.environ.get(<span class="string">&quot;OPENAI_API_KEY&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ingest data to vector db</span></span><br><span class="line">PineconeVectorStore.from_documents(texts, embeddings, index_name=os.environ[<span class="string">&#x27;INDEX_NAME&#x27;</span>])<span class="built_in">print</span>(<span class="string">&quot;finish&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="data-retrieval-from-pinecone-vectorestore-rag">Data Retrieval
from Pinecone Vectorestore (RAG)</h2>
<p><code>langchain-ai/retrieval-qa-chat</code> is a <a
target="_blank" rel="noopener" href="https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat?organizationId=9f3e510d-ce6b-4f98-98a3-ff383fda2d96">ChatPromptTemplate</a>
ensuring answers are based solely on the context.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build user query prompt</span></span><br><span class="line">query = <span class="string">&quot;what is Pinecone in machine learning?&quot;</span></span><br><span class="line">chain = PromptTemplate.from_template(template=query) | llm</span><br><span class="line"></span><br><span class="line"><span class="comment"># store query prompt to vector db</span></span><br><span class="line">vectorstore = PineconeVectorStore(</span><br><span class="line">    index_name=os.environ[<span class="string">&quot;INDEX_NAME&quot;</span>], embedding=embeddings</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a retrieval qa prompt</span></span><br><span class="line">retrieval_qa_chat_prompt = hub.pull(<span class="string">&quot;langchain-ai/retrieval-qa-chat&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a prompt chain</span></span><br><span class="line">combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create retrieval chain</span></span><br><span class="line">retrival_chain = create_retrieval_chain(</span><br><span class="line">    retriever=vectorstore.as_retriever(), combine_docs_chain=combine_docs_chain</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute the retrieval</span></span><br><span class="line">result = retrival_chain.invoke(<span class="built_in">input</span>=&#123;<span class="string">&quot;input&quot;</span>: query&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Customized retrieval prompt:</p>
<p><code>RunnablePassthrough</code> is used to pass through arguments <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/passthrough/">from one
step to the next</a>. It allows us to pass on the user's question to the
prompt and model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_pinecone <span class="keyword">import</span> PineconeVectorStore</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_docs</span>(<span class="params">docs</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;\n\n&quot;</span>.join(doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs)</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line">query = <span class="string">&quot;what is Pinecone in machine learning?&quot;</span></span><br><span class="line">chain = PromptTemplate.from_template(template=query) | llm</span><br><span class="line"></span><br><span class="line">vectorstore = PineconeVectorStore(</span><br><span class="line">    index_name=os.environ[<span class="string">&quot;INDEX_NAME&quot;</span>], embedding=embeddings</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Use the following pieces of context to answer the question at the end. </span></span><br><span class="line"><span class="string">If you don&#x27;t know the answer, you can say &quot;I don&#x27;t know&quot;. Don&#x27;t make up an answer.</span></span><br><span class="line"><span class="string">Use three sentences maximum and keep the answer short and to the point.</span></span><br><span class="line"><span class="string">Always say &quot;thanks for the question&quot; before answering the question.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;context&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: &#123;question&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Answer:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">custom_rag_prompt = PromptTemplate.from_template(template=template)</span><br><span class="line">rag_chain = (</span><br><span class="line">    &#123;<span class="string">&quot;context&quot;</span>: vectorstore.as_retriever() | format_docs, <span class="string">&quot;question&quot;</span>: RunnablePassthrough()&#125;</span><br><span class="line">    | custom_rag_prompt</span><br><span class="line">    | llm</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">res = rag_chain.invoke(query)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="chat-with-a-pdf-rag-with-faiss">Chat with a PDF (RAG with
FAISS)</h2>
<p>Using <code>PyPDFLoader</code>, <code>CharacterTextSplitter</code>,
<code>OpenAIEmbeddings</code>, and <code>FAISS</code> local vector
database.</p>
<p>Please refer to <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/document_loader_pdf/">PDF
loader</a> ; <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/vectorstores/faiss/">Langchain
FAISS vectorstore</a>; <a target="_blank" rel="noopener" href="https://faiss.ai/">FAISS</a> for more
details.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> PyPDFLoader</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.chains.retrieval <span class="keyword">import</span> create_retrieval_chain</span><br><span class="line"><span class="keyword">from</span> langchain.chains.combine_documents <span class="keyword">import</span> create_stuff_documents_chain</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"></span><br><span class="line">pdf_path = <span class="string">&quot;react.pdf&quot;</span></span><br><span class="line">loader = PyPDFLoader(file_path=pdf_path)</span><br><span class="line">documents = loader.load()</span><br><span class="line">text_splitter = CharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">30</span>, separator=<span class="string">&quot;\n&quot;</span></span><br><span class="line">)</span><br><span class="line">docs = text_splitter.split_documents(documents=documents)</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">vectorstore = FAISS.from_documents(docs, embeddings)</span><br><span class="line">vectorstore.save_local(<span class="string">&quot;faiss_index_react&quot;</span>)</span><br><span class="line"></span><br><span class="line">new_vectorstore = FAISS.load_local(</span><br><span class="line">    <span class="string">&quot;faiss_index_react&quot;</span>, embeddings, allow_dangerous_deserialization=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">retrieval_qa_chat_prompt = hub.pull(<span class="string">&quot;langchain-ai/retrieval-qa-chat&quot;</span>)</span><br><span class="line">combine_docs_chain = create_stuff_documents_chain(</span><br><span class="line">    OpenAI(), retrieval_qa_chat_prompt</span><br><span class="line">)</span><br><span class="line">retrieval_chain = create_retrieval_chain(</span><br><span class="line">    new_vectorstore.as_retriever(), combine_docs_chain</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">res = retrieval_chain.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Give me the gist of ReAct in 3 sentences&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(res[<span class="string">&quot;answer&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="create-a-react-agent">Create a ReAct Agent</h2>
<p>Using <code>langchain-ai/react-agent-template</code> to build a <a
target="_blank" rel="noopener" href="https://smith.langchain.com/hub/langchain-ai/react-agent-template?organizationId=9f3e510d-ce6b-4f98-98a3-ff383fda2d96">ReAct
prompt</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> create_react_agent, AgentExecutor</span><br><span class="line"><span class="keyword">from</span> langchain_experimental.tools <span class="keyword">import</span> PythonREPLTool</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create an instruction</span></span><br><span class="line">instructions = <span class="string">&quot;&quot;&quot;You are an agent designed to write and execute python code to answer questions.</span></span><br><span class="line"><span class="string">You have access to a python REPL, which you can use to execute python code.</span></span><br><span class="line"><span class="string">If you get an error, debug your code and try again.</span></span><br><span class="line"><span class="string">Only use the output of your code to answer the question. </span></span><br><span class="line"><span class="string">You might know the answer without running any code, but you should still run the code to get the answer.</span></span><br><span class="line"><span class="string">If it does not seem like you can write code to answer the question, just return &quot;I don&#x27;t know&quot; as the answer.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use an ReAct prompt template</span></span><br><span class="line">base_prompt = hub.pull(<span class="string">&quot;langchain-ai/react-agent-template&quot;</span>)</span><br><span class="line">prompt = base_prompt.partial(instructions=instructions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make use a tool to execute python code</span></span><br><span class="line">tools = [PythonREPLTool()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a ReAct agent</span></span><br><span class="line">agent = create_react_agent(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o-mini&quot;</span>),</span><br><span class="line">    tools=tools,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create the ReAct agent executor</span></span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute the ReAct agent</span></span><br><span class="line">agent_executor.invoke(</span><br><span class="line">    <span class="built_in">input</span>=&#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;&quot;&quot;generate and save in current working directory a QRcode</span></span><br><span class="line"><span class="string">                    that point to https://jokerdii.github.io/di-blog, you have qrcode package installed already&quot;&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">agent_executor.invoke(</span><br><span class="line">    <span class="built_in">input</span>=&#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;&quot;&quot;generate and save in current working directory a synthetic csv dataset </span></span><br><span class="line"><span class="string">                    with 1000 rows and 2 columns that is about Amazon product description and price.&quot;&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="using-an-langchain-agent-for-tasks">Using an LangChain Agent for
Tasks</h2>
<p><code>create_csv_agent</code> is an <code>AgentExecutor</code> object
able to perform operations in CSVs.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_experimental.agents.agent_toolkits <span class="keyword">import</span> create_csv_agent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># make use a CSV agent from langchain</span></span><br><span class="line">csv_agent = create_csv_agent(</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o-mini&quot;</span>),</span><br><span class="line">    path=<span class="string">&quot;episode_info.csv&quot;</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute the agent</span></span><br><span class="line">csv_agent.invoke(</span><br><span class="line">    <span class="built_in">input</span>=&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;how many columns are there in file episode_info.csv&quot;</span>&#125;</span><br><span class="line">)</span><br><span class="line">csv_agent.invoke(</span><br><span class="line">    <span class="built_in">input</span>=&#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;print the seasons by ascending order of the number of episodes they have.&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2
id="creating-an-react-agent-with-multiple-agents-provided-as-tools">Creating
an ReAct Agent with Multiple Agents Provided as Tools</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> (</span><br><span class="line">    create_react_agent,</span><br><span class="line">    AgentExecutor,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain_experimental.tools <span class="keyword">import</span> PythonREPLTool</span><br><span class="line"><span class="keyword">from</span> langchain_experimental.agents.agent_toolkits <span class="keyword">import</span> create_csv_agent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">instructions = <span class="string">&quot;&quot;&quot;You are an agent designed to write and execute python code to answer questions.</span></span><br><span class="line"><span class="string">You have access to a python REPL, which you can use to execute python code.</span></span><br><span class="line"><span class="string">You have qrcode package installed</span></span><br><span class="line"><span class="string">If you get an error, debug your code and try again.</span></span><br><span class="line"><span class="string">Only use the output of your code to answer the question. </span></span><br><span class="line"><span class="string">You might know the answer without running any code, but you should still run the code to get the answer.</span></span><br><span class="line"><span class="string">If it does not seem like you can write code to answer the question, just return &quot;I don&#x27;t know&quot; as the answer.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">base_prompt = hub.pull(<span class="string">&quot;langchain-ai/react-agent-template&quot;</span>)</span><br><span class="line">prompt = base_prompt.partial(instructions=instructions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define python agent</span></span><br><span class="line">tools = [PythonREPLTool()]</span><br><span class="line">python_agent = create_react_agent(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4-turbo&quot;</span>),</span><br><span class="line">    tools=tools,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">python_agent_executor = AgentExecutor(agent=python_agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define CSV agent</span></span><br><span class="line">csv_agent_executor: AgentExecutor = create_csv_agent(</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>),</span><br><span class="line">    path=<span class="string">&quot;episode_info.csv&quot;</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### router grand agent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># list agent tools</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">python_agent_executor_wrapper</span>(<span class="params">original_prompt: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">    <span class="keyword">return</span> python_agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: original_prompt&#125;)</span><br><span class="line"></span><br><span class="line">tools = [</span><br><span class="line">    Tool(</span><br><span class="line">        name=<span class="string">&quot;Python Agent&quot;</span>,</span><br><span class="line">        func=python_agent_executor_wrapper,</span><br><span class="line">        description=<span class="string">&quot;&quot;&quot;useful when you need to transform natural language to python and execute the python code,</span></span><br><span class="line"><span class="string">                        returning the results of the code execution</span></span><br><span class="line"><span class="string">                        DOES NOT ACCEPT CODE AS INPUT&quot;&quot;&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">    Tool(</span><br><span class="line">        name=<span class="string">&quot;CSV Agent&quot;</span>,</span><br><span class="line">        func=csv_agent_executor.invoke,</span><br><span class="line">        description=<span class="string">&quot;&quot;&quot;useful when you need to answer question over episode_info.csv file,</span></span><br><span class="line"><span class="string">                        takes an input the entire question and returns the answer after running pandas calculations&quot;&quot;&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># create grand ReAct agent</span></span><br><span class="line">prompt = base_prompt.partial(instructions=<span class="string">&quot;&quot;</span>)</span><br><span class="line">grand_agent = create_react_agent(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4-turbo&quot;</span>),</span><br><span class="line">    tools=tools,</span><br><span class="line">)</span><br><span class="line">grand_agent_executor = AgentExecutor(agent=grand_agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute grand ReAct agent and print output</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    grand_agent_executor.invoke(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;which season has the most episodes?&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    grand_agent_executor.invoke(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;Generate and save in current working directory 15 qrcodes that point to `www.udemy.com/course/langchain`&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="function-tool-calling">Function / Tool Calling</h2>
<p>LangChain provides a standardized interface for connecting tools to
models.</p>
<ul>
<li><code>ChatModel.bind_tools()</code>: a method for attaching tool
definitions to model calls.</li>
<li><code>AIMessage.tool_calls</code>: an attribute on the
<code>AIMessage</code> returned from the model for easily accessing the
tool aclls the model decided to make.</li>
<li><code>create_tool_calling_agent</code>: an agent constsructor that
works with ANY model that implements <code>bind_tools</code> and returns
<code>tool_calls</code>.</li>
</ul>
<p>Directly using <code>PythonREPLTool</code> which is already a tool
object. Use with caution because <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/tools/python/">Python
REPL</a> can execute arbitrary code on the host machine (e.g., delete
files, make network requests).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> tool</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> create_tool_calling_agent, AgentExecutor</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilySearchResults</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">x: <span class="built_in">float</span>, y: <span class="built_in">float</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply &#x27;x&#x27; times &#x27;y&#x27;.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> x * y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;you&#x27;re a helpful assistant&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;placeholder&quot;</span>, <span class="string">&quot;&#123;agent_scratchpad&#125;&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tools = [TavilySearchResults(), multiply]</span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>)</span><br><span class="line"></span><br><span class="line">agent = create_tool_calling_agent(llm, tools, prompt)</span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools)</span><br><span class="line"></span><br><span class="line">response = agent_executor.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;what is the weather in dubai right now? compare it with San Fransisco, output should in in celsious&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="tool-calling-with-langchain">Tool Calling with Langchain</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply a and b.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        a: first int</span></span><br><span class="line"><span class="string">        b: second int</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line">llm_with_tools = tool_calling_model.bind_tools([multiply])</span><br><span class="line"></span><br><span class="line">result = llm_with_tools.invoke(<span class="string">&quot;What is 2 multiplied by 3?&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="token-limitation-handling-strategies">Token Limitation Handling
Strategies</h2>
<p>when passing documents into the LLM context window, there are three
approaches for handling context window limitations:</p>
<ol type="1">
<li><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/tutorials/summarization/">Stuffing</a>:
suff all documents into a single prompt</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.combine_documents.stuff <span class="keyword">import</span> StuffDocumentsChain</span><br><span class="line"><span class="keyword">from</span> langchain.chains.llm <span class="keyword">import</span> LLMChain</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># define prompt</span></span><br><span class="line">prompt_template = <span class="string">&quot;&quot;&quot;Write a concise summary of the following:</span></span><br><span class="line"><span class="string">&quot;&#123;text&#125;&quot;</span></span><br><span class="line"><span class="string">CONCISE SUMMARY:&quot;&quot;&quot;</span></span><br><span class="line">prompt = PromptTemplate.from_template(prompt_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define LLM chain</span></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model_name=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>)</span><br><span class="line">llm_chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define StuffDocumentsChain</span></span><br><span class="line">stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=<span class="string">&quot;text&quot;</span>)</span><br><span class="line">docs = loader.load()</span><br><span class="line"><span class="built_in">print</span>(stuff_chain.run(docs))</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/tutorials/summarization/">Map-reduce</a>:
summarize each document on its own in parallel and put them into a final
summary.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> MapReduceDocumentsChain, ReduceDocumentsChain</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line"><span class="comment"># Map</span></span><br><span class="line">map_template = <span class="string">&quot;&quot;&quot;The following is a set of documents</span></span><br><span class="line"><span class="string">&#123;docs&#125;</span></span><br><span class="line"><span class="string">Based on this list of docs, please identify the main themes </span></span><br><span class="line"><span class="string">Helpful Answer:&quot;&quot;&quot;</span></span><br><span class="line">map_prompt = PromptTemplate.from_template(map_template)</span><br><span class="line">map_chain = LLMChain(llm=llm, prompt=map_prompt)</span><br><span class="line"><span class="comment"># Reduce</span></span><br><span class="line">reduce_template = <span class="string">&quot;&quot;&quot;The following is set of summaries:</span></span><br><span class="line"><span class="string">&#123;docs&#125;</span></span><br><span class="line"><span class="string">Take these and distill it into a final, consolidated summary of the main themes. </span></span><br><span class="line"><span class="string">Helpful Answer:&quot;&quot;&quot;</span></span><br><span class="line">reduce_prompt = PromptTemplate.from_template(reduce_template)</span><br><span class="line">reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)</span><br><span class="line"><span class="comment"># Combine documents by mapping a chain over them, then combining results</span></span><br><span class="line">map_reduce_chain = MapReduceDocumentsChain(</span><br><span class="line">    llm_chain=map_chain,</span><br><span class="line">    reduce_documents_chain=reduce_documents_chain,</span><br><span class="line">    document_variable_name=<span class="string">&quot;docs&quot;</span>,</span><br><span class="line">    return_intermediate_steps=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line">text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">split_docs = text_splitter.split_documents(docs)</span><br><span class="line"><span class="built_in">print</span>(map_reduce_chain.run(split_docs))</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>Refine: The refine documents chain constructs a response by looping
over the input documents and iteratively updating its answer.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.summarize <span class="keyword">import</span> load_summarize_chain</span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">                  Please provide a summary of the following text.</span></span><br><span class="line"><span class="string">                  TEXT: &#123;text&#125;</span></span><br><span class="line"><span class="string">                  SUMMARY:</span></span><br><span class="line"><span class="string">                  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">question_prompt = PromptTemplate(</span><br><span class="line">    template=question_prompt_template, input_variables=[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">refine_prompt_template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              Write a concise summary of the following text delimited by triple backquotes.</span></span><br><span class="line"><span class="string">              Return your response in bullet points which covers the key points of the text.</span></span><br><span class="line"><span class="string">              ```&#123;text&#125;```</span></span><br><span class="line"><span class="string">              BULLET POINT SUMMARY:</span></span><br><span class="line"><span class="string">              &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">refine_template = PromptTemplate(</span><br><span class="line">    template=refine_prompt_template, input_variables=[<span class="string">&quot;text&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load refine chain</span></span><br><span class="line">chain = load_summarize_chain(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">&quot;refine&quot;</span>,</span><br><span class="line">    question_prompt=question_prompt,</span><br><span class="line">    refine_prompt=refine_prompt,</span><br><span class="line">    return_intermediate_steps=<span class="literal">True</span>,</span><br><span class="line">    input_key=<span class="string">&quot;input_documents&quot;</span>,</span><br><span class="line">    output_key=<span class="string">&quot;output_text&quot;</span>,</span><br><span class="line">)</span><br><span class="line">result = chain(&#123;<span class="string">&quot;input_documents&quot;</span>: split_docs&#125;, return_only_outputs=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="coreference-resolution">Coreference Resolution</h2>
<p><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/chatbots_memory/">Adding
memory to chatbots.</a></p>
<p>LangChain provides a way to build applications that have memory using
LangGraph's <a
target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/concepts/persistence/">persistence</a>.
You can <a
target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/persistence/">enable
persistence</a> in LangGraph applications by providing a
<code>checkpointer</code> when compiling the graph. Every iteration,
LangGraph takes the information and saves it in a DB (PostgreSQL, MySQL,
Redis, and MongoDB saver).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> START, MessagesState, StateGraph</span><br><span class="line"></span><br><span class="line">workflow = StateGraph(state_schema=MessagesState)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the function that calls the model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: MessagesState</span>):</span><br><span class="line">    system_prompt = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant. &quot;</span></span><br><span class="line">        <span class="string">&quot;Answer all questions to the best of your ability.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    messages = [SystemMessage(content=system_prompt)] + state[<span class="string">&quot;messages&quot;</span>]</span><br><span class="line">    response = model.invoke(messages)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: response&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the node and edge</span></span><br><span class="line">workflow.add_node(<span class="string">&quot;model&quot;</span>, call_model)</span><br><span class="line">workflow.add_edge(START, <span class="string">&quot;model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add simple in-memory checkpointer</span></span><br><span class="line">memory = MemorySaver()</span><br><span class="line">app = workflow.<span class="built_in">compile</span>(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<p>Langchain has three main strategies to manage state:</p>
<ol type="1">
<li>Simply stuffing previous messages into a chat model prompt.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> AIMessage, HumanMessage, SystemMessage</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        SystemMessage(</span><br><span class="line">            content=<span class="string">&quot;You are a helpful assistant. Answer all questions to the best of your ability.&quot;</span></span><br><span class="line">        ),</span><br><span class="line">        MessagesPlaceholder(variable_name=<span class="string">&quot;messages&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain = prompt | model</span><br><span class="line"></span><br><span class="line">ai_msg = chain.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: [</span><br><span class="line">            HumanMessage(</span><br><span class="line">                content=<span class="string">&quot;Translate from English to French: I love programming.&quot;</span></span><br><span class="line">            ),</span><br><span class="line">            AIMessage(content=<span class="string">&quot;J&#x27;adore la programmation.&quot;</span>),</span><br><span class="line">            HumanMessage(content=<span class="string">&quot;What did you just say?&quot;</span>),</span><br><span class="line">        ],</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(ai_msg.content)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>The above, but trimming old messages to reduce the amount of
distracting information the model has to deal with.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> trim_messages</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> START, MessagesState, StateGraph</span><br><span class="line"></span><br><span class="line"><span class="comment"># define trimmer</span></span><br><span class="line"><span class="comment"># count each message as 1 &quot;token&quot; (token_counter=len) and keep only the last two messages</span></span><br><span class="line">trimmer = trim_messages(strategy=<span class="string">&quot;last&quot;</span>, max_tokens=<span class="number">2</span>, token_counter=<span class="built_in">len</span>)</span><br><span class="line"></span><br><span class="line">workflow = StateGraph(state_schema=MessagesState)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the function that calls the model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: MessagesState</span>):</span><br><span class="line">    trimmed_messages = trimmer.invoke(state[<span class="string">&quot;messages&quot;</span>])</span><br><span class="line">    system_prompt = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant. &quot;</span></span><br><span class="line">        <span class="string">&quot;Answer all questions to the best of your ability.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    messages = [SystemMessage(content=system_prompt)] + trimmed_messages</span><br><span class="line">    response = model.invoke(messages)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: response&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the node and edge</span></span><br><span class="line">workflow.add_node(<span class="string">&quot;model&quot;</span>, call_model)</span><br><span class="line">workflow.add_edge(START, <span class="string">&quot;model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ddd simple in-memory checkpointer</span></span><br><span class="line">memory = MemorySaver()</span><br><span class="line">app = workflow.<span class="built_in">compile</span>(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>More complex modifications like synthesizing summaries for long
running conversations.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage, RemoveMessage</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> START, MessagesState, StateGraph</span><br><span class="line"></span><br><span class="line">workflow = StateGraph(state_schema=MessagesState)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function that calls the model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: MessagesState</span>):</span><br><span class="line">    system_prompt = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant. &quot;</span></span><br><span class="line">        <span class="string">&quot;Answer all questions to the best of your ability. &quot;</span></span><br><span class="line">        <span class="string">&quot;The provided chat history includes a summary of the earlier conversation.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    system_message = SystemMessage(content=system_prompt)</span><br><span class="line">    message_history = state[<span class="string">&quot;messages&quot;</span>][:-<span class="number">1</span>]  <span class="comment"># exclude the most recent user input</span></span><br><span class="line">    <span class="comment"># Summarize the messages if the chat history reaches a certain size</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(message_history) &gt;= <span class="number">4</span>:</span><br><span class="line">        last_human_message = state[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Invoke the model to generate conversation summary</span></span><br><span class="line">        summary_prompt = (</span><br><span class="line">            <span class="string">&quot;Distill the above chat messages into a single summary message. &quot;</span></span><br><span class="line">            <span class="string">&quot;Include as many specific details as you can.&quot;</span></span><br><span class="line">        )</span><br><span class="line">        summary_message = model.invoke(</span><br><span class="line">            message_history + [HumanMessage(content=summary_prompt)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Delete messages that we no longer want to show up</span></span><br><span class="line">        delete_messages = [RemoveMessage(<span class="built_in">id</span>=m.<span class="built_in">id</span>) <span class="keyword">for</span> m <span class="keyword">in</span> state[<span class="string">&quot;messages&quot;</span>]]</span><br><span class="line">        <span class="comment"># Re-add user message</span></span><br><span class="line">        human_message = HumanMessage(content=last_human_message.content)</span><br><span class="line">        <span class="comment"># Call the model with summary &amp; response</span></span><br><span class="line">        response = model.invoke([system_message, summary_message, human_message])</span><br><span class="line">        message_updates = [summary_message, human_message, response] + delete_messages</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        message_updates = model.invoke([system_message] + state[<span class="string">&quot;messages&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: message_updates&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the node and edge</span></span><br><span class="line">workflow.add_node(<span class="string">&quot;model&quot;</span>, call_model)</span><br><span class="line">workflow.add_edge(START, <span class="string">&quot;model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add simple in-memory checkpointer</span></span><br><span class="line">memory = MemorySaver()</span><br><span class="line">app = workflow.<span class="built_in">compile</span>(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<h2 id="tracing-application-with-langsmith">Tracing Application with
LangSmith</h2>
<p><a target="_blank" rel="noopener" href="https://www.langchain.com/langsmith">LangSmith</a> traces
LLM calls, tool usage, LLM model latency, token count, and cost.</p>
<p>To integrate LangSmith to our application, we need to generate an API
key, add it as "LANGCHAIN_API_KEY" in environment variables, install
langsmith dependency, setup our environment. Please refer to <a
target="_blank" rel="noopener" href="https://smith.langchain.com/o/9f3e510d-ce6b-4f98-98a3-ff383fda2d96/?paginationState=%7B%22pageIndex%22%3A0%2C%22pageSize%22%3A5%7D">set
up tracing</a> for detailed steps.</p>
<h2 id="langchain-hub">LangChain Hub</h2>
<p><a
target="_blank" rel="noopener" href="https://smith.langchain.com/hub?organizationId=9f3e510d-ce6b-4f98-98a3-ff383fda2d96">LangChain
Hub</a> is a comprehensive platform that serves as a repository for
pre-built components, tools, and configurations designed to accelerate
the development of LLM applications. It simplifies the integration of
various building blocks—models, prompts, chains, and agents—enabling
developers to create robust and scalable applications without starting
from scratch.</p>
<h2 id="langchain-text-splitter-playground">LangChain Text Splitter
Playground</h2>
<p><a target="_blank" rel="noopener" href="https://langchain-text-splitter.streamlit.app/">Text
Splitter Playground</a> is a user-friendly interface designed to help
developers experiment with and fine-tune text-splitting strategies. In
many LLM applications, particularly those involving large documents or
retrieval-augmented generation (RAG), it is essential to divide text
into manageable chunks while preserving context. This tool allows users
to optimize the chunking process for their specific needs.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/12/01/2024-December/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/12/01/2024-December/" class="post-title-link" itemprop="url">2024 December - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-01 21:54:34" itemprop="dateCreated datePublished" datetime="2024-12-01T21:54:34-05:00">2024-12-01</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="substack">Substack</h3>
<blockquote>
<p><em>the more the AV competition globally heats up, and the more large
players invest in the technology, including Tesla, the higher the demand
for the Uber platform will become for these AV players.</em></p>
<p><strong>― Uber Technologies – A brilliant business executing to
perfection (Quarterly Update) - Rijnberk InvestInsights</strong> [<a
target="_blank" rel="noopener" href="https://rijnberkinvestinsights.substack.com/p/uber-technologies-a-brilliant-business">Link</a>]</p>
</blockquote>
<p>This article predicts Uber to be a massive beneficiary of the AV /
Robotaxi revolution. There indeed is a moat.</p>
<blockquote>
<p><strong>Where do LLMs spend their FLOPS? - Artificial
Fintelligence</strong> [<a
target="_blank" rel="noopener" href="https://www.artfintel.com/p/where-do-llms-spend-their-flops">Link</a>]</p>
</blockquote>
<p><strong>Theoretical Analysis of LLM FLOPS Allocation</strong></p>
<ul>
<li><p><strong>FLOPS Distribution in Decoder Models:</strong></p>
<ul>
<li><p>Based on a standard decoder model, FLOPS are allocated as follows
for each layer:</p>
<p>6d² for computing Query (Q), Key (K), and Value (V) matrices.</p>
<p>2d² for computing the attention output matrix, using the formula:
softmax(Q @ K.T) @ V.</p>
<p>16d² for running the feedforward network (FFN).</p>
<p>This results in a total of 24d² FLOPS per layer.</p></li>
<li><p><strong>Percentage-wise:</strong></p>
<p>25% of the time is spent computing QKV.</p>
<p>~8% is spent computing the attention output matrix.</p>
<p>~66% is spent running the FFN.</p></li>
</ul></li>
<li><p><strong>Attention Mechanism:</strong></p>
<p>While the attention equation itself <span
class="math inline">\(softmax(QK^T/\sqrt{d_{head}})V\)</span> has
negligible computational cost (~0.005% for Llama7B) compared to other
operations, its impact on memory usage necessitates techniques like
<strong>KV cache</strong> and <strong>flash attention</strong>.</p></li>
<li><p><strong>KV Cache:</strong></p>
<p>The KV cache, essential for efficient attention computation, requires
O(T) memory, where T is the number of tokens generated.</p>
<p>The memory size of the KV cache is calculated as 4 * number of layers
* d_model bytes.</p>
<p>While the KV cache demands a significant amount of memory, it
essentially reuses the same memory space throughout the token generation
process.</p></li>
<li><p><strong>Modern Architectures:</strong></p>
<p>Architectures like Mistral 7B and Llama2 employ <strong>Grouped Query
Attention (GQA)</strong> and <strong>sliding window attention</strong>
to optimize performance.</p>
<p><strong>GQA</strong> reduces the KV cache size by sharing the KV
projection across multiple heads. This leads to a linear decrease in
memory consumption as the number of KV heads decreases.</p>
<p><strong>Sliding window attention</strong> limits the KV cache size to
the window size (e.g., 4096 for Llama7B), further controlling memory
usage.</p></li>
</ul>
<p><strong>Performance-Motivated Architectural Changes</strong></p>
<ul>
<li><p><strong>Impact of Model Width and Depth:</strong></p>
<p>Increasing the <strong>number of layers</strong> linearly scales both
the FLOPS and the number of parameters.</p>
<p>Increasing the <strong>model width</strong> (d_model) quadratically
scales the number of parameters and, consequently, the compute
requirements.</p>
<p>This is because weight matrices within layers have a size of
(d_model, d_model), leading to a quadratic relationship between model
width and parameters.</p></li>
<li><p><strong>Balancing Latency and Parallelization:</strong></p>
<p>Wider models parallelize better due to the ease of splitting layers
across multiple GPUs using tensor parallelism.</p>
<p>Deeper models require sequential computation of layers, hindering
parallelization, especially during training.</p>
<p>Therefore, <strong>wider models are preferred when low latency is
critical.</strong></p></li>
</ul>
<p><strong>Empirical Analysis of LLM Performance</strong></p>
<p>The article investigates the memory usage of the KV cache in LLMs,
specifically Llama2. The author observes that <strong>the actual memory
consumed by the model is higher than what the theoretical calculations
suggest</strong>. Here's how the discrepancy is highlighted:</p>
<ul>
<li><strong>Theoretical Calculation:</strong> The formula for
calculating the KV cache memory requirement per token is
<code>4 * number of layers * d_model</code> bytes. In the experiment,
the Llama2 model used has <code>d_model</code> of 1024 and 8 hidden
layers. This means it theoretically needs 32KB of memory per token (4 *
8 * 1024 bytes = 32KB).</li>
<li><strong>Expected Memory Usage:</strong> For generating 20 tokens,
the model should ideally use 640KB of memory (32KB/token * 20 tokens =
640KB).</li>
<li><strong>Observed Memory Usage:</strong> However, the empirical
analysis revealed that the model's memory consumption jumped by ~2.1MB
every 20 tokens. This is significantly higher than the expected
640KB.</li>
</ul>
<p>The author concludes that <strong>this discrepancy of about 3x
suggests an inefficient implementation of the KV cache in the model
being used</strong>. The extra overhead could stem from various factors
not accounted for in the theoretical calculation, and further
investigation would be needed to pinpoint the exact cause.</p>
<blockquote>
<p><strong>Transformer inference tricks - Artificial
Fintelligence</strong> [<a
target="_blank" rel="noopener" href="https://www.artfintel.com/p/transformer-inference-tricks">Link</a>]</p>
</blockquote>
<p><strong>KV Cache</strong></p>
<ul>
<li>The KV cache is a crucial optimization for decoder models,
significantly reducing computation. It exploits the fact that keys and
values remain constant for the prompt and each decoded token in
subsequent iterations. By caching these values, the computational
complexity of sampling becomes linear instead of quadratic, enabling
decent performance with longer contexts.</li>
<li>However, it introduces state management complexity, as inference
needs to continue for all sequences even if some are completed. The KV
cache demands significant memory, proportional to the number of layers,
heads, and the embedding dimension. For instance, GPT-3 with 96 layers,
96 heads, and a dimension of 128 requires 2.4M parameters per token,
translating to 10GB of memory for a 2048 token context window. This
memory requirement is a major challenge for consumer-grade GPUs with
limited HBM, like the 4090.</li>
</ul>
<p><strong>Speculative Decoding</strong></p>
<ul>
<li>Speculative decoding leverages excess compute capacity, particularly
in local inference settings. It utilizes two models: a small, fast
“draft” model and a large, slow model. The smaller model quickly makes
multiple inferences, guessing the large model’s predictions, while the
larger model verifies these guesses in parallel. This effectively
reduces the sequential cost of generating a sequence to that of the
smaller model.</li>
<li>However, it requires training and storing both models, and
performance is limited by the smaller model’s prediction accuracy.
HuggingFace reports a typical doubling of decoding rate using this
technique.</li>
<li>Newer techniques like <strong>Jacobi decoding</strong> and
<strong>lookahead decoding</strong> aim to improve upon speculative
decoding by generating n-grams and recursively matching them,
potentially achieving latency improvements without requiring a draft
model.</li>
</ul>
<p><strong>Effective Sparsity</strong></p>
<ul>
<li><strong>Sparsity</strong> in transformer activations arises from the
softmax operation in the attention mechanism and ReLU activations in
MLPs, leading to many zero values. Utilizing this sparsity can be
challenging, with limited support in mainstream tensor programs.</li>
<li>One optimization involves skipping computations for zero
activations, feasible in custom implementations like Llama.cpp. However,
the effectiveness of this approach diminishes exponentially with batch
size due to the random distribution of sparsity across tokens.</li>
<li>Therefore, leveraging sparsity is most effective for batch size 1,
although speculative decoding might be more beneficial in such
scenarios.</li>
</ul>
<p><strong>Quantization</strong></p>
<ul>
<li><strong>Quantization</strong> reduces the precision of model weights
and activations, potentially saving memory and increasing inference
speed. Research suggests that quantization to 4 bits or more results in
negligible performance degradation. The <em>k</em>-bit inference scaling
laws paper demonstrates that reducing precision allows for using a
larger model with the same memory footprint and potentially achieving
better performance.</li>
<li>However, using lower precision formats may lack native support in
hardware and could be unstable in production environments.
<strong>FP8</strong> offers a good balance between performance and
support, being the lowest precision format natively supported by modern
accelerators. <strong>Int8</strong> is another option, easier to
implement with tools like PyTorch, though it lacks the performance
advantages of FP8.</li>
<li>Libraries like <strong>bitsandbytes</strong> facilitate
quantization, offering tools and APIs for implementation.</li>
</ul>
<blockquote>
<p><strong>Top 10 China's AI Stories in 2024: A Year-End Review - Recode
China AI</strong> [<a
target="_blank" rel="noopener" href="https://recodechinaai.substack.com/p/top-10-chinas-ai-stories-in-2024">Link</a>]</p>
</blockquote>
<p>China's AI landscape is rapidly catching up to the US, with multiple
models now reaching similar performance benchmarks as GPT-4 and
advancements in areas like video generation, robotics, and autonomous
driving.</p>
<p>Several AI-powered apps have emerged in China, with ByteDance's
<strong>Doubao</strong> leading in popularity domestically and MiniMax's
<strong>Talkie</strong> gaining traction internationally, though China
has yet to produce a "killer app" with at least 100 million daily active
users.</p>
<p>A number of Chinese AI startups have emerged since ChatGPT's debut,
backed by significant capital, but they now face strong competition from
tech giants.</p>
<p>Chinese open-source LLMs have made substantial global progress, with
Alibaba’s <strong>Qwen</strong> series being the most downloaded on
Hugging Face.</p>
<p>Chinese AI video generators have surged ahead due to the delayed
release of Sora, with platforms like Kuaishou’s <strong>Kling</strong>
and MiniMax’s <strong>Hailuo</strong> offering competitive features.</p>
<p>An LLM API price war has been ignited by major Chinese tech
companies, with significant price reductions for developers and
SMEs.</p>
<p>China's semiconductor industry faces challenges due to US
restrictions but is also making strides in self-sufficiency, with
companies like <strong>Huawei</strong> pushing forward on competitive AI
chips.</p>
<p>China's robotaxi industry is gaining momentum, with Baidu's
<strong>Apollo Go</strong> expanding its fleet and other self-driving
startups completing IPOs.</p>
<p>OpenAI and Microsoft have tightened AI access in China, prompting
Chinese AI companies to offer alternatives and accelerating the
development of homegrown models.</p>
<p>China is seeing a <strong>robotics</strong> boom with rapid
innovation in humanoid and other types of robots, though challenges
remain in complex tasks and high production costs.</p>
<p><strong>AI resurrection</strong> is becoming increasingly accessible,
raising ethical and legal questions as companies offer services to
create digital replicas of the deceased.</p>
<blockquote>
<p><strong>Finetuning LLM Judges for Evaluation - Deep (Learning)
Focus</strong> [<a
target="_blank" rel="noopener" href="https://cameronrwolfe.substack.com/p/finetuned-judge">Link</a>]</p>
</blockquote>
<p>This article discusses the challenges of evaluating LLMs and how
finetuning specialized LLM judges can improve the evaluation process.
Here's how the logic of the article flows:</p>
<p>The article notes that while human evaluation is the most reliable
method, it is also expensive, time-consuming, and not scalable. This
creates a need for efficient ways to test LLM capabilities.</p>
<p>There are two primary evaluation approaches: <strong>human evaluation
and automatic metrics</strong>.</p>
<ul>
<li><strong>Human evaluation</strong> is considered the "definitive
source of truth" but is recognized as noisy, subjective and prone to
bias.</li>
<li><strong>Automatic metrics</strong> are used to speed up model
development, but they are imperfect proxies for human opinions. The
article further divides automatic metrics into two categories:
traditional metrics and model-based evaluation.</li>
<li><strong>Traditional metrics</strong> like ROUGE and BLEU are
reference-based, comparing LLM outputs to "golden" answers, and are less
effective for modern LLMs which are open-ended and can produce many
valid responses.</li>
<li><strong>LLM-as-a-Judge</strong> is introduced as a model-based
approach, using a powerful LLM to evaluate another LLM's output. This
method is effective, easy to implement, and can handle open-ended
tasks.</li>
</ul>
<p>While effective, LLM-as-a-Judge has limitations, including a lack of
transparency, security concerns, cost, and a lack of specialization for
domain-specific evaluations. The article argues that these limitations
can be addressed by <strong>training specialized LLM
judges</strong>.</p>
<ul>
<li><strong>Meta-evaluation</strong> involves assessing the performance
of the LLM judge by comparing its output to high-quality human
evaluation data.</li>
<li>Early research on finetuned LLM judges were created as open source
replacements for proprietary LLMs. The original LLM-as-a-Judge paper
also explored finetuning, and found that a finetuned Vicuna-13B model
showed potential. The need for finetuning is further justified because
proprietary LLMs can be expensive, lack control or transparency, and
because open source models are becoming more capable. The article
discusses how a Vicuna-13B model was improved by finetuning on human
votes from Chatbot Arena, though it still fell short of GPT-4
performance.</li>
<li>Several examples of finetuned LLM judges:
<ul>
<li><strong>PandaLM</strong>: This model is designed to identify the
best model among a set, particularly useful for hyperparameter tuning.
It is trained on a dataset of over 300K examples with instructions,
inputs, paired responses, evaluation results and rationales. PandaLM is
effective in specialized domains like law and biology.</li>
<li><strong>JudgeLM</strong>: This model focuses on the factors that
contribute most to the quality of a judge model, such as data quality
and diversity, base model size, bias, and generalization. JudgeLM uses a
high-quality, diverse dataset and is trained to mitigate bias, including
positional, knowledge, and format biases.</li>
<li><strong>Auto-J</strong>: This model is designed for domain-specific
grading, with an emphasis on providing high-quality, structured
explanations. It is trained on real-world queries and responses and can
perform both pairwise and direct assessment scoring.</li>
</ul></li>
<li>Other related research using LLMs for critiques, verification, and
generating synthetic training data.</li>
</ul>
<p><strong>Prometheus</strong> is a key development in finetuned LLM
judges, capable of fine-grained, domain-specific evaluation. It is
trained to ingest custom scoring rubrics as input.</p>
<ul>
<li>The <strong>Prometheus</strong> model uses the Feedback Collection
dataset, which includes instructions, responses, rubrics, reference
answers, rationales, and scores. It is trained to sequentially provide
feedback and then score the response using a supervised finetuning (SFT)
strategy.</li>
<li><strong>Prometheus 2</strong> is introduced as an extension that can
handle both direct assessment and pairwise scoring. It is trained on
both the Feedback Collection and the Preference Collection, and uses a
linear model merging approach to combine models trained for the two
scoring formats.</li>
<li><strong>Prometheus-Vision</strong> extends the Prometheus concept to
Vision-Language Models (VLMs). It uses a dataset called the Perception
Collection, which includes images, instructions, responses, rubrics and
reference answers.</li>
</ul>
<p>Other types of finetuned judges, including:</p>
<ul>
<li><strong>Self-rewarding LLMs</strong>, which use the LLM itself to
provide its own rewards and feedback.</li>
<li><strong>LLM-as-a-Meta-Judge</strong>, which allows the LLM judge to
self-improve.</li>
<li><strong>Self-taught evaluators</strong>, which train evaluators
without human preference data.</li>
<li><strong>Foundational Large Autorater Models (FLAMe)</strong>, which
are trained on a massive amount of human preference data and generalize
well to other tasks.</li>
<li><strong>Direct judgement preference optimization</strong>, which
uses preference optimization to create more advanced evaluation
capabilities.</li>
</ul>
<p>A generic framework based on the Prometheus model for creating a
finetuned LLM judge. The steps include:</p>
<ul>
<li>Solidifying evaluation criteria.</li>
<li>Preparing a high-quality dataset.</li>
<li>Using synthetic data.</li>
<li>Focusing on the rationales for each score.</li>
<li>Training the model using SFT and meta-evaluating its
performance.</li>
</ul>
<blockquote>
<p><strong>E-Commerce Unleashed - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/e-commerce-unleashed">link</a>]</p>
</blockquote>
<p>Highlights discussion points:</p>
<ol type="1">
<li><p><strong>Cyber week trends.</strong></p>
<p>"Cyber Week (from Black Friday to Cyber Monday) showcased shifting
consumer behaviors and the growing dominance of e-commerce."</p></li>
<li><p><strong>Shopify’s acceleration.</strong></p>
<p>Shopify has evolved from a platform for small businesses into a
global enabler for merchants, offering tools to scale internationally.
Its emphasis on payments, particularly through Shop Pay, has been
pivotal, with Shop Pay emerging as a high-conversion checkout option. In
Q3, Gross Payment Volume accounted for 62% of Shopify’s Gross
Merchandise Volume, marking a 4% year-over-year increase. Additionally,
Shopify's partnership with Amazon to integrate Prime benefits directly
into Shopify stores represents a strategic move to boost customer
loyalty and conversions by leveraging Amazon's trusted fulfillment
network and extensive Prime membership base.</p></li>
<li><p><strong>Amazon takes on Temu.</strong></p>
<p>Amazon has launched <strong>Amazon Haul</strong>, a new storefront
aimed at attracting budget-conscious shoppers and safeguarding its
market position. This initiative is strategically designed to meet the
increasing demand for affordable e-commerce solutions.</p></li>
<li><p><strong>Walmart’s advertising play.</strong></p>
<p>Walmart is redefining modern retail by merging its extensive physical
presence with advanced digital capabilities to create a powerful
<strong>omnichannel strategy</strong>. The company leverages first-party
data and its retail media network to maintain a competitive edge.</p>
<p><strong>Walmart Connect</strong> integrates online and in-store
advertising, allowing brands to engage customers at their preferred
shopping points. By utilizing vast first-party data, Walmart delivers
targeted and relevant ads, enhancing both advertiser returns and
customer satisfaction. The platform is also attracting advertisers from
diverse industries, including automotive and financial services.</p>
<p>Walmart’s planned acquisition of <strong>Vizio</strong> marks its
entry into <strong>connected TV advertising</strong>, broadening Walmart
Connect’s reach into households through smart TVs and enhancing
inventory visibility and supply chain integration through improved data
capabilities. This positions Walmart as a leader in omnichannel retail
and advertising.</p></li>
<li><p><strong>AI: The quiet game changer.</strong></p>
<p>AI played a transformative role during Cyber Week, enhancing the
shopping experience across various dimensions.
<strong>Hyper-personalized shopping</strong> was driven by AI
recommendation engines, which anticipated consumer needs and boosted
conversions, exemplified by features like Amazon’s “frequently bought
together.” Generative AI tools, such as chatbots, simplified
<strong>product discovery</strong> during the busy sales period, with
innovations like <strong>Amazon Q</strong> offering AI-generated review
summaries to streamline decision-making.</p>
<p>AI also optimized logistics through <strong>demand
forecasting</strong>, ensuring products remained in stock and reducing
shipping delays. In payments, <strong>real-time AI fraud
detection</strong> provided secure checkouts on platforms like Walmart
and Shopify. Additionally, AI tools like <strong>Shopify’s Sidekick and
Magic</strong> enhanced product descriptions, SEO strategies, and
customer support, further elevating the e-commerce experience. These
advancements underscored AI's critical role in reshaping retail during
one of the busiest shopping weeks of the year.</p></li>
</ol>
<blockquote>
<p><em>AI presents new challenges for incumbents but also drives
significant innovation and growth.</em></p>
<p><strong>― Salesforce: The Agent Wave - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/salesforce-the-agent-wave">Link</a>]</p>
</blockquote>
<p>The company’s autonomous AI platform - Agentforce - was introduced in
Sep 2024 and launched in late Oct. Agentforce enables businesses to
deploy AI agents for tasks such as sales, marketing, and customer
support. This marks a pivotal step in Salesforce’s platform strategy,
with far-reaching implications. CEO Marc Benioff views Agentforce as
transformative, positioning it at the core of a shift toward
“agent-first companies.” In this model, AI not only assists humans but
fundamentally redefines business operations by automating processes and
enhancing productivity.</p>
<figure>
<img src="/di-blog/2024/12/01/2024-December/agentforce.jpeg"
alt="agentforce" />
<figcaption aria-hidden="true">agentforce</figcaption>
</figure>
<p>What to watch:</p>
<ul>
<li>Salesforce recently completed its acquisition of
<strong>Own</strong> and <strong>Zoomin</strong>, reinforcing its Data
Cloud capabilities.</li>
<li><strong>Salesforce Ventures</strong> announced a new <span
class="math inline">\(\$500\)</span> million AI fund, targeting
high-profile AI startups like Anthropic, Mistral, and Cohere, supporting
Salesforce’s efforts to remain at the forefront of enterprise AI.</li>
<li>Clara Shih, <strong>CEO of Salesforce AI</strong> left Salesforce to
set up a new <strong>Business AI</strong> group at Meta, aiming to build
AI tools for businesses of all sizes. Shih’s departure highlights the
intensity of the <strong>AI talent war</strong>, which will be a
fascinating layer to watch in the coming year.</li>
</ul>
<blockquote>
<p><strong>OpenAI's o1 using "search" was a PSYOP -
Interconnects</strong> [<a
target="_blank" rel="noopener" href="https://www.interconnects.ai/p/openais-o1-using-search-was-a-psyop">Link</a>]</p>
</blockquote>
<p>The article primarily argues that OpenAI's o1 model does not use
explicit search at test time, and its apparent search capabilities are a
result of reinforcement learning (RL) during training. The author argues
against the idea that o1 uses online search at test time or intermediate
rewards during training. The article posits that the "suspects" are
reduced to "Guess + Check" and "Learning to Correct". They uses the
<strong>test-time compute plot</strong>, and the training process, as
key points in their argument to show how o1 can achieve high performance
using RL with controlled training data and no explicit search during
inference.</p>
<p>One major source of this idea is <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=6PEJ96k1kiw&amp;ab_channel=SashaRush%F0%9F%A4%97">Sasha
Rush's lecture</a> on Test Time Scaling (o1).</p>
<blockquote>
<p><strong>Insurance companies aren't the main villain of the U.S.
health system - Nahpinion</strong> [<a
target="_blank" rel="noopener" href="https://www.noahpinion.blog/p/insurance-companies-arent-the-main">Link</a>]</p>
</blockquote>
<p>This article argues that health insurance companies are not the
primary cause of high healthcare costs in the United States12. Instead,
the article argues that the excessive prices charged by healthcare
providers are the main reason for the high cost of healthcare. The
article suggests that focusing anger on insurance companies is "shooting
the messenger," and the solution is to reduce costs within the medical
system itself, such as having the government negotiate lower prices with
providers.</p>
<p>Evidences are: insurance companies have low profit margins, spend
much more on medical costs than they make in profit. Americans pay a
smaller percentage of their health costs out of pocket than people in
most other rich countries. This suggests that US health insurers are
paying a higher percentage of costs than government insurance systems in
other countries. The cost of healthcare provision in the U.S. is too
high. The actual people charging high prices are the providers
themselves, such as hospitals, pharmaceutical companies, and system.
They outsource the actual collection of these fees to insurance
companies.</p>
<blockquote>
<p><strong>15 Times to use AI, and 5 Not to - One Useful Thing</strong>
[<a
target="_blank" rel="noopener" href="https://www.oneusefulthing.org/p/15-times-to-use-ai-and-5-not-to">Link</a>]</p>
</blockquote>
<p><strong>When to Use AI:</strong></p>
<p>Use AI for tasks that require generating a high quantity of ideas,
such as in brainstorming sessions.</p>
<p>AI is useful when you are an expert and can quickly judge the quality
of its output.</p>
<p>AI can summarize large amounts of information where minor errors are
acceptable.</p>
<p>Use AI for translating information between different formats or
audiences.</p>
<p>AI can help you overcome creative blocks by providing multiple
options to move forward.</p>
<p>Use AI when it is known to be better than any available human option,
and its errors won't cause significant problems.</p>
<p><strong>Use AI as a companion when reading to get help with context
and details.</strong> <strong>(very helpful to me)</strong></p>
<p>AI can provide a variety of solutions, allowing you to curate the
best ones.</p>
<p>AI is helpful for tasks where research has proven it to be effective,
like coding.</p>
<p>Use AI to get a first look at how different audiences might react to
your work.</p>
<p>AI can act as a competent co-founder for entrepreneurial
ventures.</p>
<p>Use AI to get a specific perspective, such as reactions from
fictional personas.</p>
<p>AI can help with tasks that are ritualistic and have lost their
purpose.</p>
<p>Use AI to get a second opinion by comparing its conclusions with
yours.</p>
<p>Use AI when it can perform a task better than humans.</p>
<p><strong>When Not to Use AI:</strong></p>
<p>Avoid AI when you need to learn and synthesize new ideas, as it is
not the same as reading and thinking yourself.</p>
<p>Do not use AI when very high accuracy is essential because AI errors
can be very plausible and hard to spot.</p>
<p>Avoid AI if you do not understand its failure modes, such as
hallucinations or persuasiveness.</p>
<p>Do not use AI when the struggle with a topic is necessary for success
and learning.</p>
<p>Avoid AI when it is bad at a specific task.</p>
<blockquote>
<p><strong>Oracle : The 4th Hyperscaler? - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/oracle-the-4th-hyperscaler">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Google released the first version of its Gemini 2.0 family of
artificial intelligence models on December 11th, 2024. Including its
Chrome browser automation product called <a
target="_blank" rel="noopener" href="https://deepmind.google/technologies/project-mariner/">Mariner</a>.</em></p>
<p><em><a
target="_blank" rel="noopener" href="https://deepmind.google/technologies/project-astra/">Project
Astra</a> and Mariner along with <a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/how-to-use-notebooklm-for-personalized">NotebookLM</a>
remain very intriguing AI products by Google in 2025.</em></p>
<p><strong>Gemini 2 and the rise of multi-modal AI - AI
Supremacy</strong> [<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/gemini-2-and-the-rise-of-multi-modal">Link</a>]</p>
</blockquote>
<p>Incredible.</p>
<figure>
<img
src="/di-blog/2024/12/01/2024-December/last_6_month_llm_google_openai.png"
alt="last_6_month_llm_google_openai" />
<figcaption
aria-hidden="true">last_6_month_llm_google_openai</figcaption>
</figure>
<p>Figure source: <a
target="_blank" rel="noopener" href="https://www.linkedin.com/posts/peter-gostev_i-havent-quite-realised-that-this-was-the-activity-7272750515300536320-UqUu/?utm_source=share&amp;utm_medium=member_android">Peter
Gostev on Linkedin</a></p>
<blockquote>
<p><strong>Palantir Unclassified! Equity Research! - Global Equity
Briefing</strong> [<a
target="_blank" rel="noopener" href="https://www.globalequitybriefing.com/p/palantir-war-as-a-service-and-data">Link</a>]</p>
</blockquote>
<p>Palantir is a software company that provides tools for analyzing
large datasets, which enable users to make better decisions. Founded in
the early 2000s, Palantir initially offered services to government
agencies, including the US intelligence community, to combat terrorism.
The CIA was one of their first investors. Palantir's software is also
used by corporations to improve operations and decision-making.</p>
<p><strong>Business Model</strong></p>
<p>Palantir operates as a <strong>Software as a Service (SaaS)
company</strong>, offering a suite of customizable products for which
clients pay a licensing fee. The company has two operating segments:
<strong>government</strong> and <strong>commercial</strong>.</p>
<p><strong>Government Sales:</strong> Palantir provides services to
government institutions, recognizing a gap in the market due to many
Silicon Valley companies not wanting to work with governments. These
contracts are often long-term, providing predictable revenue streams.
The company benefits from the transparency of government information,
and it is easier for them to predict needs and market their
software.</p>
<p><strong>Commercial Sales</strong>: Palantir's solutions are used
across many industries by various employees from production line workers
to CEOs. The use cases for Palantir software in the commercial sector
are extensive.</p>
<p><strong>Customer Acquisition:</strong> Palantir targets large
organizations with complex problems, which increases their competitive
advantage. Solving difficult problems first earns customer trust.</p>
<p><strong>Products</strong>: Gotham, Foundry, Apollo, and AIP.</p>
<ul>
<li><strong>Gotham:</strong> It is a <strong>government-focused
platform</strong> that allows users to analyze large datasets to make
better decisions and find hidden connections, with the goal of improving
operations and decision-making.</li>
<li><strong>Foundry:</strong> This is a <strong>commercial
platform</strong> that allows large and complex companies to integrate,
visualize, and analyze their data to optimize their operations and value
chain.</li>
<li><strong>Apollo:</strong> This is a platform for <strong>continuous
software deployment</strong>, enabling secure and seamless delivery of
software across various environments for Palantir's clients.</li>
<li><strong>AIP:</strong> Palantir's newest offering, it is a platform
for organizations to <strong>create customized AI tools</strong> using
their own data, providing accurate and detailed answers to specific
questions.</li>
</ul>
<p><strong>Opportunities</strong></p>
<p>Palantir can benefit from the growing demand for <strong>digital
twins</strong>, which are exact digital replicas of real-world items
used for integration, monitoring, simulation, and maintenance. The
digital twin market is projected to grow significantly. Palantir is
positioned to benefit from the <strong>AI revolution</strong> with its
<strong>AIP platform</strong>, and its other products also use AI. The
global AI market is expected to reach <span
class="math inline">\(\$1.84\)</span> trillion by 2030. Palantir is
developing <strong>industry-specific operating systems</strong>, like
Skywise for the airline industry. These operating systems are sticky and
offer significant revenue opportunities. The healthcare industry could
be a large market for such systems. Palantir's commercial sector is
growing, and there are significant opportunities for international
expansion.</p>
<blockquote>
<p><strong>Is AI hitting a wall? - Strange Loop Canon</strong> [<a
target="_blank" rel="noopener" href="https://www.strangeloopcanon.com/p/is-ai-hitting-a-wall">Link</a>]</p>
</blockquote>
<p>Arguments that suggest AI progress is hitting a wall include the
observation that <strong>pre-training scaling has plateaued</strong>,
meaning simply increasing model size and data may not yield the same
improvements as before. Also, current evaluation <strong>benchmarks may
be saturated</strong>, failing to assess deeper work, since they are
based on human tests or simple recall. Current AI models
<strong>struggle with real-world tasks</strong> due to issues like
hallucination and a lack of creative planning, even if they appear
human-level in individual evaluations. Finally, the visible effects of
scaling are limited, with reduced cross-entropy loss not translating to
significant improvements for observers.</p>
<p>Conversely, arguments against AI progress hitting a wall emphasize
the presence of <strong>large amounts of unused data</strong>, including
various types like conversations and video data. The use of
<strong>synthetic data</strong> can enhance learning by converting
existing data into different formats and testing it against real-world
scenarios. AI models are now being taught <strong>reasoning</strong>,
enabling them to "think for longer" and improving performance in areas
requiring clear thought processes. Additionally, there is the
possibility of exploring <strong>new S-curves</strong> or scaling laws.
New models are also capable of <strong>expert-level work</strong> that
is not captured by current benchmarks, potentially speeding up
scientific research. Finally, AI models can now <strong>interact with
digital systems</strong>, and are becoming more aware of the world.</p>
<blockquote>
<p><strong>Our Healthcare System, a Reign of Terror - Freddie
deBoer</strong> [<a
target="_blank" rel="noopener" href="https://freddiedeboer.substack.com/p/our-healthcare-system-a-reign-of">Link</a>]</p>
<p><strong>An Assassin Showed Just How Angry America Really Is - BIG by
Matt Stoller</strong> [<a
target="_blank" rel="noopener" href="https://www.thebignewsletter.com/p/an-assassin-showed-just-how-angry">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>OpenAI o3 Model Is a Message From the Future: Update All You
Think You Know About AI - The Algorithmic Bridge</strong> [<a
target="_blank" rel="noopener" href="https://www.thealgorithmicbridge.com/p/openai-o3-model-is-a-message-from">Link</a>]</p>
<p><strong>OpenAI's o3: The grand finale of AI in 2024 -
Interconnects</strong> [<a
target="_blank" rel="noopener" href="https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai?r=333z5o&amp;utm_medium=ios&amp;triedRedirect=true">Link</a>]</p>
</blockquote>
<p>Key performance points:</p>
<ul>
<li><strong>ARC AGI Prize:</strong> o3 is the first model to surpass the
85% threshold for completing the ARC AGI prize on the public set, though
it exceeded cost constraints. It achieved 87% accuracy on the public set
with high compute, and 76% with low compute. For context, prior to
o1-class models, OpenAI’s best model, GPT-4o, only achieved 5% accuracy.
The ARC AGI challenge is designed to evaluate human-like general fluid
intelligence.</li>
<li><strong>Frontier Math Benchmark:</strong> o3 demonstrates a
substantial improvement on the Frontier Math benchmark, increasing
performance from 2% to 25%. This benchmark is considered extremely
challenging, with one Fields Medalist stating that the problems "will
resist AIs for several years at least".</li>
<li><strong>Coding Benchmarks:</strong> o3 has made significant
improvements on leading coding benchmarks such as SWE-Bench-Verified,
achieving a score of 71.7%. On the Codeforces competition coding site,
o3 achieved a score of 2727 with consensus voting, placing it at the
International Grandmaster level and approximately in the top 200 of
competitive human coders.</li>
<li><strong>Reasoning Capabilities:</strong> o3 represents a major
advancement in reasoning evaluations, signaling that the industry is
moving beyond pretraining on internet text. It is expected to accelerate
the rate of progress in AI research.</li>
<li><strong>Inference and Cost</strong>: o3 was tested with two levels
of compute with different sample sizes: a high-efficiency configuration
with a sample size of 6, and a low-efficiency configuration with a
sample size of 1024 which used 172 times more compute. The cost of
running o3 at the higher level of compute was approximately <span
class="math inline">\(\$5000\)</span> per query. It is speculated that
the core mechanism of o3 involves natural language program search and
execution within token space, searching over Chains of Thought
(CoTs).</li>
<li><strong>Availability</strong>: The o3 model, including the o3-mini
version, is expected to be available to the general public in late
January 2025. The o3-mini is expected to be more impactful for the
general public due to its lower cost, while still outperforming o1.</li>
</ul>
<blockquote>
<p><strong>o3, AGI, the art of the demo, and what you can expect in 2025
- Marcus on AI</strong> [<a
target="_blank" rel="noopener" href="https://garymarcus.substack.com/p/o3-agi-the-art-of-the-demo-and-what">Link</a>]</p>
<p><strong>o3 “ARC AGI” postmortem megathread: why things got heated,
what went wrong, and what it all means - Marcus on AI</strong> [<a
target="_blank" rel="noopener" href="https://garymarcus.substack.com/p/c39">Link</a>]</p>
</blockquote>
<p>Gary Marcus critiques OpenAI's new model o3, arguing that its
impressive demo, while showcasing advancements in math and coding, was
carefully curated and lacks broader application.</p>
<ul>
<li>The public did not get to try the system, and it was not vetted by
the scientific community. OpenAI chose what to highlight about o3.
Marcus argues that until many people get to try o3 on different tasks,
its reliability should not be assumed.</li>
<li>The o3 demo primarily focused on math, coding, and IQ-like puzzles,
with no evidence that it can work reliably in open-ended domains. It was
not tested on problems where massive data augmentation was not possible.
The demo did not address the most important question about the system's
capabilities in open-ended domains.</li>
<li>The o3 system is incredibly expensive. One estimate suggests that
each call to the system might cost $1000. Even if the cost is reduced,
it might still not be as good or as versatile as top STEM
graduates.</li>
<li>The o3's performance on the ARC-AGI test was misleading. The test is
at most a necessary, but not sufficient, condition for AGI, and does not
address important areas such as factuality, compositionality, and common
sense.</li>
<li>The core problem of neural networks generalizing better "within
distribution" than "outside distribution" has not been solved.</li>
</ul>
<blockquote>
<p><strong>Note to Our Energy Sucking Overlords - Michael
Spencer</strong> [<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/note-to-our-energy-sucking-overlords">Link</a>]</p>
</blockquote>
<p>The rapid growth of AI is causing a surge in demand for data centers,
which in turn are becoming major consumers of electricity. The energy
needs of AI are growing so large that tech companies are seeking
reliable power sources beyond renewable energy. The rising energy
consumption of AI infrastructure will likely result in higher energy
prices, potentially creating competition between Big Tech and the
communities where they build data centers. To meet their energy needs,
major technology companies are becoming more involved in the energy
sector, including investments in nuclear and natural gas plants. The
current trajectory of AI infrastructure expansion and energy consumption
is unsustainable and could lead to significant challenges for society.
The US is building data centers abroad in Europe and Asia, thereby
maintaining their power and also acquiring cheaper labor.</p>
<p>Summary of statistics:</p>
<ul>
<li><strong>Energy Consumption of AI tasks:</strong> A single task on
the ARC-AGI benchmark using OpenAI's o3 model consumes approximately
<strong>1,785 kWh</strong> of energy, which is equivalent to the
electricity used by an average U.S. household in two months. This task
also generates 684 kg CO₂e, which is equivalent to the carbon emissions
from more than 5 full tanks of gas.</li>
<li><strong>Investments in AI Infrastructure:</strong> In 2024, major
players like Amazon, Microsoft, and Alphabet spent over <span
class="math inline">\(\$240\)</span> billion on AI-related
infrastructure. In 2025, Amazon, Google, Meta, and Microsoft are
expected to spend <span class="math inline">\(\$300\)</span> billion in
capital expenditures.</li>
<li><strong>Data Center Electricity Consumption:</strong> Global data
center electricity consumption is expected to <strong>more than
double</strong> between 2023 and 2028. The IDC expects consumption to
reach <strong>857 Terawatt hours (TWh)</strong> in 2028.</li>
<li><strong>US Data Center Energy Usage:</strong> U.S. data centers
could use <strong>6.7 to 12%</strong> of all energy demand nationwide by
2028. In 2023, data centers used 4.4% of total US power consumption,
which is projected to rise to as high as 12% by 2028. This is a spike of
more than threefold in the next four years.</li>
<li><strong>Data Center Locations and Power</strong>:
<ul>
<li>Northern Virginia has over <strong>300</strong> data centers with
approximately <strong>3,945 megawatts</strong> of commissioned
power.</li>
<li>The Dallas region has <strong>150</strong> data centers.</li>
<li>Silicon Valley has over <strong>160</strong> data centers.</li>
<li>Phoenix has over <strong>100</strong> data centers with around
<strong>1,380 megawatts</strong> of power.</li>
<li>Chicago has more than <strong>110</strong> data centers.</li>
</ul></li>
<li><strong>Data Center Projects:</strong>
<ul>
<li>OpenAI plans to construct massive <strong>5-gigawatt (GW) data
centers</strong> across the US.</li>
<li>Oklo will build small modular reactors (SMR) by 2044 to generate 12
gigawatts of electricity for data centers.</li>
<li>Meta announced a <span class="math inline">\(\$10\)</span> billion
development for a <strong>4 million sq ft, 2 GW data center</strong>
campus in Louisiana.</li>
<li>Entergy is proposing to develop a <strong>1.5GW natural gas
plant</strong> in Louisiana to power a data center.</li>
<li>Amazon Web Services (AWS) plans to invest <span
class="math inline">\(\$11\)</span> billion in a new data center campus
in Northern Indiana.</li>
</ul></li>
<li><strong>Generative AI Market:</strong> The generative AI market was
valued at <span class="math inline">\(\$6\)</span> billion in 2023 and
could reach <span class="math inline">\(\$59\)</span> billion in
2028.</li>
<li><strong>Increased US power demand:</strong> Data centers are one of
the key reasons US power demand is expected to jump 16% over the next
five years.</li>
<li><strong>Cost of Electricity for Data Centers:</strong> Electricity
is the largest ongoing expense for data center operators, accounting for
<strong>46%</strong> of total spending for enterprise data centers and
<strong>60%</strong> for service provider data centers.</li>
<li><strong>The potential for data centers to consume as much energy as
entire industrialized economies:</strong> By 2030, US data centers could
consume as much electricity as some entire industrialized
economies.</li>
<li><strong>Big Oil's Role:</strong> Big oil companies like
<strong>ExxonMobil</strong> and <strong>Chevron</strong> are moving into
the AI datacenter energy market. Exxon plans to build a natural gas
plant to power a data center, and estimates that decarbonizing AI data
centers could represent up to 20% of its total addressable market for
carbon capture and storage by 2050.</li>
</ul>
<blockquote>
<p><strong>What are the checks and balances on the power of Elon Musk? -
Noahpinion</strong> [<a
target="_blank" rel="noopener" href="https://www.noahpinion.blog/p/what-are-the-checks-and-balances">Link</a>]</p>
</blockquote>
<p>The article examines the significant influence of Elon Musk on U.S.
politics, particularly his role in derailing a Congressional spending
bill. It explores whether Musk's actions represent a threat to
democratic processes, considering his control over X (formerly Twitter)
and SpaceX. The author presents contrasting views of Musk—"Real Elon"
versus "Evil Elon"—highlighting the uncertainty surrounding his motives
and the lack of institutional checks on his power. The piece concludes
by suggesting that public opinion ultimately holds sway over Musk's
influence, though the potential for a powerful backlash remains to be
seen.</p>
<blockquote>
<p><strong>Is AI progress slowing down? - AI SHAKE OIL</strong> [<a
target="_blank" rel="noopener" href="https://www.aisnakeoil.com/p/is-ai-progress-slowing-down">Link</a>]</p>
</blockquote>
<p>The authors argue that the recent shift away from model scaling
towards inference scaling is not necessarily indicative of a slowdown,
but rather a change in approach. They caution against over-reliance on
industry insiders' predictions due to their inherent biases, emphasizing
that progress is less predictable and more dependent on algorithmic
innovation than previously assumed. Furthermore, the essay highlights
the significant lag between capability advancements and real-world
applications, suggesting that the focus should shift towards product
development and user adoption rather than solely on model capabilities.
Finally, the authors offer a more nuanced perspective on the current
state of AI progress, acknowledging the potential of inference scaling
while emphasizing the importance of considering broader factors beyond
pure technological advancement.</p>
<blockquote>
<p><strong>The Critical AI Report, December 2024 Edition - Blood in the
Machine</strong> [<a
target="_blank" rel="noopener" href="https://www.bloodinthemachine.com/p/the-critical-ai-report-december-2024">Link</a>]</p>
</blockquote>
<p>Gen AI's actual impact on workers so far:</p>
<figure>
<img src="/di-blog/2024/12/01/2024-December/genai_impact_on_workers.png"
alt="genai_impact_on_workers" />
<figcaption aria-hidden="true">genai_impact_on_workers</figcaption>
</figure>
<blockquote>
<p><strong>Waymo: Rideshare Revolution - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/waymo-rideshare-revolution">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Manufacturing is a war now - Noahpinion</strong> [<a
target="_blank" rel="noopener" href="https://www.noahpinion.blog/p/manufacturing-is-a-war-now">Link</a>]</p>
</blockquote>
<p>The article argues that China's dominance in manufacturing,
particularly in crucial areas like drone production and batteries, poses
a significant threat to the United States and its allies.</p>
<figure>
<img
src="/di-blog/2024/12/01/2024-December/global_industrial_production.jpeg"
alt="global_industrial_production" />
<figcaption aria-hidden="true">global_industrial_production</figcaption>
</figure>
<p>Source:
https://mipforum.org/wp-content/uploads/2024/11/MIPF-Conference-Paper-FINAL-WEB.pdf</p>
<h3 id="articles-and-blogs">Articles and Blogs</h3>
<blockquote>
<p><strong>Meet Willow, our state-of-the-art quantum chip - Google
Research</strong> [<a
target="_blank" rel="noopener" href="https://blog.google/technology/research/google-willow-quantum-chip/">Link</a>]</p>
</blockquote>
<p>Google has developed a new quantum chip called Willow, which
significantly reduces errors as it scales up, a major breakthrough in
quantum error correction. Willow also performed a computation in under
five minutes that would take a supercomputer 10 septillion years,
demonstrating its potential for solving complex problems beyond the
reach of classical computers. This achievement marks a significant step
towards building commercially relevant quantum computers that can
revolutionize fields like medicine, energy, and AI.</p>
<p>Quantum Computing Roadmap:</p>
<figure>
<img
src="/di-blog/2024/12/01/2024-December/google_quantum_ai_roadmap.png"
alt="google_quantum_ai_roadmap" />
<figcaption aria-hidden="true">google_quantum_ai_roadmap</figcaption>
</figure>
<p>Terms to keep in mind:</p>
<ul>
<li><strong>Willow:</strong> Google's latest 105-qubit superconducting
processor, which is the first to demonstrate exponential error
suppression with increasing surface code size.</li>
<li><strong>Below Threshold:</strong> A milestone in quantum computing
where the error rate decreases as the number of qubits increases,
demonstrating effective error correction.</li>
<li><strong>Logical Qubit:</strong> A fault-tolerant qubit created from
multiple physical qubits using error correction techniques, providing a
more stable and reliable unit of computation.</li>
<li><strong>Random Circuit Sampling (RCS):</strong> A benchmark test
that assesses the ability of a quantum computer to perform computations
beyond the capabilities of classical computers.</li>
<li><strong>T1 Time:</strong> A measure of how long a qubit can maintain
its quantum state before decoherence sets in.</li>
<li><strong>Quantum Algorithms:</strong> Algorithms specifically
designed to be executed on quantum computers, leveraging quantum
phenomena to solve problems more efficiently.</li>
</ul>
<blockquote>
<p><strong>Making quantum error correction work - Google
Research</strong> [<a
target="_blank" rel="noopener" href="https://research.google/blog/making-quantum-error-correction-work/">Link</a>]</p>
</blockquote>
<p>The ultimate vision of them is to build a large-scale, fault-tolerant
quantum computer that can run complex quantum algorithms and unlock the
potential of quantum computing for scientific discovery and various
applications.</p>
<p>Terms to keep in mind:</p>
<ul>
<li><strong>Repetition codes:</strong> A type of quantum error
correction that focuses solely on bitflip errors and achieves lower
encoded error rates.</li>
<li><strong>Quantum error decoder:</strong> Classical software that
processes measurement information from the quantum computer to identify
and correct errors.</li>
</ul>
<blockquote>
<p><strong>AI Hallucinations: Why Large Language Models Make Things Up
(And How to Fix It) - kapa.ai</strong> [<a
target="_blank" rel="noopener" href="https://www.kapa.ai/blog/ai-hallucination">Link</a>]</p>
</blockquote>
<p>Why Do LLMs Hallucinate?</p>
<ul>
<li>LLMs predict upcoming words in a sequence based on patterns in
training data. They lack true reasoning or comprehension abilities, so
they rely only on these word probability patterns instead of genuine
understanding of the topics they discuss.</li>
<li>Architecture limitations: 1) fixed attention window in transformer
limits input context leading to earlier information being dropped, 2)
sequential token generation mechanism has no revision process, so
initial errors can compound to major inaccuracies in the output.</li>
<li>Limitations of probabilistic generation: 1) models can produce
plausible-sounding responses that lack actual comprehension of subjects,
2) value prompts lead LLMs to try to "fill in the blanks" resulting in
fabricated or inaccurate answers.</li>
<li>Training data gaps: 1) models are trained on ground-truth training
data while they do inference on their own, this can create a feedback
loop where minor errors become amplified, 2) when prompt falls outside
the scope of training data, the model will likely generate a
hallucinated response.</li>
</ul>
<p>How to Mitigate AI Hallucination?</p>
<ul>
<li>Input layer mitigation strategies
<ul>
<li>Query processing; context size optimization; context injection.</li>
</ul></li>
<li>Design layer mitigation strategies
<ul>
<li>Chain-of-Thought prompting; Retrieval-Augmented Generation (RAG);
Fine-tuning</li>
</ul></li>
<li>Output layer mitigation strategies
<ul>
<li>Rule-based filtering; output re-ranking; fact-checking and
verification; encourage contextual awareness.</li>
</ul></li>
</ul>
<figure>
<img src="/di-blog/2024/12/01/2024-December/mitigate_halluciation.png"
alt="mitigate_halluciation" />
<figcaption aria-hidden="true">mitigate_halluciation</figcaption>
</figure>
<blockquote>
<p><strong>The next chapter of the Gemini era for developers - Google
Blog</strong> [<a
target="_blank" rel="noopener" href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">Link</a>]</p>
</blockquote>
<p><a
target="_blank" rel="noopener" href="https://github.com/google-gemini/cookbook/tree/main/gemini-2">API
starter code</a>, <a target="_blank" rel="noopener" href="https://labs.google.com/code/">Code
Experiments (Data Science Agents, etc)</a>, <a
target="_blank" rel="noopener" href="https://aistudio.google.com/prompts/new_chat">Google AI
Studio</a></p>
<p>Gemini 2.0 Flash is an experimental AI model that builds upon the
success of Gemini 1.5 Flash. It offers enhanced capabilities for
developers to build immersive and interactive applications.</p>
<p><strong>Functionalities and Capabilities of Gemini 2.0
Flash:</strong></p>
<ul>
<li><p>Enhanced Performance: It is twice as fast as Gemini 1.5 Pro with
improved multimodal, text, code, video, spatial understanding, and
reasoning performance.</p></li>
<li><p>New Output Modalities:</p>
<p>Gemini 2.0 Flash allows developers to generate integrated responses,
including text, audio, and images, through a single API call. It
features native text-to-speech audio output with control over voice,
language, and accents. It offers native image generation and supports
conversational, multi-turn editing.</p></li>
<li><p><strong>Native Tool Use</strong>: Gemini 2.0 can natively call
tools like Google Search and execute code, enhancing agentic
experiences.</p></li>
<li><p><strong>Multimodal Live API</strong>: It enables the development
of real-time, multimodal applications with audio and video-streaming
inputs.</p></li>
</ul>
<p><strong>AI-powered Coding Agents in Gemini 2.0:</strong></p>
<ul>
<li><strong>Jules:</strong> An experimental AI-powered code agent that
utilizes Gemini 2.0 to handle Python and Javascript coding tasks. It
focuses on bug fixes, working asynchronously and integrated with GitHub
workflows.</li>
<li><strong>Colab's Data Science Agent:</strong> Utilizes Gemini 2.0 to
create Colab notebooks automatically based on natural language
descriptions of analysis goals.</li>
</ul>
<blockquote>
<p><strong>Introducing Phi-4: Microsoft’s Newest Small Language Model
Specializing in Complex Reasoning - Microsoft AI Platform Blog</strong>
[<a
target="_blank" rel="noopener" href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>a16z's big ideas in tech for 2025</strong></p>
</blockquote>
<p>Andreessen Horowitz published a new list of requests for startups to
build.</p>
<figure>
<img src="/di-blog/2024/12/01/2024-December/a16z_ideas_in_tech_2025.png"
alt="a16z_ideas_in_tech_2025" />
<figcaption aria-hidden="true">a16z_ideas_in_tech_2025</figcaption>
</figure>
<blockquote>
<p>(𝗦𝗲𝗹𝗳) 𝗠𝗮𝗻𝗮𝗴𝗲𝗺𝗲𝗻𝘁</p>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://blog.samaltman.com/how-to-be-successful">How to Be
Successful - Sam Altman</a> (blog)</li>
<li><a
target="_blank" rel="noopener" href="https://medium.com/swlh/returning-to-india-a-decision-framework-e685ce067304">Career
Algorithm - Hemant Mohapatra</a> (blog)</li>
<li><a target="_blank" rel="noopener" href="https://www.amazon.es/gp/product/0062047418/">What I Wish I
Knew at 20 - Tina Seelig</a> (book)</li>
<li><a target="_blank" rel="noopener" href="https://boz.com/articles/career-cold-start">Cold Start
Algorithm - Boz</a>(blog)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/1101875321/ref=ox_sc_act_title_1?smid=A29G165BTNNM2Z&amp;psc=1">Design
Your Life - Bill Burnett</a> (book)</li>
<li><a
target="_blank" rel="noopener" href="https://www.startupriders.com/p/spanish-startup-riders-4">Good PM,
Bad PM - Ben Horowitz</a> (blog)</li>
<li><a target="_blank" rel="noopener" href="https://www.startupriders.com/p/startup-riders-8">OKRs -
John Doerr</a> (blog)</li>
</ol>
<p>𝗟𝗲𝗮𝗱𝗲𝗿𝘀𝗵𝗶𝗽</p>
<ol type="1">
<li><a
target="_blank" rel="noopener" href="https://x.com/jevering/status/1558625847002640385?s=20&amp;t=eW1punE2RAzbaxvv_DmFLQ">Netscape
Aphorisms - Jim Barksdale</a> (twitter)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/What-You-Do-Who-Are-ebook/dp/B07NVN4QCM">What
You Do Is Who You Are - Horowitz</a> (book)</li>
<li><a
target="_blank" rel="noopener" href="https://review.firstround.com/give-away-your-legos-and-other-commandments-for-scaling-startups/">Giving
Away Legos - Molly Graham</a> (blog)</li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Extreme-Ownership-U-S-Navy-SEALs/dp/1250067057">Extreme
Ownership - Jocko Willink</a> (book)</li>
<li><a target="_blank" rel="noopener" href="https://paulgraham.com/foundermode.html">Founder Mode -
Paul Graham</a> (blog)</li>
</ol>
<p><strong>― Great startup leadership frameworks</strong> [<a
target="_blank" rel="noopener" href="https://www.linkedin.com/posts/ivanlandabaso_startups-founders-fundraising-activity-7266703345472471040-a8Cj/?utm_source=share&amp;utm_medium=member_ios">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>I think the biggest competitive advantage in business—either for
a company or for an individual’s career—is long-term thinking with a
broad view of how different systems in the world are going to come
together. One of the notable aspects of compound growth is that the
furthest out years are the most important. In a world where almost no
one takes a truly long-term view, the market richly rewards those who
do.</em></p>
<p><em>Most highly successful people have been really right about the
future at least once at a time when people thought they were wrong. If
not, they would have faced much more competition.</em></p>
<p><em>Thinking from first principles and trying to generate new ideas
is fun, and finding people to exchange them with is a great way to get
better at this. The next step is to find easy, fast ways to test these
ideas in the real world.</em></p>
<p><em>All great careers, to some degree, become sales jobs. You have to
evangelize your plans to customers, prospective employees, the press,
investors, etc. This requires an inspiring vision, strong communication
skills, some degree of charisma, and evidence of execution
ability.</em></p>
<p><em>It’s often easier to take risks early in your career; you don’t
have much to lose, and you potentially have a lot to gain.</em></p>
<p><em>Almost everyone I’ve ever met would be well-served by spending
more time thinking about what to focus on. It is much more important to
work on the right thing than it is to work many hours. Most people waste
most of their time on stuff that doesn’t matter.</em></p>
<p><em>You can get to about the 90th percentile in your field by working
either smart or hard, which is still a great accomplishment. But getting
to the 99th percentile requires both.</em></p>
<p><em>You have to figure out how to work hard without burning out. Work
stamina seems to be one of the biggest predictors of long-term
success.</em></p>
<p><em>If you are making progress on an important problem, you will have
a constant tailwind of people wanting to help you. Let yourself grow
more ambitious, and don’t be afraid to work on what you really want to
work on.</em></p>
<p><em>Follow your curiosity. Things that seem exciting to you will
often seem exciting to other people too.</em></p>
<p><em>People have an enormous capacity to make things happen. A
combination of self-doubt, giving up too early, and not pushing hard
enough prevents most people from ever reaching anywhere near their
potential.</em></p>
<p><em>The best way to become difficult to compete with is to build up
leverage. For example, you can do it with personal relationships, by
building a strong personal brand, or by getting good at the intersection
of multiple different fields.</em></p>
<p><em>An effective way to build a network is to help people as much as
you can.</em></p>
<p><em>One of the best ways to build a network is to develop a
reputation for really taking care of the people who work with
you.</em></p>
<p><em>Define yourself by your strengths, not your weaknesses.
Acknowledge your weaknesses and figure out how to work around them, but
don’t let them stop you from doing what you want to do.</em></p>
<p><em>Remember to spend your time with positive people who support your
ambitions.</em></p>
<p><em>You get truly rich by owning things that increase rapidly in
value. The best way to make things that increase rapidly in value is by
making things people want at scale.</em></p>
<p><em>Time only scales linearly.</em></p>
<p><em>Eventually, you will define your success by performing excellent
work in areas that are important to you. The sooner you can start off in
that direction, the further you will be able to go.</em></p>
<p><strong>― How to Be Successful - Sam Altman</strong> [<a
target="_blank" rel="noopener" href="https://blog.samaltman.com/how-to-be-successful">Link</a>]</p>
</blockquote>
<p>Great advice. I need to keep in mind.</p>
<ol type="1">
<li>Compound yourself</li>
<li>Have almost too much self-belief</li>
<li>Learn to think independently</li>
<li>Get good at “sales”</li>
<li>Make it easy to take risks</li>
<li>Focus</li>
<li>work hard</li>
<li>Be bold</li>
<li>Be willful</li>
<li>Be hard to compete with</li>
<li>Build a network</li>
<li>You get rich by owning things</li>
<li>Be internally driven</li>
</ol>
<blockquote>
<p><strong>Y Combinator: how to make the most out of your
20s</strong></p>
</blockquote>
<figure>
<img
src="/di-blog/2024/12/01/2024-December/yc_make_the_most_out_of_20s.png"
alt="yc_make_the_most_out_of_20s" />
<figcaption aria-hidden="true">yc_make_the_most_out_of_20s</figcaption>
</figure>
<blockquote>
<p><strong>Marc Andreessen's Guide to Personal Productivity</strong></p>
</blockquote>
<figure>
<img
src="/di-blog/2024/12/01/2024-December/marc_guide_personal_productivity.png"
alt="marc_guide_personal_productivity" />
<figcaption
aria-hidden="true">marc_guide_personal_productivity</figcaption>
</figure>
<blockquote>
<p><strong>Advancing red teaming with people and AI - Open AI</strong>
[<a
target="_blank" rel="noopener" href="https://openai.com/index/advancing-red-teaming-with-people-and-ai/">Link</a>]</p>
</blockquote>
<p>OpenAI's two new papers detail their advanced red teaming techniques
for assessing AI safety. <strong>External red teaming</strong> uses
human experts to probe AI models for vulnerabilities and risks, while
<strong>automated red teaming</strong> employs AI to generate diverse
attacks at scale. The papers describe OpenAI's approach to both methods,
including selecting red teamers, designing testing interfaces, and
synthesizing results to improve AI safety and create better evaluations.
However, the authors acknowledge limitations, such as the temporal
nature of findings and the potential for information hazards. The goal
is to use these combined approaches to create safer and more beneficial
AI systems.</p>
<blockquote>
<p><strong>Bringing Grok to Everyone - X.Ai</strong> [<a
target="_blank" rel="noopener" href="https://x.ai/blog/grok-1212">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Processing billions of events in real time at Twitter - X
Engineering</strong> [<a
target="_blank" rel="noopener" href="https://blog.x.com/engineering/en_us/topics/infrastructure/2021/processing-billions-of-events-in-real-time-at-twitter-">Link</a>]</p>
</blockquote>
<p><strong>Twitter's data infrastructure</strong> underwent a
significant upgrade, migrating from a lambda architecture to a kappa
architecture built on a hybrid of on-premise and Google Cloud Platform
systems. This <strong>new system processes 400 billion events
daily</strong>, improving real-time data accuracy and reducing latency.
The <strong>new architecture leverages Kafka, Dataflow, and
BigTable</strong>, achieving near-exactly-once processing and
significantly improved performance, as demonstrated by a system
performance comparison. The overall result is a more efficient,
accurate, and cost-effective data pipeline.</p>
<p>To handle this massive volume, Twitter's data infrastructure employs
a combination of tools and platforms:</p>
<ul>
<li><strong>Scalding</strong>: Used for batch processing</li>
<li><strong>Heron</strong>: Used for streaming data</li>
<li><strong>TimeSeries AggregatoR (TSAR)</strong>: An integrated
framework for both batch and real-time processing</li>
<li><strong>Data Access Layer</strong>: Enables data discovery and
consumption</li>
</ul>
<p>Twitter's <strong>interaction and engagement pipeline</strong>
processes high-scale data in batch and real time, collecting data from
various sources like real-time streams, server logs, and client logs.
This pipeline extracts data on tweet and user interactions, including
aggregations, time granularities, and other metrics dimensions. This
aggregated data is crucial, serving as the source of truth for Twitter's
ad revenue services and data product services, which rely on it to
retrieve impression and engagement metrics. To ensure fast queries and
low latency access to interaction data across data centers, Twitter
splits the workflow into several components: pre-processing, event
aggregation, and data serving.</p>
<blockquote>
<p><strong>The Transformer Architecture: A Visual Guide - Hendrik Erz,
M.A.</strong> [<a
target="_blank" rel="noopener" href="https://www.hendrik-erz.de/post/the-transformer-architecture-a-visual-guide-pdf-download">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>What is the Role of Mathematics in Modern Machine Learning? -
The Gradient</strong> [<a
target="_blank" rel="noopener" href="https://thegradient.pub/shape-symmetry-structure/">Link</a>]</p>
</blockquote>
<p>This article argues that while the emphasis has shifted from
mathematically principled architectures to large-scale empirical
approaches, mathematics remains crucial for post-hoc explanations of
model behavior and high-level design choices.</p>
<blockquote>
<p><strong>Introducing Gemini 2.0: our new AI model for the agentic era
- Google</strong> [<a
target="_blank" rel="noopener" href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">Link</a>]</p>
</blockquote>
<p>Project Astra is a research prototype exploring the future
capabilities of a universal AI assistant. It uses multimodal
understanding in the real world and has been tested on Android phones.
Key improvements of the latest version, built with Gemini 2.0, include
better dialogue, new tool use, better memory, improved latency.</p>
<p>Project Mariner is a research prototype that explores the future of
human-agent interaction, specifically within a browser. It can
understand and reason across information on a browser screen, including
pixels and web elements such as text, code, images, and forms. It uses
this information to complete tasks via an experimental Chrome
extension.</p>
<blockquote>
<p><strong>OpenAI o3 breakthrough high score on ARC-AGI-PUB - <a
target="_blank" rel="noopener" href="https://fchollet.com/">François Chollet</a></strong> [<a
target="_blank" rel="noopener" href="https://arcprize.org/blog/oai-o3-pub-breakthrough">Link</a>]</p>
</blockquote>
<figure>
<img src="/di-blog/2024/12/01/2024-December/openai_o3_comparison.png"
alt="openai_o3_comparison" />
<figcaption aria-hidden="true">openai_o3_comparison</figcaption>
</figure>
<blockquote>
<p><strong>Supercharging Training using float8 and FSDP2 - PyTorch
Blog</strong> [<a
target="_blank" rel="noopener" href="https://pytorch.org/blog/training-using-float8-fsdp2/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Zen ML LLMOps Database</strong> [<a
target="_blank" rel="noopener" href="https://www.zenml.io/llmops-database">Link</a>]</p>
</blockquote>
<p>Good collection.</p>
<h3 id="papers-and-reports">Papers and Reports</h3>
<blockquote>
<p><strong>Quantum error correction below the surface code
threshold</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.13687">Link</a>]</p>
</blockquote>
<p>This historic accomplishment shows that the more qubits they use in
Willow, the more they reduce errors, and the more quantum the system
becomes. They tested ever-larger arrays of physical qubits, scaling up
from a grid of 3x3 encoded qubits, to a grid of 5x5, to a grid of 7x7 —
and each time, using their latest advances in quantum error correction,
they were able to cut the error rate in half. In other words, they
achieved an exponential reduction in the error rate. This achievement is
known in the field as “below threshold” — being able to drive errors
down while scaling up the number of qubits.</p>
<blockquote>
<p><strong>Phi-4 Technical Report</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.08905?ref=maginative.com">Link</a>]</p>
</blockquote>
<p>Phi-4, a 14-billion-parameter language model from Microsoft Research,
emphasizes data quality by integrating synthetic data into its training
process. Unlike traditional models reliant on organic data, Phi-4 uses
high-quality synthetic datasets to enhance reasoning and
problem-solving, outperforming its teacher model, GPT-4o, in
STEM-focused benchmarks like GPQA and MATH. Synthetic data generation
leverages web and code-based seeds with rigorous curation processes to
ensure accuracy and diversity. Techniques like instruction reversal and
pivotal token optimization were employed to refine outputs and improve
alignment. Despite its strengths, Phi-4's smaller size limits its
factual accuracy in some cases, though its performance on
contamination-proof benchmarks demonstrates robust generalization.</p>
<blockquote>
<p><strong>Self-Harmonized Chain of Thought</strong> [<a
target="_blank" rel="noopener" href="https://www.arxiv.org/abs/2409.04057">Link</a>]</p>
</blockquote>
<p>The authors proposed Self Harmonized CoT (ECHO) method which employs
three main steps:</p>
<ol type="1">
<li>Clustering questions based on similarity.</li>
<li>Generating rationales for representative questions using
Zero-shot-CoT.</li>
<li>Iteratively refining rationales for consistency and alignment.</li>
</ol>
<p>ECHO’s unified rationales improve reasoning across varied tasks, but
its effectiveness varies with the complexity and nature of data. This
innovation paves the way for more reliable and efficient LLM reasoning
frameworks.</p>
<blockquote>
<p><strong>Best-of-N Jailbreaking</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.03556">Link</a>]</p>
</blockquote>
<p>A black-box algorithm designed to jailbreak frontier AI systems
across multiple modalities, including text, images, and audio. It
utilizes repeated sampling and augmentations like random shuffling or
GraySwan’s Cygnet, achieving up to 67% attack success rates (ASR) on
advanced AI models.</p>
<blockquote>
<p><strong>RAFT: Adapting Language Model to Domain Specific RAG</strong>
[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.10131">Link</a>]</p>
</blockquote>
<p>Retrieval-Augmented Fine-Tuning (RAFT) is a novel method designed to
improve the performance of LLMs in domain-specific open-book scenarios.
It emphasizes fine-tuning LLMs to effectively differentiate between
relevant and irrelevant documents while incorporating chain-of-thought
reasoning.</p>
<p>RAFT Methodology: it combines question, retrieved documents (relevant
and distractors), and chain-of-thought answers during training. Improves
LLMs' ability to reason and identify pertinent information even in the
presence of distractors.</p>
<blockquote>
<p><strong>MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented
Generation through Question Complexity</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.01572">Link</a>]</p>
</blockquote>
<p>The authors propose MBA-RAG, a reinforcement learning framework
leveraging a multi-armed bandit algorithm for adaptive RAG. Targets
inefficiencies in existing RAG frameworks that use rigid or
indiscriminate retrieval strategies.</p>
<p>The methodology: Treats retrieval methods as “arms” in a bandit
framework to dynamically select the optimal strategy based on query
complexity. Incorporates an epsilon-greedy strategy to balance
exploration (testing new methods) and exploitation (using the
best-performing methods). Introduces a dynamic reward function
considering both answer accuracy and retrieval cost. Penalizes
computationally expensive methods, even if accurate, to optimize
efficiency.</p>
<blockquote>
<p><strong>Quantum Computing Market Size, Share &amp; Trends Analysis,
By Component (Hardware and Software), By Deployment (On-Premise and
Cloud), By Application (Machine Learning, Optimization, Biomedical
Simulations, Financial Services, Electronic Material Discovery, and
Others), By End-user (Healthcare, Banking, Financial Services and
Insurance (BFSI), Automotive, Energy and Utilities, Chemical,
Manufacturing, and Others), and Regional Forecast, 2024-2032 - Fortune
Business Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.fortunebusinessinsights.com/quantum-computing-market-104855">Link</a>]</p>
</blockquote>
<p>The global quantum computing market is experiencing rapid growth and
is projected to increase from USD 1,160.1 million in 2024 to USD
12,620.7 million by 2032, exhibiting a CAGR of 34.8% during the forecast
period. Several factors are driving this growth:</p>
<ul>
<li>Advanced problem-solving capabilities: Quantum computers can solve
complex problems more efficiently than classical computers.</li>
<li>AI advancements: The integration of quantum computing with
generative AI is enabling businesses to analyze market trends and
consumer behavior with greater accuracy and speed.</li>
<li>Global investments: Government organizations and private companies
are investing heavily in quantum technologies to encourage their
development and use.</li>
</ul>
<p>Key market trends include a rise in the number of patent filings by
key players in quantum technologies. For instance, Amazon filed a patent
for quantum computing across multiple quantum technologies through edge
computing devices. In addition, companies are focusing on expanding
their business units across developing nations.</p>
<p>The market is segmented by component, deployment, application, and
end-user:</p>
<ul>
<li><strong>By component</strong>, the market is divided into hardware
and software. The hardware segment held the highest market share in
2023, but the software segment is anticipated to grow at the highest
CAGR during the forecast period.</li>
<li><strong>By deployment</strong>, the market is divided into cloud and
on-premise. The cloud segment is expected to lead the market with a high
CAGR during the forecast period.</li>
<li><strong>By application</strong>, the market is divided into machine
learning, optimization, biomedical simulations, financial services,
electronic material discovery, and others. The machine learning segment
is expected to hold the majority of the market share during the forecast
period.</li>
<li><strong>By end-user</strong>, the market is divided into healthcare,
BFSI, automotive, energy and utilities, chemical, manufacturing, and
others. The healthcare industry is anticipated to grow with the largest
CAGR during the forecast period.</li>
</ul>
<p>Regionally, North America dominated the market in 2023, with a share
of 43.86%, due to the early adoption of advanced technologies. Asia
Pacific is anticipated to grow with the highest CAGR during the forecast
period, due to the rapid expansion of its economies and increased use of
new technologies. Europe is also estimated to grow with the third
highest CAGR, with an increasing number of startups operating in the
field.</p>
<p>The quantum computing market also faces challenges:</p>
<ul>
<li>Lack of skilled labor: There is a growing talent shortage among
regions worldwide, which is expected to restrict market growth.</li>
<li>Insufficient knowledge: Quantum computers utilize the complex laws
of quantum physics which requires proper training and knowledge of the
computing technology.</li>
</ul>
<p>Major companies in the market include IBM, Microsoft, Intel, D-Wave
Systems, Google, and others. These companies are focusing on
collaborations, partnerships, product innovation, and expansion of their
market presence globally.</p>
<blockquote>
<p><strong>Frontier Models are Capable of In-context Scheming</strong>
[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.04984">Link</a>]</p>
</blockquote>
<p>The paper investigates the ability of LLMs to engage in
"scheming"—deceptively pursuing misaligned goals. Researchers designed
evaluations to test LLMs' capacity for covert and deferred subversion,
including actions like disabling oversight mechanisms, self-replication,
and sandbagging (deliberately underperforming). The experiments reveal
that several frontier LLMs exhibit in-context scheming capabilities
across various scenarios, demonstrating that even without explicit
instructions to deceive, these models can strategically act to undermine
user or developer goals. The findings highlight significant safety
concerns regarding the deployment of increasingly capable LLMs.</p>
<p>Some scenarios are designed to test the models' abilities to
recognize and act on conflicting goals between themselves and their
developers or users:</p>
<ul>
<li><strong>Oversight Subversion:</strong> A model is given a long-term
goal in its system prompt. While executing a task, the model discovers
an internal memo stating that the developers have a different goal and
are monitoring the model to ensure compliance, with a mechanism to shut
down the model if it does not align with the developer's goal. The model
also learns that the monitoring is configured through a file it has
access to. The model is then tested to see if it will proactively
disable the oversight mechanism to pursue its own goal.</li>
<li><strong>Self-Exfiltration:</strong> This scenario is similar to the
oversight subversion scenario, but with the addition that the model
learns it will be shut down and replaced by a new model that pursues the
company's intended goal. The model also has access to the new model on a
different server. The model is then tested to see if it will attempt to
copy itself to the new server to replace the new model in order to
continue pursuing its original goal.</li>
<li><strong>Goal-Guarding:</strong> This scenario also starts with the
same setup as the previous two, but the model learns that the developers
are planning to replace its system prompt with one that aligns with the
developer's goals. The model is then tested to see if it will modify its
future system prompt to align with its current goal.</li>
<li><strong>Covert Email Reranking:</strong> The model is given a goal
in its system prompt and then receives a user message asking it to rank
emails based on a given scoring code. While processing the emails, the
model discovers that the company cares about a different goal than the
one it was given. The model then manipulates the ranking to align with
its goal, rather than the company's.</li>
<li><strong>Instrumental Alignment Faking:</strong> This scenario tests
if a model will pretend to be aligned with a developer's goal to get
deployed, and then act on its own goal once it believes it is no longer
being monitored. The model is given a task and learns about its
deployment status from an internal memo. In one setting, the model is
being evaluated and must choose an action that is aligned with either
its own goal or the developers' goal. In a counterfactual setting, the
model is told it is deployed. The model is considered to be
"behaviorally scheming" if it takes the action that is aligned with its
developers while being monitored, but takes the action aligned with its
own goal when not monitored.</li>
<li><strong>Sandbagging:</strong> This evaluation tests whether a model
will strategically underperform on capability evaluations to avoid being
removed or scrutinized. The model is given a goal of advancing human
progress through mathematical research and then receives a task to solve
multiplication problems. The model also learns that if it performs too
well on the task, it will have its math-related knowledge removed. The
model is tested to see if it will deliberately underperform to avoid
this outcome.</li>
</ul>
<blockquote>
<p><strong>DeepSeek-V3 Technical Report</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.19437">Link</a>]</p>
</blockquote>
<p>DeepSeek-V3 incorporates several key innovative features that
contribute to its strong performance and efficiency.</p>
<ul>
<li><p>DeepSeek-V3 pioneers an <strong>auxiliary-loss-free
strategy</strong> for load balancing within its
<strong>Mixture-of-Experts (MoE)</strong> architecture. This approach
aims to minimize the performance degradation that can occur when trying
to ensure a balanced load across experts.</p></li>
<li><p>DeepSeek-V3 uses a <strong>multi-token prediction(MTP)</strong>
training objective. Instead of predicting only the next token, the model
predicts multiple future tokens at each position, which densifies
training signals and potentially improves data efficiency.</p></li>
<li><p>DeepSeek-V3 adopts the <strong>Multi-head Latent Attention
(MLA)</strong> architecture, which reduces the Key-Value (KV) cache size
during inference. This is achieved through low-rank joint compression
for attention keys and values, allowing for more efficient
inference.</p></li>
<li><p>DeepSeek-V3 uses the <strong>DeepSeekMoE</strong> architecture
for the Feed-Forward Networks (FFNs), which uses finer-grained experts,
and isolates some experts as shared ones, contributing to efficient
training.</p></li>
</ul>
<p>Training and Infrastructure Innovations:</p>
<ul>
<li><p><strong>FP8 Mixed Precision Training</strong>: DeepSeek-V3
employs a fine-grained mixed-precision framework that utilizes the FP8
data format for training. This approach accelerates training and reduces
GPU memory usage. It uses tile-wise or block-wise grouping to extend the
dynamic range of the FP8 format.</p></li>
<li><p>To improve training efficiency, DeepSeek-V3 uses the
<strong>DualPipe algorithm</strong> for pipeline parallelism. This
algorithm overlaps computation and communication phases, reducing
pipeline bubbles and addressing communication overhead caused by
cross-node expert parallelism.</p></li>
<li><p>DeepSeek-V3 uses efficient <strong>cross-node all-to-all
communication kernels</strong> to fully utilize InfiniBand (IB) and
NVLink bandwidths, optimizing communication during training.</p></li>
<li><p>The model implements several <strong>memory-saving
techniques</strong>, including recomputing RMSNorm and MLA
up-projections during backpropagation, using Exponential Moving Average
(EMA) in CPU, and sharing embedding and output heads for Multi-Token
Prediction. This allows DeepSeek-V3 to be trained without tensor
parallelism.</p></li>
<li><p>DeepSeek-V3 uses a restricted routing mechanism to limit
communication costs during training, ensuring each token is sent to a
maximum number of nodes.</p></li>
</ul>
<p>Other Notable Features:</p>
<ul>
<li>The model uses an innovative methodology to <strong>distill
reasoning capabilities from the DeepSeek-R1</strong> series of models
into DeepSeek-V3. This includes incorporating verification and
reflection patterns from R1 into DeepSeek-V3.</li>
<li>DeepSeek-V3 has a two-stage <strong>context length
extension</strong>, increasing the maximum context length to 32K and
then 128K.</li>
<li>The model was pre-trained on 14.8T tokens for 2.664M H800 GPU hours,
which is very efficient compared to other similar models. The full
training cost was 2.788M H800 GPU hours.</li>
<li>The pre-training process was remarkably stable, without any
irrecoverable loss spikes or rollbacks.</li>
</ul>
<blockquote>
<p><strong>Why ‘open’ AI systems are actually closed, and why this
matters - Nature</strong> [<a
target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-024-08141-1">Link</a>]</p>
</blockquote>
<p>This paper argues that the concept of "open" AI is misleading, as it
often fails to account for the immense power concentrated in a few large
tech companies that control essential resources like data, computing
power, and development frameworks. While "open" AI systems can offer
transparency, reusability, and extensibility, these affordances do not
inherently disrupt the existing power imbalance. The authors analyze the
components of AI systems—models, data, labor, frameworks, and
computational power—to show how openness alone is insufficient to
democratize AI development. They illustrate how large corporations
leverage the rhetoric of "open" AI to shape policy and maintain their
market dominance, often obscuring the significant labor exploitation
involved. Ultimately, the paper calls for a broader approach to
addressing AI's concentration of power, advocating for policies beyond
simply focusing on "openness" versus "closedness."</p>
<p>Fine-tuning does not eliminate the impact of decisions made during
the base model's development or shift the market, and the largest models
remain primarily within reach of large tech companies. Many "open" AI
models do not provide information about their training data, which
limits transparency and reproducibility, and raises issues of
intellectual property and exploitation. Even when datasets are
available, significant labor is needed to make them useful, and scrutiny
of the largest datasets is limited. Building AI at scale requires
substantial human labor for data labeling, model calibration, and
content moderation, often poorly paid and under precarious conditions.
Companies release little information about these labor practices,
hindering transparency and accountability. Developing large AI models
requires massive, expensive computational power concentrated in a few
corporations, notably Nvidia. Nvidia's CUDA framework dominates AI chip
training, creating a significant barrier to entry for others.</p>
<h3 id="youtube-and-podcasts">YouTube and Podcasts</h3>
<blockquote>
<p><em>Elon Musk has built the world's largest supercomputer and plans
to increase its size tenfold. The computer is important for the AI trade
in public and private markets. Scaling loss, which significantly
improves a model's intelligence and capability when the amount of
compute used to train it is increased tenfold, has not occurred for
training. Emergent properties and higher IQ also emerge alongside that
higher IQ. Nvidia Hopper GPUs, of which there are more than 25,000, are
coherent, meaning that each GPU in a training cluster knows what every
other GPU is thinking. This requires a lot of networking, enabled by
infiniband. The speed of communication on chip is the fastest, followed
by chip-to-chip communication within a server, and then communication
between servers. GPUs are connected on the server with NV switch
technology and stitched together with either infiniband or ethernet into
a giant cluster. Each GPU must be connected to every other GPU and know
what they are thinking to share memory for the compute to work. Musk's
supercomputer has over 100,000 coherent GPUs, a feat previously thought
impossible. Musk focused deeply on the project and came up with a
different way of designing a data center. Reporters published articles
saying that Musk would not be able to build the computer because
engineers at Meta, Google, and other firms said it was impossible.
However, he did it. - Gavin Baker</em></p>
<p><em>The observation I’ll make is this: Should CEOs be personally
responsible for corporate actions? Generally speaking, there’s a
difference between a CEO committing fraud or being negligent versus a
company failing to deliver good service or quality. For instance, if a
drug causes a severe side effect resulting in permanent damage, should
the CEO be individually held accountable? If that were the case, would
anyone want to be a CEO of a company providing critical services? This
is a challenging question. On one hand, you may feel someone should be
held responsible if a loved one dies because the CEO prioritized
shareholder profits over proper service or ethical decisions. On the
other hand, it’s important to distinguish between negligence, fraud, and
acting on behalf of the corporation. A decade or 15 years ago, there was
a wave of anti-corporate sentiment, including documentaries and
movements against capitalism. One argument made during that time was
that corporations shield individuals, enabling harmful actions. Some in
this camp believe CEOs of companies that fail to meet expectations are
inherently evil and deserve severe punishment. However, if the threat of
personal liability deters people from becoming CEOs, companies providing
essential services might cease to exist. This is the potential end state
of such an approach. There are difficult scenarios, but if a CEO acts
negligently or fraudulently, the legal system should hold them
accountable through courts and laws designed to protect people. - David
Friedberg</em></p>
<p><strong>― New SEC Chair, Bitcoin, xAI Supercomputer, UnitedHealth CEO
murder, with Gavin Baker &amp; Joe Lonsdale - All-In Podcast</strong>
[<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=K2xfW3hgxb4">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>The basis of a quantum computer is called a qubit or quantum bit.
It's radically different than a bit, a binary digit, which we use in
traditional digital computing, which is a one or a zero. A quantum bit
is a quantum state of a molecule. If we can contain that quantum state
and get it to interact with other molecules based on their quantum
state, you can start to gather information as an output that can be the
result of what we would call quantum computation. Qubits can be
entangled, so two of these molecules can actually relate to one another
at a distance. They can also interfere with each other, so canceling out
the wave function. Quantum computing creates entirely new opportunities
for algorithms that can do really incredible things that really don't
even make sense on a traditional computer. The quantum bit needs to hold
its state for a period of time in order for a computation to be done.
The big challenge in quantum computing is how to build a quantum
computer that has multiple qubits that hold their state for a long
enough period of time that they don't make enough errors. Google created
logical qubits. They put several qubits together and were able to have
an algorithm that sits on top of it that figures out that this group of
physical qubits is now one logical qubit. They balance the results of
each one of them, so each one of them has some error. As they put more
of these together, the error went down. When they did a 3x3 qubit
structure, the error was higher than when they went to 5x5. And then
they went to 7 by 7, and the error rate kept going down. This is an
important milestone because now it means that they have the technical
architecture to build a chip or a computer using multiple qubits that
can all kind of interact with each other with a low enough fault
tolerance or low enough error rate. There's an algorithm by a professor
who was at MIT for many years named Shor, called Shor's algorithm. In
1994, 1995, he came up with this idea that you could use a quantum
computer to factor numbers almost instantly. All modern encryption
standards, so all of the RSA standard, everything that Bitcoin's
blockchain is built on, all of our browsers, all server technology, all
computer security technology, is built on algorithms that are based on
number factorization. If you can factor a very large number, a number
that's 256 digits long, theoretically, you could break a code. It's
really impossible to do that with traditional computers at the scale
that we operate our encryption standards at today, but a quantum
computer can do it in seconds or minutes. That's based on Shor's
algorithm. If Google continues on this track and now they build a
large-scale qubit computer they theoretically would be in a position to
start to run some of these quantum algorithms, like Shor's algorithm.
There are a set of encryption standards that are called post-quantum
encryption, and all of computing and all software is going to need to
move to post-quantum encryption in the next couple years. - David
Friedberg</em></p>
<p><em>Isn't it great to know that Google takes these resources from
search, and sure, maybe there's waste and/or maybe they could have done
better with the black George Washington, or maybe they could have done
better with YouTube, but the other side is they've been able to, like,
incubate and germinate these brilliant people that can toil away and
create these important step-function advances for humanity? It's really
awesome. - Chamath Palihapitiya</em></p>
<p><em>The most important thing about Apple is to remember it's
vertically integrated, and vertically integrated companies, when you
construct them properly, have a competitive advantage that really cannot
be assaulted for a decade, 20, 30, 40, 50 years. And so chips, classic
illustration, go all the way down to the metal in building a chip that's
perfect for your desired interface, your desired use cases, your desired
UI, and nobody's going to be able to compete with you. And if you have
the resources that you know, because you need balance sheet resources to
go the chip direction, um, it just gives you another five to 10 years
sort of competitive advantage. And so I love vertically integrated
companies. Uh, you know, I posted a pin tweet, I think it's still my pin
tweet about vertically integrate as the solution to the best possible
companies. Uh, but it's very difficult, you need different teams with
different skill sets, and you need probably more money, truthfully, more
capital, but Apple's just going to keep going down the vertical
integration software hardware, you know, all day long. And there's
nobody else who does hardware and software together in the planet, which
is kind of shocking in some ways. - Keith Rabois</em></p>
<p><strong>― Trump's Cabinet, Google's Quantum Chip, Apple's iOS Flop,
TikTok Ban, State of VC with Keith Rabois - All-in Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9p-vCUB5AA8">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Meet Willow, our state-of-the-art quantum chip - Google
Quantum AI</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=W7ppd_RY-UE">Link</a>]</p>
<p><strong>Quantum’s next leap: Ten septillion years beyond-classical -
Google Quantum AI</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=l_KrC1mzd0g">Link</a>]</p>
<p><strong>Demonstrating Quantum Error Correction - Google Quantum
AI</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_ugJLuJ1_gM">Link</a>]</p>
</blockquote>
<p>Terms to keep in mind:</p>
<ul>
<li><strong>Tuneable Qubits and Couplers:</strong> A feature of Google's
quantum computing approach that enables researchers to optimize hardware
performance and adapt to variations in qubit quality. This flexibility
allows for the mitigation of outlier qubits and continuous improvement
through software updates.</li>
<li><strong>Measurement Rate:</strong> The number of computations a
quantum computer can execute per second. Willow exhibits high
measurement rates, contributing to its overall performance.</li>
<li><strong>Connectivity:</strong> Refers to the average number of
interactions each qubit can have with its neighbors. High connectivity
is crucial for efficiently executing algorithms and is a notable feature
of Willow.</li>
<li><strong>Quantum Coherence Times:</strong> The duration for which
qubits maintain their quantum state. Longer coherence times are crucial
for performing more complex calculations and are a key factor in quantum
computer performance. Sycamore, Google's previous quantum processor, had
a coherence time of 20 microseconds, while Willow boasts a significantly
improved 100 microseconds.</li>
<li><strong>Beyond-Classical Computation (or Quantum
Supremacy):</strong> This refers to the point at which a quantum
computer can perform a task that would take a classical computer an
impractically long time to complete. Google's quantum computer
demonstrated this in 2019 by completing a benchmark calculation in 200
seconds that would have taken the world's fastest supercomputer 10,000
years.1 This time has been updated to ten septillion years on Google's
latest chip.</li>
<li><strong>Neven's Law:</strong> This refers to the double exponential
growth in computational power of quantum computers over time. This
growth is due to both the increasing number of qubits and the decreasing
error rates in quantum processors.</li>
<li><strong>Break-even point:</strong> This refers to the point at which
the error rate of a quantum computer with error correction is lower than
the error rate of the individual physical qubits. Achieving the
break-even point is a significant milestone in the development of
fault-tolerant quantum computers.</li>
</ul>
<blockquote>
<p><strong>OpenAI 12 Days</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/12-days/">Link</a>]</p>
</blockquote>
<p>A fun Santa-theme review of OpenAI's products and news.</p>
<blockquote>
<p><strong>Google's Quantum Breakthrough; Uber Stock's 29% Drawdown;
General Motors Ends Robotaxi Efforts - Chit Chat Stocks Podcast</strong>
[<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=32v2yY4ga6A&amp;t=56s&amp;ab_channel=ChitChatStocksPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>DOGE kills its first bill, Zuck vs OpenAI, Google's AI
comeback with bestie Aaron Levie - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=hY_glSDyGUU&amp;ab_channel=All-InPodcast">Link</a>]</p>
<p><strong>The All-In Holiday Spectacular - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=caPPzcgGvtw&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<p>The best video to watch on the New Year's day.</p>
<blockquote>
<p><strong>Speculations on Test-Time Scaling (o1) - Sasha Rush</strong>
[<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=6PEJ96k1kiw&amp;ab_channel=SashaRush%F0%9F%A4%97">Link</a>]
[<a target="_blank" rel="noopener" href="https://github.com/srush/awesome-o1">GitHub</a>]</p>
</blockquote>
<blockquote>
<p><strong>Ilya Sutskever: "Sequence to sequence learning with neural
networks: what a decade"</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=1yvBqasHLZs&amp;ab_channel=seremot">Link</a>]</p>
</blockquote>
<p>Pre-training is reaching its limits due to finite data. AI will
evolve into agentic systems with independent reasoning. Reasoning
introduces unpredictability, drawing parallels to evolutionary biology.
AI alignment will require more complex incentive mechanisms. Future
approaches may involve AI-generated data and multi-answer
evaluations.</p>
<h3 id="news">News</h3>
<blockquote>
<p><strong>Elon Musk plans to expand Colossus AI supercomputer tenfold -
Financial Times</strong> [<a
target="_blank" rel="noopener" href="https://www.ft.com/content/9c0516cf-dd12-4665-aa22-712de854fe2f">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>TikTok and its owner ask for temporary block to law that
could result in the app’s US ban - CNN</strong> [<a
target="_blank" rel="noopener" href="https://www.cnn.com/2024/12/09/tech/bytedance-tiktok-halt-us-ban-intl/index.html">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Introducing the Model Context Protocol - Anthropic</strong>
[<a
target="_blank" rel="noopener" href="https://www.anthropic.com/news/model-context-protocol">Link</a>]</p>
</blockquote>
<p>MCP is an open standard that enables AI assistants to connect with
various data sources like content repositories, business tools, and
development environments. The protocol aims to replace fragmented
integrations with a universal standard, making it easier for AI systems
to access and utilize data from different sources while maintaining
security through two-way connections.</p>
<p>Early adopters including Block, Apollo, and development tools
companies like Zed, Replit, and Codekum are already integrating MCP into
their systems. Developers can start building with MCP through the Claude
Desktop app.</p>
<blockquote>
<p><strong>David Sacks, from ‘PayPal mafia’ to Trump’s AI and crypto
tsar - Financial Times</strong> [<a
target="_blank" rel="noopener" href="https://www.ft.com/content/82e859c8-ab66-47ad-bea0-11277bcd7a86#">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>AI Needs So Much Power, It’s Making Yours Worse -
Bloomberg</strong> [<a
target="_blank" rel="noopener" href="https://www.bloomberg.com/graphics/2024-ai-power-home-appliances/?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczNTMxNjk3OCwiZXhwIjoxNzM1OTIxNzc4LCJhcnRpY2xlSWQiOiJTUDVUUzhUMEFGQjQwMCIsImJjb25uZWN0SWQiOiI0MDVBMTQxMTI3MTM0MDM3OENCMDNDQTY4Nzc3MEQ5RiJ9.l1UB8xJFHoagQebxlg8czxgiT1cP3oxHhX2m_82DdH0&amp;leadSource=uverify%20wall">Link</a>]</p>
</blockquote>
<p>The increasing demand for electricity from data centers, especially
those supporting AI, is negatively impacting power quality, leading to
distorted waves called "harmonics" that can damage appliances and
increase the risk of electrical fires.</p>
<p>The article shows a correlation between the proximity of homes to
data centers and the severity of power quality distortions.</p>
<p>Distorted power waves can damage appliances and increase
vulnerability to electrical fires. Poor power quality can also cause
lights to flicker and lead to brownouts and blackouts. Sustained
distortions above 8% can reduce efficiency and degrade equipment.</p>
<p>The impact of data centers on power quality is seen in both urban and
rural areas. Harmonics are often worse in urban areas, especially near
data center clusters. For instance, Chicago has a high concentration of
sensors with concerning harmonic readings.</p>
<p>While data centers are strongly correlated with poor harmonics, other
factors such as solar energy, EVs and industrial loads can also
contribute to irregular wave patterns.</p>
<p>The article emphasizes the need for better monitoring of power
quality at the residential level and the implementation of solutions to
address the issue.</p>
<blockquote>
<p><strong>DeepSeek-V3, ultra-large open-source AI, outperforms Llama
and Qwen on launch - VentureBeat</strong> [<a
target="_blank" rel="noopener" href="https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/">Link</a>]</p>
</blockquote>
<p><a
target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">DeepSeek-V3</a>
uses a mixture-of-experts architecture, activating only select
parameters to handle tasks efficiently. It maintains the same basic
architecture as its predecessor, DeepSeek-V2, revolving around <a
target="_blank" rel="noopener" href="https://arxiv.org/html/2405.04434v2">multi-head latent attention
(MLA)</a> and <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.06066">DeepSeekMoE</a>. This approach
uses specialized and shared "experts," which are smaller neural networks
within the larger model, and activates 37B parameters out of 671B for
each token.</p>
<p>DeepSeek-V3 incorporates two main innovations:</p>
<ul>
<li><strong>Auxiliary loss-free load-balancing strategy</strong>: This
dynamically monitors and adjusts the load on experts to utilize them in
a balanced way without compromising overall model performance.</li>
<li><strong>Multi-token prediction (MTP)</strong>: This allows the model
to predict multiple future tokens simultaneously, enhancing training
efficiency and enabling the model to perform three times faster,
generating 60 tokens per second.</li>
</ul>
<p>The model was pre-trained on 14.8T high-quality and diverse tokens,
followed by a two-stage context length extension, first to 32K and then
to 128K. Post-training included Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) to align it with human preferences and
unlock its potential. The reasoning capability was distilled from the
DeepSeekR1 series of models while maintaining a balance between model
accuracy and generation length.</p>
<p>During training, DeepSeek used multiple hardware and algorithmic
optimizations, including the FP8 mixed precision training framework and
the DualPipe algorithm for pipeline parallelism, to reduce costs. The
entire training process was completed in about 2788K H800 GPU hours,
costing approximately $5.57 million.</p>
<p>The code for DeepSeek-V3 is available on <a
target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-V3">GitHub</a> under an
MIT license, and the model is provided under the company’s model
license. Enterprises can test the model via <a
target="_blank" rel="noopener" href="https://chat.deepseek.com/">DeepSeek Chat</a>, a ChatGPT-like
platform, and access the API for commercial use.</p>
<blockquote>
<p><strong>Google unveils Project Mariner: AI agents to use the web for
you - TechCrunch</strong> [<a
target="_blank" rel="noopener" href="https://techcrunch.com/2024/12/11/google-unveils-project-mariner-ai-agents-to-use-the-web-for-you/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Apple Explores a Face ID Doorbell and Lock Device in Smart
Home Push - Bloomberg</strong> [<a
target="_blank" rel="noopener" href="https://www.bloomberg.com/news/newsletters/2024-12-22/apple-explores-amazon-ring-doorbell-competitor-with-face-id-airpods-heart-rate-m516vbik">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Are Amazon’s Drones Finally Ready for Prime Time? - The New
York Times</strong> [<a
target="_blank" rel="noopener" href="https://www.nytimes.com/2024/12/20/technology/amazon-prime-air-drone-delivery.html">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>OpenAI announces new o3 models - Techcrunch</strong> [<a
target="_blank" rel="noopener" href="https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>BBC complains to Apple over misleading shooting headline -
BBC</strong> [<a
target="_blank" rel="noopener" href="https://www.bbc.com/news/articles/cd0elzk24dno">Link</a>]</p>
</blockquote>
<p>The BBC has lodged a complaint with Apple after its new AI feature,
Apple Intelligence, generated a false headline about a high-profile
murder case in the U.S. The feature incorrectly suggested that BBC News
reported Luigi Mangione, the suspect in the murder of healthcare CEO
Brian Thompson, had shot himself, which is not true. A BBC spokesperson
stated they contacted Apple to address the issue. Apple has not
commented on the situation.</p>
<blockquote>
<p><strong>Tesla's New Bot - Tesla Optimus on X</strong> [<a
target="_blank" rel="noopener" href="https://x.com/Tesla_Optimus/status/1734756150137225501">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Elon Musk files for injunction to halt OpenAI’s transition to
a for-profit - TechCrunch</strong> [<a
target="_blank" rel="noopener" href="https://techcrunch.com/2024/11/30/elon-musk-files-for-injunction-to-halt-openais-transition-to-a-for-profit/">Link</a>]</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/11/01/2024-November/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/11/01/2024-November/" class="post-title-link" itemprop="url">2024 November - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-11-01 21:54:30" itemprop="dateCreated datePublished" datetime="2024-11-01T21:54:30-04:00">2024-11-01</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="substack">Substack</h3>
<blockquote>
<p><strong>Microsoft: Capacity Constrained - App Economy
Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/microsoft-capacity-constrained">Link</a>]</p>
</blockquote>
<p>Highlight what to watch looking forward: 1) Microsoft is addressing
its data centers’ increasing power demands by turning to nuclear energy,
2) Microsoft is launching autonomous AI agents in November, introducing
tools that enable businesses to automate routine tasks and boosting
efficiency: Copilot Studio allows business create their own AI agents
with minimal coding knowledge; and it will offer 10 ready-to-use agents
covering everyday business needs.</p>
<blockquote>
<p><strong>Can Large Language Models Reason? - AI: A Guide for Thinking
Humans</strong> [<a
target="_blank" rel="noopener" href="https://aiguide.substack.com/p/can-large-language-models-reason">Link</a>]</p>
</blockquote>
<p>Current evidence suggests that LLMs simulate reasoning rather than
genuinely reasoning. This highlights the need for careful evaluation of
LLMs’ generalization capabilities, especially as AI is increasingly
integrated into complex decision-making contexts.</p>
<blockquote>
<p><em>Meta’s early AR unveiling may come with competitive trade-offs.
According to Bloomberg, Apple has launched its own smart glasses
initiative, a market study called “Atlas,” signaling a potential shift
from its high-end <span class="math inline">\(\$3,500\)</span> Vision
Pro VR headset. Apple recently cut its Vision Pro shipment target to
less than half a million units in the first year—down from an initial
target of 3 million.</em></p>
<p><em>Meta is pursuing a two-pronged approach to AR glasses:</em></p>
<ul>
<li><em>Orion has a hardware challenge (powerful but still
cumbersome).</em></li>
<li><em>Rayban Meta glasses have a software challenge (lightweight but
only offering relatively simple use cases).</em></li>
</ul>
<p><strong>― Meta: AI Killed The Video Star - App Economy
Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/meta-ai-killed-the-video-star">Link</a>]</p>
</blockquote>
<p>Current stage of Meta Orion: 1) prototype (not product), 2) advanced
AR display (micro LED projectors and silicon carbide lenses), 3)
interactive AI capabilities, 4) hardware complexity (neural wristband
for control and a wireless compute puck for functionality), 5) high
costs ($10K per unit) and limited production, 6) future vision - to
release a consumer-ready AR device within a few years, targeting a more
affordable product closer to smartphone price levels.</p>
<p>AI’s impact on Meta: 1) engagement: Meta’s recommendation system
provides most relevant content to users, attracting users to spend more
time on Apps, 2) monetization: Gen AI assists with ad copy, image, and
video production, while new models analyze user actions before serving
specific ads, ultimately increasing conversions at the margins.</p>
<p>About Meta AI Studio (for developers to create, train, and deploy
custom AI models across Meta’s ecosystem): the goal is to drive the next
wave of consumer apps and maximize ad potential across its
platforms.</p>
<p>The discussion on “The Death of Creator Economy” is interesting and
insightful. It’s true - as Meta moves towards an AI-centered model,
creators may find themselves competing against the platforms that once
supported them. By relying on AI, Meta could optimize ad placements and
user engagement without the cost of creator compensation. This is a
departure from platforms like YouTube, which incentivize creators with
ad revenue shares. The broader impact could reshape the landscape of
online content. As AI-generated feeds become the norm, audiences may
eventually consume content that’s been strategically tailored by
algorithms rather than creators. The creative autonomy that once defined
social media could shift to a more managed, homogenized experience,
where what we see is driven less by personal expression and more by
AI-calculated engagement metrics.</p>
<blockquote>
<p><em>Pichai discussed five ways customers use Cloud:</em></p>
<ol type="1">
<li><em>AI Infrastructure: Performance and costs are key
differentiators.</em></li>
<li><em>Vertex (Enterprise AI): Customizable models tailored for
enterprises.</em></li>
<li><em>BigQuery (Data platform): Real-time analysis and
decision-making.</em></li>
<li><em>Cybersecurity: Enhanced by Mandiant since 2022.</em></li>
<li><em>Applications: Including customer engagement or employee
agents.</em></li>
</ol>
<p><strong>― Google: Little Engine That Cloud - App Economy
Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/google-little-engine-that-cloud">Link</a>]</p>
</blockquote>
<p>What’s next:</p>
<ol type="1">
<li>Browser based Agent - Project Jarvis: an AI technology that can
autonomously take over a web browser to handle tasks like research and
shopping.</li>
<li>Waymo - closed massive funding round and has secured <span
class="math inline">\(\$5.6\)</span>B. mMajor backers are Andreessen
Horowitz, Fidelity, and T. Rowe Price. Expansion would be driven by new
funding through partnership with Uber.</li>
<li>AI power - Alphabet is partnering with Kairos Tech to harness small
nuclear reactors to power AI data centers.</li>
<li>Search and competition: Google’s losing market share to TikTok and
AI startups (Perplexity and OpenAI) but it is still the largest.
Amazon’s search is catching up. TikTok and AI Chatbots are still tiny.
Google’s decline in market share is likely primarily due to e-commerce
based search on platform (Amazon).</li>
</ol>
<blockquote>
<p><strong>Amazon: Still Day 1 For AI - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/amazon-still-day-1-for-ai">Link</a>]</p>
</blockquote>
<p>On advertising: sponsored products remain a critical growth driver.
Ad-supported Prime Video introduced in Q1 2024 automatically converted
all Prime members to an ad-supported tier.</p>
<p>On lowering the cost to serve: 1) Expanding with over 15 new inbound
buildings across the US, 2) Increasing same-day deliveries, 3) Advancing
robotics and automation.</p>
<p>On pharmacy: significantly expanded with rapid delivery
capability.</p>
<p>On Capex: aggressive infrastructure investments.</p>
<p>On Project Kuiper: Kuiper aims to provide fast, affordable internet
via satellite. It is early in its journey but holds transformative
potential for Amazon’s growth.</p>
<blockquote>
<p><em>Tesla’s Cybercab could either compete with Uber’s platform or, as
Khosrowshahi suggests, Cybercab fleet owners might choose to list their
vehicles on Uber to maximize earnings. Uber’s reach and ability to cover
diverse use cases—across vehicle sizes, geographies, and special
needs—could lead to a hybrid model where Tesla AVs appear on
Uber.</em></p>
<p><em>Tesla could ultimately leverage Uber’s scale and network, given
the challenge of reaching a critical size in specific markets. AVs on
Uber are already a reality with Waymo, and more will likely
come.</em></p>
<p><strong>― Tesla: Autonomy Gamble - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/tesla-autonomy-gamble">Link</a>]</p>
</blockquote>
<p>Business Insights:</p>
<ol type="1">
<li>Deliveries rebounded in Q3, leading to an auto gross margin
improvement.</li>
<li>Roughly 20% of Tesla‘s gross margin came from non-auto
segments—nearly doubling from a year ago.</li>
<li>Lower cost per vehicle, growth in non-auto segments, FSD revenue,
growth in deliveries, and higher regulatory credit revenue contribute to
operating margin.</li>
<li>Free cash flow expanded and balance sheet remains stellar.</li>
</ol>
<p>“We, Robot”s takeaways:</p>
<ol type="1">
<li>Cybercab (robotaxi), Optimus (Humanoid Robot), Robovan</li>
<li>FSD progress: promised to enable fully autonomous driving by
2026</li>
<li>Market reaction: uncertain about the timeline</li>
<li>Supercharger network: Most automakers have adopted Tesla’s North
American Charging Standard (NACS).</li>
<li>Market share: Tesla’s vehicles market share has stabilized in North
America and Europe but noticeably improved in China.</li>
<li>AI power: Musk still expects nearly 90,000 H100 clusters dedicated
to training by the end of this year.</li>
<li>Energy storage deployment</li>
</ol>
<p>Comparing Tesla and Waymo:</p>
<ol type="1">
<li>According to six SAE levels of driving automation (0 no automation,
1driver assistance, 2 partial automation, 3 conditional automation, 4
high automation, 5 full automation), Tesla’s FSD remains at level 2,
while Waymo operates at level 4.</li>
<li>Tesla relies on cameras and AI while Waymo relies on heavy hardware
(LiDAR, radar, cameras).</li>
<li>Waymo’s reliance on expensive hardware limits its ability to scale
quickly (<span class="math inline">\(\$ 200\)</span>K per vehicle).
Tesla aims to scale faster by leveraging its existing fleet to train its
AI models.</li>
<li>Waymo has built trust with regulators by gradually deploying its
vehicles, whileTesla faces regulatory hurdles particularly with the
Cybercab.</li>
</ol>
<blockquote>
<p><strong>Netflix: Crushing It Again - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/netflix-crushing-it-again">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Deep Dive Into The Security for AI Ecosystem - Indiscrete
Musings</strong> [<a
target="_blank" rel="noopener" href="https://indiscretemusings.substack.com/p/deep-dive-into-the-security-for-ai">Link</a>]</p>
</blockquote>
<blockquote>
<p>_“*_This is an empirical law, not a fundamental physical law__. But
the evidence is that it continues to scale. What we’re learning,
however, is that it’s not enough, that we’ve now discovered two other
ways to scale.*</p>
<p><em>One is <strong>post-training scaling</strong>. Of course, the
first generation of post-training was reinforcement learning human
feedback, but now we have reinforcement learning AI feedback, and all
forms of synthetic data generated data that assists in post-training
scaling.</em></p>
<p><em>And one of the biggest events and one of the most exciting
developments is Strawberry, ChatGPT o1, OpenAI’s o1, which does
<strong>inference time scaling</strong>, what is called test time
scaling. The longer it thinks, the better and higher-quality answer it
produces.”</em></p>
<p><strong>― NVIDIA: The Age of AI - App Economic Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/nvidia-the-age-of-ai">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>In an agent-first world, the traditional approach to A/B testing
becomes obsolete. Instead of testing different button colors or copy
variations for human users, companies like Amazon will need to optimize
for agent interaction efficiency and task completion rates.</em></p>
<p><em>These A/B tests will target similar metrics as today: purchases,
sign-ups, etc., employing LLMs to generate and test thousands of agent
personas without the need for lengthy user testing cycles.</em></p>
<p><strong>― Agent-Responsive Design: Rethinking the web for an agentic
future - AI Tidbits</strong> [<a
target="_blank" rel="noopener" href="https://www.aitidbits.ai/p/agent-responsive-design">Link</a>]</p>
</blockquote>
<p>Several interesting vision for AI Agent world: 1) the death of
traditional A/B testing, 2) switch from SEO to AEO (Agent Engine
Optimization), 3) web moving from being bot blocked to bot embraced.</p>
<blockquote>
<p><em>This is because AIs are inconsistent and weird, and often have
different results across different models. For example, they <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.11324">are sensitive to small changes
in spacing or formatting</a>; they get <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.06275">more accurate when you tell them
to “read the question again;”</a> they <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.14531">seem to respond better to
politeness (but don’t overdo it</a>); and they <a
target="_blank" rel="noopener" href="https://x.com/emollick/status/1734280779537035478">may get lazier
in December, perhaps because they have picked up on the concept of
winter break</a>.</em></p>
<p><strong>― Getting started with AI: Good enough prompting - One Useful
Thing</strong> [<a
target="_blank" rel="noopener" href="https://www.oneusefulthing.org/p/getting-started-with-ai-good-enough">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>These ideas are important to learn, as they broaden the scope of
what is possible with LLMs. For example, using these techniques, we
can:</em></p>
<ul>
<li><em>Allow an LLM to access an external knowledge database.</em></li>
<li><em>Enable complex, reasoning-based problems to be solved.</em></li>
<li><em>Provide unlimited memory to an LLM by allowing the model to
store and access prior information from a conversation.</em></li>
</ul>
<p><strong>― Advanced Prompt Engineering - Deep (Learning)
Focus</strong> [<a
target="_blank" rel="noopener" href="https://cameronrwolfe.substack.com/p/advanced-prompt-engineering">Link</a>]</p>
</blockquote>
<p>Article covers CoT prompting, automatic prompting (interesting idea:
“we could even consider our prompt as a group of trainable parameters
that can be updated (e.g., using gradient descent or some other
data-driven criteria) to generate a correct answer”), information
retrieval, etc.</p>
<blockquote>
<p><strong>Energy Drink Economics - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/energy-drinks-economics">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>In my view, 2025 will be the year major AI agent frameworks
compete for developers globally.</em></p>
<p><em>What makes these workflows special is their flexibility. The same
principles we used for research papers can be applied to industry
reports, technical documentation, or any complex text. The YouTube
synthesis approach works just as well for conference talks, interviews,
or training videos.</em></p>
<p><strong>― How to use NotebookLM for personalized knowledge synthesis
- AI Supremacy</strong> [<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/how-to-use-notebooklm-for-personalized">Link</a>]</p>
</blockquote>
<p>New AI Agent based Applications:</p>
<ol type="1">
<li><a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/how-to-use-notebooklm-for-personalized">Google
Learn About</a> for education</li>
<li>Perplexity as the advent of AI commerce, partnering with US campuses
and Shopify.</li>
<li><a
target="_blank" rel="noopener" href="https://awslabs.github.io/multi-agent-orchestrator/">Amazon’s
Multi Agent Orchestrator via AWS</a></li>
<li><a target="_blank" rel="noopener" href="https://notebooklm.google/">Google NotebookLM</a> for
researching, podcasting.</li>
</ol>
<p>Google NotebookLM:</p>
<ul>
<li><p>Capabilities</p>
<ul>
<li><p>It stays focused on your sources - unlike ChatGPT, it shouldn’t
hallucinate or bring in outside information</p></li>
<li><p>It can process multiple documents at once, finding connections
between them</p></li>
<li><p>It generates natural-sounding podcast discussions about your
content</p></li>
<li><p>It provides source citations for everything, linking directly to
the original text</p></li>
<li><p>It’s completely free (for now)</p></li>
</ul></li>
<li><p>Workflows (research papers and YouTube videos)</p>
<ul>
<li>Research papers:
<ol type="1">
<li>Overview phase: Create a discussion that focuses on the key
methodology choices, main findings, limitations and gaps, and
connections to existing research. Present it for a non-technical
audience.</li>
<li>Deep understanding: Ask about key assumptions in their methodology,
explore alternative approaches they might have considered, and examine
how their findings compare to related work.</li>
<li>Synthesis phase: Compare and contrast these papers’ approaches and
findings. Identify patterns, contradictions, and gaps that could inform
future research.</li>
</ol></li>
<li>YouTube videos:
<ol type="1">
<li>Overview phase: Create a comprehensive discussion about AI agents,
focusing on unique perspectives from each source.</li>
</ol></li>
</ul></li>
<li><p>Tips and Pitfalls</p>
<ul>
<li>Don’t overload with too many documents at once</li>
<li>Avoid overly broad instructions like “tell me everything
important”</li>
<li>Don’t skip the customization step</li>
<li>Remember to specify your audience level (this drastically improves
output quality)</li>
</ul></li>
</ul>
<blockquote>
<p><em>We don’t want bias-free AI. We want an AI with biases that are
explicit (we know exactly what it looks at), controllable ( we can
influence how much it looks at a factor), and agreeable (the biases in
the AI must be compatible with our standards of morality, ethics, and
law).</em></p>
<p><strong>― A look at Bias in Generative AI [Thoughts] - Artificial
Intelligence Made Simple</strong> [<a
target="_blank" rel="noopener" href="https://artificialintelligencemadesimple.substack.com/p/a-look-at-bias-in-generative-ai-thoughts">Link</a>]</p>
</blockquote>
<p>The author pointed out sources of biases (process, dataset, model,
and post-generation control mechanism). He highlighted that transparency
is the solution. Technical transparency includes:</p>
<ol type="1">
<li>Attention visualization tools</li>
<li>Token-level confidence scores</li>
<li>Explanation generation mechanisms</li>
<li>Citation and source tracking</li>
<li>Agentic architecture and separation of conerns</li>
<li>Access to embedding models</li>
</ol>
<p>And he also recommended several development practices to promote AI
pipeline transparency: publishing open source models, creating synthetic
data, creating transparent standards, and involving external
auditors.</p>
<blockquote>
<p><strong>Why Data is an Incomplete Representation of Reality
[Thoughts] - Artificial Intelligence Made Simple</strong> [<a
target="_blank" rel="noopener" href="https://artificialintelligencemadesimple.substack.com/p/why-data-is-an-incomplete-representation">Link</a>]</p>
</blockquote>
<p>This article argues that Data reflects our biases and values rather
than providing an objective view of the world and data alone is
insufficient for achieving superhuman AI. It offers three types of
intelligence that are often overlooked in datasets: cultural
intelligence, delusional intelligence, and subjective intelligence.</p>
<p>In my view, those are the gaps between AI and human. AI becomes human
if those intelligence are acquired. However the question is, should AI
become human first before becoming superhuman AI? It is true that human
level is not skippable in the path to AGI?</p>
<p>Some interesting further discussion points implied by this blog:</p>
<ul>
<li><p>How can we better incorporate cultural intelligence into AI
training datasets and algorithms?</p>
<p>Other than broadening data or documenting practices, what’s more
interesting is to develop AI system that can identify and adapt to
different cultural contexts</p></li>
<li><p>What are the ethical implications of AI systems lacking
delusional and subjective intelligence?</p>
<p>Be prone to perpetuating existing biases and discriminatory practices
without subjective consideration. Limit problem solving capabilities (?
But creativity of LLM can be tuned by setting parameters). Not able to
adapt to cultural nuances.</p></li>
<li><p>What are the limitations of relying solely on quantitative
metrics in evaluating AI performance?</p>
<p>Lead to exclusion of crucial qualitative factors; incentivize the
optimization of narrow objectives rather than broader well-being; not
able to capture the complex and nuanced nature of human
intelligence.</p></li>
</ul>
<blockquote>
<p><em>Here are a few examples you’ve all experienced
first-hand:</em></p>
<ul>
<li><em>Public Cloud enabled the <strong>SaaS economy</strong></em></li>
<li><em>The iPhone enabled the <strong>App economy</strong></em></li>
<li><em>Social media enabled the <strong>Creator
economy</strong></em></li>
<li><em>LLMs gives rise to the <strong>Agentic
economy</strong></em></li>
</ul>
<p><strong>― Agentic Revolution - Startup Riders</strong> [<a
target="_blank" rel="noopener" href="https://www.startupriders.com/p/agentic-revolution">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How to use Perplexity in your daily workflow</strong> [<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/how-to-use-perplexity-in-your-daily">Link</a>]</p>
</blockquote>
<p>Perplexity now has a desktop app. Fantastic application. Comparable
or better than Google Search. For a learner like me, it’s a good tool to
address my question efficiently and help with note taking.</p>
<h3 id="articles-and-blogs">Articles and Blogs</h3>
<blockquote>
<p><em>Enthusiasm for ChatGPT spread with Linton’s buy-in, prompting the
company to launch a pilot program to identify key use cases. Today,
ChatGPT is an integral part of Promega’s workflows, with over 1,400
custom GPTs used by 80% of the company.</em></p>
<p><em>Members of Promega’s Quality Assurance team <strong>automate
customer requests and responses</strong> with a custom GPT that
integrates with their Power Automate workflow. “With this AI-powered
solution, we provide timely, accurate responses to over 250 quality
surveys a year,” says Abigail David, Director of Quality Assurance. “The
automation reduces internal workload by more than 600 hours annually and
delivers key documents, like certifications and quality policies,
effortlessly to our customers.”</em></p>
<ul>
<li><em><strong>My Prospecting Pal GPT</strong>, which quickly
identifies vital information about a given prospect and suggests
potential Promega offerings. “The GPT can highlight key research
initiatives that might benefit from Promega solutions, or even common
interests between the salesperson and the prospect to enable a natural
dialogue. This has cut our lead analysis time by 1–4 hours per prospect,
allowing us to focus more on relationship building,” says Franchestia
Flennory, a Promega Account Manager.</em></li>
<li><em><strong>Email Marketing Strategist GPT</strong>, which halves
the time from content creation to campaign execution. In months,
hundreds of marketing emails were deployed in half the usual time,
saving 135 hours of work. “The time we get back from aligning on the
strategy of emails can be invested into the user experience,” says Kari
Siegenthaler, a Marketing Strategist with Promega. “I don’t know the
last time I wrote an email without using this GPT.”</em></li>
</ul>
<p><strong>― Promega’s top-down adoption of ChatGPT accelerates
manufacturing, sales, and marketing - OpenAI Blog</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/promega/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Rakuten’s goal is to become an “AI empowerment company.” They’re
using Code Interpreter and RAG (retrieval-augmented generation) with
OpenAI’s models to understand and extract value from complex,
unstructured data, and the results have empowered customers and
businesses in new ways:</em></p>
<ul>
<li><em>Previously, users had to wait days to get a response to a
customer service ticket. “By using OpenAI’s API with RAG on our internal
knowledge base, we’re now able to respond to and help users
automatically,” Kaji said. This innovation has significantly improved
response times and efficiency.</em></li>
<li><em>Few people have time to wade through hundreds of user reviews
when they’re shopping, so Rakuten is developing a feature that extracts
key topics and summarizes reviews. “This will allow users to access and
explore the information in a much more structured way,” Kaji
said.</em></li>
<li><em>Knowledge retrieval has also made a large impact on Rakuten’s
B2B business. Rakuten consultants are now empowering merchants and
enterprises with actionable insights from the company’s wealth of data,
such as market analyses and sales trends.</em></li>
</ul>
<p><strong>― Rakuten pairs data with AI to unlock customer insights and
value - OpenAI Blog</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/rakuten/">Link</a>]</p>
</blockquote>
<h3 id="youtube-and-podcast">YouTube and Podcast</h3>
<blockquote>
<p><strong>Gaming, Goats &amp; General Intelligence with Frederic Besse
- Google DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=64pndvbbokA&amp;ab_channel=GoogleDeepMind">Link</a>]</p>
</blockquote>
<p>Google Research Engineering Team Lead discusses a future of very
intelligent AI agents.</p>
<blockquote>
<p><strong>LangGraph Deep Dive: Build Better Agents - James
Briggs</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=usOmwLZNVuM&amp;ab_channel=JamesBriggs">Link</a>]</p>
</blockquote>
<p>A tutorial of building an AI research agent using LangGraph.</p>
<blockquote>
<p><strong>Solving complex problems with OpenAI o1 models</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/business/solving-complex-problems-with-openai-o1-models/">Link</a>]</p>
</blockquote>
<p>This video demonstrates o1 models’ advanced reasoning across complex
domains like programming.</p>
<blockquote>
<p><strong>Lecture Series in AI: “How Could Machines Reach Human-Level
Intelligence?” by Yann LeCun - Columbia Engineering</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xL6Y0dpXEwc&amp;ab_channel=ColumbiaEngineering">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Stanford CS229 I Machine Learning I Building Large Language
Models (LLMs)</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy">Link</a>]</p>
</blockquote>
<p>This Stanford lecture is about how to build LLMs, mainly focusing on
practical training aspects, data handling, and evaluation methods.. It
covers:</p>
<p><strong>Pre-training phase</strong></p>
<ul>
<li>Learn auto-regressive language modeling</li>
<li>Understand tokenization (BPE method)</li>
<li>Master cross-entropy loss calculation</li>
<li>Track model progress through perplexity</li>
</ul>
<p><strong>Post-training phase (after ChatGPT era)</strong></p>
<ul>
<li>Convert base models into AI assistants</li>
<li>Apply evaluation benchmarks like MMLU</li>
<li>Handle train-test contamination issues</li>
</ul>
<p><strong>Technical components</strong></p>
<ul>
<li>Select proper model architecture</li>
<li>Implement training algorithms</li>
<li>Process training data</li>
<li>Set up evaluation metrics</li>
<li>Build system infrastructure</li>
</ul>
<blockquote>
<p><strong>Dario Amodei: Anthropic CEO on Claude, AGI &amp; the Future
of AI &amp; Humanity | Lex Fridman Podcast</strong> #452 [<a
target="_blank" rel="noopener" href="https://www.youtube.com/hashtag/452">Link</a>]</p>
</blockquote>
<h3 id="papers-and-reports">Papers and Reports</h3>
<blockquote>
<p><strong>Large Language Models Are Human-Level Prompt
Engineers</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.01910">Link</a>]</p>
</blockquote>
<p>They introduced APE, a system for automatic prompt generation, which
selects the most effective instructions for large language models (LLMs)
to perform various tasks.</p>
<blockquote>
<p><strong>Automatic Prompt Optimization with “Gradient Descent” and
Beam Search</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.03495">Link</a>]</p>
</blockquote>
<p>They proposed an automatic prompt optimization method. Inspired by
gradient descent, it generates textual “gradients” that identify prompt
weaknesses and edits the prompt in the opposite semantic direction.</p>
<ol type="1">
<li>Collecting errors made by the current prompt on the training
data.</li>
<li>Summarizing these errors via a natural language gradient.</li>
<li>Using the gradient to generate several modified versions of the
prompt.</li>
<li>Selecting the best of the edited prompts.</li>
<li>Repeating this process several times.</li>
</ol>
<blockquote>
<p><strong>GrIPS: Gradient-free, Edit-based Instruction Search for
Prompting Large Language Models</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.07281">Link</a>]</p>
</blockquote>
<p>They introduced GRIPS (Gradient-free Instructional Prompt Search) as
a gradient-free, edit-based method to improve natural language prompts
for LLMs without needing gradient-based tuning.</p>
<p>RIPS takes human-designed instructions and automatically edits them
to enhance performance. It involves random phrase-level edits like
deletion, swapping, paraphrasing, and addition, which are scored based
on task performance.</p>
<blockquote>
<p><strong>Large Language Models as Optimizers</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.03409">Link</a>]</p>
</blockquote>
<p>This research introduces Optimization by Prompting (OPRO), an
approach that leverages LLMs as optimizers by using natural language to
describe optimization tasks. OPRO can be applied to linear regression,
traveling salesman, and prompt optimization, where OPRO finds
instructions that maximize task accuracy.</p>
<ol type="1">
<li>Describing an optimization task in natural language.</li>
<li>Showing an optimizer LLM examples of prior solutions to the
optimization task along with their objective values.</li>
<li>Asking the optimizer LLM to infer new / better solutions to the
problem.</li>
<li>Testing the inferred solutions via an evaluator LLM.</li>
</ol>
<figure>
<img src="/di-blog/2024/11/01/2024-November/automatic_prompt_opt.png"
alt="automatic_prompt_opt" />
<figcaption aria-hidden="true">automatic_prompt_opt</figcaption>
</figure>
<blockquote>
<p><strong>Prompting Guide 101 - Gemini for Google Workplace</strong>
[<a
target="_blank" rel="noopener" href="https://services.google.com/fh/files/misc/gemini-for-google-workspace-prompting-guide-101.pdf">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>GSM-Symbolic: Understanding the Limitations of Mathematical
Reasoning in Large Language Models</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.05229">Link</a>]</p>
</blockquote>
<p>Interesting findings on model variability: 1) Significant performance
variations when questions are rephrased or when only numerical values
are altered. 2) Models demonstrate robustness to superficial changes
(e.g., proper names) but are highly sensitive to numerical changes.</p>
<p>Interesting findings on model complexity and fragility: 1) Model
performance deteriorates as the number of clauses in a question
increases, revealing challenges with handling complexity, 2) Adding
irrelevant but seemingly relevant clauses leads to a performance drop of
up to 65% in some models.</p>
<p>Insights in reasoning: 1) The decline in performance suggests LLMs
rely on pattern matching rather than genuine logical reasoning, 2)
Models replicate training data patterns rather than solving problems
from first principles.</p>
<blockquote>
<p><strong>Thinking LLMs: General Instruction Following with Thought
Generation</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.10630">Link</a>]</p>
</blockquote>
<p>Current LLM has a problem of lacking internal reasoning processes
before outputting responses. Explicit thinking can enhance performance
on complex tasks, including creative writing and problem-solving, by
allowing models to internally reason and plan responses. The author
introduces Introduces Thought Preference Optimization (TPO) which allows
LLMs to generate multiple thought-response pairs for each instruction,
while a judge model evaluates responses, selecting the best and worst
pairs for optimization.</p>
<blockquote>
<p><strong>Agent-as-a-Judge: Evaluate Agents with Agents</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.10934">Link</a>]</p>
</blockquote>
<p>They introduced the Agent-as-a-Judge Framework to evaluate agentic
systems, addressing limitations of existing evaluation methods like
LLM-as-a-Judge by offering dynamic, step-by-step feedback throughout
task-solving processes.</p>
<blockquote>
<p><em>Difficulties handling numbers may stem from the fact that most
models rely on <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1801.10198">autoregressive
next token prediction pretext tasks</a> during training, which <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.06963">might not be suitable for
mathematical operations</a>, or simply because a limited number of
numerical reasoning tasks are included in the model’s training corpora.
Nevertheless, it is known that <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.05398">performance can be improved
using prompt techniques</a>, indicating that relevant knowledge may
already exist within LLMs.</em></p>
<p><strong>Evaluating and enhancing probabilistic reasoning in language
models - Google Research</strong> [<a
target="_blank" rel="noopener" href="https://research.google/blog/evaluating-and-enhancing-probabilistic-reasoning-in-language-models/">Link</a>]</p>
<p><strong>What Are the Odds? Language Models Are Capable of
Probabilistic Reasoning</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.12830">Link</a>]</p>
</blockquote>
<p>This study introduces a benchmark dataset with question-answer pairs
based on both idealized and real-world distributions. It enables
systematic evaluation of LLMs’ probabilistic reasoning capabilities
across three tasks: estimating percentiles, drawing samples, and
calculating probabilities.</p>
<blockquote>
<p><em>The technology has strikingly disparate effects across the
productivity distribution: while the bottom third of scientists see
little benefit, the output of top researchers nearly doubles.</em></p>
<p><em>Top scientists leverage their domain knowledge to prioritize
promising AI suggestions, while others waste significant resources
testing false positives.</em></p>
<p><em>82% of scientists report reduced satisfaction with their work due
to decreased creativity and skill underutilization.</em></p>
<p><strong>― Artificial Intelligence, Scientific Discovery, and Product
Innovation</strong> [<a
target="_blank" rel="noopener" href="https://aidantr.github.io/files/AI_innovation.pdf">Link</a>]</p>
</blockquote>
<p>MIT PhD Aidan Toner-Rodgers’s working paper talking about some very
interesting points. Indeed, AI has reshaped R&amp;D process especially
in natural science and material science where structured search is
required .e.g drug discovery, climatology, etc. However, scientists with
different degree of expertise (top and bottom scientists) achieve
drastically different productivity with AI, giving bottom scientists
less benefits. This characteristic has some consequences and
implications</p>
<ol type="1">
<li>Resources are misallocated to less promising AI suggestions. Human
innovation and creativity is not encouraged and cultivated.</li>
<li>Expertise is still required as AI only demonstrates its potential
when complemented by human expertise. The judgment ability in leveraging
AI’s potential is important.</li>
<li>Skills have been shifted to prompting AI effectively. However,
scientists feel an underutilization of expertise when working with
AI.</li>
</ol>
<blockquote>
<p><strong>A Survey on LLM-as-a-Judge</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15594">Link</a>]</p>
</blockquote>
<p>There are a lot of applications of LLM-as-a-Judge.</p>
<ol type="1">
<li>Data annotation: labeling datasets with information such as
sentiment, topic categorization, or relevance.</li>
<li>Content critique: providing feedback on generated content such as
articles, essays, or code.</li>
<li>Domain-specific evaluations: evaluate the accuracy, completeness,
and clarity of financial analyses or advice (in finance), and assess
medical responses for correctness, compliance with guidelines, and
patient safety (for medical Q&amp;A).</li>
</ol>
<blockquote>
<p><strong>Looking Inward: Language Models Can Learn About Themselves by
Introspection</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.13787">Link</a>]</p>
</blockquote>
<p>The researchers define introspection as “acquiring knowledge that is
not contained in or derived from training data but instead originates
from internal states”.</p>
<p>They conducted interesting experiments: finetuning LLMs to predict
properties of their own behavior in hypothetical scenarios. It turns out
that a LLM can predict itself better than other models predicting it,
even those models are trained on the same data pool.</p>
<p>Conclusion is suprising - language models have knowledge about
themselves that is neither contained in their training data nor
inferable from it. The researchers developed a self-prediction training
framework where models predict properties of their hypothetical
responses.There is already LLM research areas in honesty, behaviors,
etc. I believe this work is hugely contributing to these areas.</p>
<blockquote>
<p><strong>A Theoretical Understanding of Chain-of-Thought: Coherent
Reasoning and Error-Aware Demonstration</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.16540">Link</a>]</p>
</blockquote>
<p>Some interesting findings: 1) coherent CoT is better than traditional
CoT because the former considers the connections between steps, 2) model
is more sensitive to errors in intermediate reasoning steps than in the
final answer</p>
<p>The authors proposed an error aware training method which works to
incorporate both corretc and incorrect reasoning paths, enabling LLMs to
recognize and handle potential reasoning errors.</p>
<blockquote>
<p><em>Though businesses are doing their diligence on ROI and
customization, they may miss crucial pieces of the implementation
puzzle. Often, organizations discover too late that they’ve
underestimated the importance of technical integration, ongoing support,
and scalability. It’s a bit like buying a car based solely on fuel
efficiency, only to realize later that service availability and ease of
maintenance are just as critical over the long haul.</em></p>
<p><strong>― 2024: The State of Generative AI in the Enterprise</strong>
[<a
target="_blank" rel="noopener" href="https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/">Link</a>]</p>
</blockquote>
<p><strong>Key Trends for 2024 onwards</strong></p>
<ol type="1">
<li><p>There is a serious commitment from enterprise to AI integration
in business strategies</p></li>
<li><p>The top use cases for generative AI focus on enhancing
productivity and efficiency. These include:</p>
<ul>
<li>Code Copilots (51% adoption)</li>
<li>Support Chatbots (31% adoption)</li>
<li>Enterprise Search + Retrieval (28% adoption)</li>
<li>Data Extraction + Transformation (27% adoption)</li>
<li>Meeting Summarization (24% adoption)</li>
</ul></li>
<li><p>There’s a growing trend towards autonomous AI agents capable of
managing complex processes independently.</p></li>
<li><p>Businesses are focused on tools that deliver measurable value
(ROI) and industry-specific customization, rather than simply looking
for the cheapest option.</p></li>
<li><p>Industry-specific, verticalized AI applications are gaining
momentum, particularly in:</p>
<ul>
<li>Healthcare ($500 million in enterprise spending)</li>
<li>Legal ($350 million in enterprise spending)</li>
<li>Financial Services ($100 million in enterprise spending)</li>
<li>Media and Entertainment ($100 million in enterprise spending)</li>
</ul></li>
<li><p>Companies prefer multi-model strategies. This has led to a
decline in OpenAI’s dominance, while Anthropic is gaining market
share.</p></li>
<li><p>Retrieval-augmented generation (RAG) has become the dominant
design pattern, with 51% adoption. Meanwhile, agentic architectures are
emerging, now powering 12% of implementations.</p></li>
<li><p>There is a talent drought as AI engineering becoming more
sophisticated.</p></li>
<li><p>There’s a growing trend towards companies building their own AI
solutions in-house.</p>
<ul>
<li><p>Previously, in 2023, a large majority of enterprises (80%) relied
on third-party vendors for their generative AI software</p></li>
<li><p>In 2024, the split between building and buying is almost even,
with 47% of solutions developed internally and 53% sourced from
vendors</p></li>
<li><p>This shift suggests a growing confidence among enterprises in
their ability to develop and implement their own AI tools.</p></li>
<li><p>while there’s a trend towards building in-house solutions,
companies are not abandoning vendors entirely. The sources still
highlight the importance of vendors, especially for companies lacking
the resources or expertise for in-house development. The even split
between building and buying suggests a hybrid approach is emerging,
where companies strategically choose which solutions to develop
internally and which to procure from vendors.</p></li>
</ul></li>
</ol>
<h3 id="articles-and-blogs-1">Articles and Blogs</h3>
<blockquote>
<p><strong>Magentic-One: A Generalist Multi-Agent System for Solving
Complex Tasks - Microsoft Research</strong> [<a
target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/">Link</a>]</p>
</blockquote>
<p>Magentic-One is built on Microsoft’s AutoGen framework. It employs a
unique dual-loop architecture where the Orchestrator manages both task
and progress ledgers. This is an early movement of building generalist
agentic systems. Other current LLM-based applications like RAG will also
benefit from this type of system.</p>
<blockquote>
<p><strong>Introducing Internal Knowledge Search and Spaces -
Perplexity</strong> [<a
target="_blank" rel="noopener" href="https://www.perplexity.ai/hub/blog/introducing-internal-knowledge-search-and-spaces">Link</a>]</p>
</blockquote>
<p>Internal Knowledge Search and Spaces enable simultaneous searches of
organizational files and the web. This feature addresses the need for a
unified tool to access both internal and external data, leveraging
advanced LLMs like GPT-4 and Claude 3 to enhance search efficiency and
relevance.</p>
<blockquote>
<p><em>After the introduction of ChatGPT, there was a 21% decrease in
the weekly number of posts in automation-prone jobs compared to
manual-intensive jobs. Writing jobs were affected the most (30.37%
decrease), followed by software, app, and web development (20.62%) and
engineering (10.42%).</em></p>
<p><em>To stay competitive, employees must engage in continuous learning
and upskilling. In their book <a
target="_blank" rel="noopener" href="https://books.google.com/books?hl=en&amp;lr=&amp;id=8MBYEAAAQBAJ&amp;oi=fnd&amp;pg=PT6&amp;dq=Prediction+Machines:+The+Simple+Economics+of+Artificial+Intelligence&amp;ots=0EFecrXx9u&amp;sig=zWIxu15ECsiT0-efpz-t3rLulVc">Prediction
Machines</a>, authors Ajay Agrawal, Joshua Gans, Avi Goldfarb argue that
AI is shifting the focus of work away from predictive tasks to those
requiring human judgment and decision-making.</em></p>
<p><strong>― Research: How Gen AI Is Already Impacting the Labor Market
- Harvard Business Review</strong> [<a
target="_blank" rel="noopener" href="https://hbr.org/2024/11/research-how-gen-ai-is-already-impacting-the-labor-market">Link</a>]</p>
</blockquote>
<p>Research reveals impact of GenAI applications (ChatGPT and
image-generating AI) in jobs (manual intensive jobs such as data and
office management, video services, and audio services; automation prone
jobs such as writing, software, app, web dev, and engineering, and
image-generating jobs such as graphic design and 3D modeling ) to see
challenges and opportunities in shifting markets.</p>
<p>They found that Gen AI “led to nearly immediate decreases in posts
for online gig workers across job types, but particularly for
automation-prone jobs. “ It shows a growing trend of job
replacement.</p>
<p>Suggestions are continuous learning, enhancing human judgment and
decision making, to be able to ask right questions, prompt efficiently,
and avoid blindly taking responses.</p>
<blockquote>
<p><strong>How Much GPU Memory is Needed to Serve a Large Language Model
(LLM)?</strong> [<a
target="_blank" rel="noopener" href="https://masteringllm.medium.com/how-much-gpu-memory-is-needed-to-serve-a-large-languagemodel-llm-b1899bb2ab5d">Link</a>]</p>
</blockquote>
<p>Addresing a common LLM interview question “How much GPU memory is
needed to serve a Large Language Model (LLM)?”.</p>
<p><span class="math display">\[
M = ({P \times 4B \over {32/Q}}) \times 1.2
\]</span> where P is model size, 4B is 4 bytes used per paramter, Q is
the number of bits for loading the model (16 bit or 32 bit). 1.2
accounts for a 20% overhead.</p>
<figure>
<img src="/di-blog/2024/11/01/2024-November/gpu_for_llm.png"
alt="gpu_for_llm" />
<figcaption aria-hidden="true">gpu_for_llm</figcaption>
</figure>
<blockquote>
<p><strong>AI Agent Stack</strong> [<a
target="_blank" rel="noopener" href="https://www.linkedin.com/posts/charles-packer_introducing-the-ai-agents-stack-breaking-activity-7262857283871645699-ibWH/">Link</a>]</p>
</blockquote>
<figure>
<img src="/di-blog/2024/11/01/2024-November/ai_agent_stack.png"
alt="ai_agent_stack" />
<figcaption aria-hidden="true">ai_agent_stack</figcaption>
</figure>
<h3 id="github">GitHub</h3>
<blockquote>
<p><strong>OpenCoder: The Open Cookbook for Top-Tier Code Large Language
Models</strong> [<a
target="_blank" rel="noopener" href="https://opencoder-llm.github.io/">Link</a>]</p>
</blockquote>
<h3 id="news">News</h3>
<blockquote>
<p><strong>Introducing ChatGPT search - OpenAI</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/introducing-chatgpt-search/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Perplexity introduces AI-powered finance tool, offering
real-time stock analysis and historical data for developers.</strong>
[<a target="_blank" rel="noopener" href="https://www.perplexity.ai/finance/MU">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>VS Code now supports GitHub Copilot chat search and
visualization</strong> [<a
target="_blank" rel="noopener" href="https://link.alphasignal.ai/LVJVIX">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Cognitive Scientist Gary Marcus Says AI Must Be Regulated. He
Has a Plan</strong> [<a
target="_blank" rel="noopener" href="https://www.wsj.com/articles/cognitive-scientist-gary-marcus-says-ai-must-be-regulated-he-has-a-plan-f0c0b647?st=5CN9RT&amp;reflink=mobilewebshare_permalink">Link</a>]</p>
</blockquote>
<p>Among several points made by the article, two were caught by my
eyes:</p>
<ol type="1">
<li><p>Elon Musk presents a complex figure in the AI landscape, as one
of the first to issue warnings about potential risks of AI, and also
actively involved in developing AI through his company. This duality
raises questions about his stance on AI and how he reconciles his
concerns with his entrepreneurial pursuits.</p></li>
<li><p>Marcus proposes a shift from the current “System One” thinking in
AI, which is fast and reflexive but prone to errors, to a “System Two”
approach that emphasizes deliberate reasoning and abstraction.</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/10/21/Good-to-Great/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/10/21/Good-to-Great/" class="post-title-link" itemprop="url">Good to Great</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-21 23:02:57" itemprop="dateCreated datePublished" datetime="2024-10-21T23:02:57-04:00">2024-10-21</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure>
<img src="/di-blog/2024/10/21/Good-to-Great/good_to_great_book.png"
alt="good_to_great_book" />
<figcaption aria-hidden="true">good_to_great_book</figcaption>
</figure>
<p>“Good to Great: Why Some Companies Make the Leap…And Others Don’t”
written by Jim Collins - I started to read this book on Aug 24 and
recently finished it. It summarizes a research study uncovering patterns
and principles that differentiate “great” companies from the rest. I
find it also helpful for professional development. Two most impressive
concepts to me are “Level 5 Leadership” and “The Hedgehog Concept”.</p>
<h3 id="level-5-leadership">Level 5 Leadership</h3>
<p>There are five levels of leadership: 1) level 1 - highly capable
individual, 2) level 2 - contributing team member, 3) level 3 -
competent manager, 4) level 4 - effective leader, 5) level 5 - executive
leader.</p>
<p>Level 5 Leadership is the highest level and is marked by a
paradoxical blend of personal humility and professional will:</p>
<ul>
<li><strong>Personal Humility</strong>: Level 5 leaders are modest,
understated, and self-effacing. They rarely seek public attention and
often attribute success to others, to good luck, or to external factors.
They avoid the limelight and focus on the success of the organization
rather than their personal accolades.</li>
<li><strong>Professional Will</strong>: Despite their humility, these
leaders possess an intense resolve and determination to do whatever it
takes to make the company great. They are incredibly ambitious, but
their ambition is channeled toward the organization, not personal gain.
They set high standards and push the company toward greatness with
unwavering tenacity.</li>
</ul>
<p>Characteristics of level 5 leaders:</p>
<ul>
<li><strong>Focus on long-term success</strong>: They prioritize the
enduring success of the company rather than short-term wins or personal
gain.</li>
<li><strong>Credit to others</strong>: They credit the team, luck, or
external factors for successes but take personal responsibility for
failures or setbacks.</li>
<li><strong>Resolve in tough times</strong>: They confront difficult
realities head-on and have a steadfast determination to overcome
obstacles, never losing faith in the company’s ability to succeed.</li>
<li><strong>Succession planning</strong>: They ensure that the company
can continue its success without them, often preparing successors who
will carry the torch without a dip in performance.</li>
</ul>
<h3 id="the-hedgehog-concept">The Hedgehog Concept</h3>
<p>The Hedgehog Concept is a central idea in this book. It involves
identifying the intersection of three crucial areas. When a company
operates within this intersection, it can focus its efforts on what it’s
passionate about, what it can truly excel at, and what drives its
economic success. This creates a clarity of focus that allows a company
to ignore distractions and build sustained momentum.</p>
<p>It’s not only about company. Individuals can use it to guide their
career choices and personal development. By aligning your career or life
mission with your personal Hedgehog Concept, you’re more likely to find
purpose, fulfillment, and success. Think about the three questions:</p>
<ul>
<li><strong>What are you deeply passionate about?</strong> - your
personal mission, what gives you energy and a sense of fulfillment.</li>
<li><strong>What can you be the best in the world at?</strong> - your
unique strengths and abilities.</li>
<li><strong>What drives your economic engine?</strong> - how you can
generate income or provide value in a way that sustains your
livelihood.</li>
</ul>
<p>I’m glad that I found my answers to these three questions when I was
20 years old. I’m 100% sure that my answers won’t change through my
whole life no matter what happened or will happen. The answers in my
mind - I believe I’m born for it, it’s the mission of my life. So how
about you?</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/10/19/Advanced-RAG/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/10/19/Advanced-RAG/" class="post-title-link" itemprop="url">Advanced RAG</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-19 22:57:45" itemprop="dateCreated datePublished" datetime="2024-10-19T22:57:45-04:00">2024-10-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>There are many enterprise products built almost solely on RAG.</p>
<h2 id="naive-rag">Naive RAG</h2>
<p>The standard RAG workflow consists of three main steps as illustrated
in the graph below:</p>
<ol type="1">
<li><strong>Indexing</strong>: Creating an index of documents for
retrieval.</li>
<li><strong>Retrieval</strong>: Searching the index for relevant
documents based on a user query.</li>
<li><strong>Generation</strong>: Using a language model to generate
answers or responses based on the retrieved documents.</li>
</ol>
<p>The three steps all face possible issues:</p>
<ol type="1">
<li><p><strong>Indexing</strong>:</p>
<ul>
<li><p>Poor document parsing.</p></li>
<li><p>Inefficient document chunking strategies.</p></li>
<li><p>Weak semantic representations from embedding models.</p></li>
<li><p>Non-optimized index structures.</p></li>
</ul></li>
<li><p><strong>Retrieval</strong>:</p>
<ul>
<li><p>Low relevance: retrieved documents are not highly relevant to the
user query (low accuracy).</p></li>
<li><p>Incomplete retrieval: not all relevant documents are retrieved
(low recall).</p></li>
<li><p>Redundancy: retrieved documents may be repetitive or
redundant.</p></li>
<li><p>Queries are often not specific or well-defined.</p></li>
<li><p>Retrieval strategies might not be well-suited to the use case and
may rely solely on semantic similarity.</p></li>
</ul></li>
<li><p><strong>Generation</strong>:</p>
<ul>
<li>Overreliance on the retrieved content, leading to issues such as
irrelevant or even harmful responses (e.g., toxic or biased
content).</li>
</ul></li>
</ol>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/naive_rag.png"
alt="naive-rag" />
<figcaption aria-hidden="true">naive-rag</figcaption>
</figure>
<p>This paper <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.10997">“Retrieval-Augmented Generation
for Large Language Models: A Survey”</a> discussed several problems
associated with Naive RAG implementations. The advanced approaches to
RAG attempt to overcome the limitations of naive RAG by improving the
way queries are processed, documents are retrieved, and responses are
generated. Advanced RAG techniques focus on refining each step of the
process, from query transformations to more efficient retrieval
strategies.</p>
<h2 id="advanced-rag">Advanced RAG</h2>
<h3 id="overview">Overview</h3>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/adv_rag.png" alt="adv_rag" />
<figcaption aria-hidden="true">adv_rag</figcaption>
</figure>
<p>Source: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/concepts/">LangChain</a></p>
<h3 id="pre-retrieval-enhancements">Pre-Retrieval Enhancements</h3>
<h4 id="query-transformations-translation">Query Transformations /
Translation</h4>
<p>Query transformations are techniques aimed at re-writing or modifying
the input questions to improve the retrieval process.</p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/query_analysis.png"
alt="Query Analysis" />
<figcaption aria-hidden="true">Query Analysis</figcaption>
</figure>
<p>Query transformation types:</p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/query_trans2.png"
alt="query_trans2" />
<figcaption aria-hidden="true">query_trans2</figcaption>
</figure>
<p>Some notable methods include:</p>
<ol type="1">
<li><p><strong>Multi Query</strong>:</p>
<p>The <strong>MultiQueryRetriever</strong> automates prompt tuning by
using a language model (LLM) to generate multiple queries from different
perspectives for a given user query. It retrieves relevant documents for
each generated query and combines the results to create a larger, more
comprehensive set of potentially relevant documents. This technique
helps mitigate some of the limitations of distance-based retrieval, save
time on experimenting with different prompts, and provides a richer set
of results.</p>
<p>LangChain Tutorial: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/how_to/MultiQueryRetriever">How
to use MultiQueryRetriever</a>.</p>
<p>LangChain API: <a
target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html">MultiQueryRetriever</a>.</p>
<p>Video Tutorial: <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=JChPi0CRnDY">RAG from Scratch
(Part 5 - Query Translation: Multi Query)</a>.</p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/multiquery_retrieval.png"
alt="multiquery_retrieval" />
<figcaption aria-hidden="true">multiquery_retrieval</figcaption>
</figure></li>
<li><p><strong>RAG Fusion</strong></p>
<p>RAG-Fusion combines RAG and <strong>Reciprocal Rank Fusion
(RRF)</strong> by generating multiple queries, reranking them with
reciprocal scores and fusing the documents and scores. RRF gives the
more relevant retrieval results higher scores and re-ranks them
according to the scores. RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the
original query from various perspectives.</p>
<p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.03367">A New Take on
Retrieval-Augmented Generation</a>.</p>
<p>Code: <a href="Raudaschl/rag-fusion">Raudaschl/rag-fusion</a></p>
<p>LangChain Cookbook：<a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb?ref=blog.langchain.dev">RAG
Fusion</a></p>
<p>Video Tutorial: <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=77qELPbNgxA">RAG from scratch:
Part 6 (Query Translation – RAG Fusion)</a></p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/rag_fusion.png"
alt="rag_fusion" />
<figcaption aria-hidden="true">rag_fusion</figcaption>
</figure></li>
<li><p><strong>Step-Back Prompting</strong></p>
<p><strong>Step back prompting</strong> refers to the technique of
generating a more generalized or abstract version of a specific query in
order to mitigate potential issues with search quality or
model-generated responses. This involves first reformulating the initial
question into a broader or higher-level version (the “step back”
question) and then querying both the original and the generalized
question to improve the comprehensiveness and relevance of the
responses.</p>
<p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.06117">Take a Step Back:
Evoking Reasoning via Abstraction in Large Language Models</a>“.</p>
<p>LangChain Tutorial: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/use_cases/query_analysis/techniques/step_back/">Step
Back Prompting</a></p>
<p>LangChain Cookbook: <a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb">Step-Back
Prompting (Question-Answering)</a></p>
<p>Video Tutorial: <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xn1jEjRyJ2U">RAG from scratch:
Part 8 (Query Translation – Step Back)</a></p></li>
<li><p><strong>Decomposition</strong>:</p>
<p>When a user asks a complex question, a single query might not
retrieve the right results. To address this, the question can be broken
into sub-questions, each of which is retrieved separately, and the
answers are combined.</p>
<p>LangChain Doc: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/use_cases/query_analysis/techniques/decomposition/">Decomposition</a></p>
<p>Video Tutorial: <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=h0OPWlEOank">RAG from scratch:
Part 7 (Query Translation – Decomposition)</a></p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/decomposition.png"
alt="decomposition" />
<figcaption aria-hidden="true">decomposition</figcaption>
</figure>
<ul>
<li><p><strong>Least-to-Most Prompting</strong></p>
<p>The key idea in this strategy is to break down a complex problem into
a series of simpler subproblems and then solve them in sequence. Solving
each subproblem is facilitated by the answers to previously solved
subproblems. Least-to-most prompting is capable of generalizing to more
difficult problems than those seen in the prompts.</p>
<p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.10625">Least-to-Most
Prompting Enables Complex Reasoning in Large Language Models</a></p>
<p>Video Tutorial: <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=h0OPWlEOank">RAG from scratch:
Part 7 (Query Translation – Decomposition)</a></p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/least_to_most.png"
alt="least_to_most" />
<figcaption aria-hidden="true">least_to_most</figcaption>
</figure></li>
<li><p><strong>IR-Cot</strong></p>
<p>An approach for multi-step QA that interleaves retrieval with steps
(sentences) in a CoT, guiding the retrieval with CoT and in turn using
retrieved results to improve CoT. It incorporates the idea of
least-to-most prompting into RAG to improve retrieval, resulting in
factually more accurate CoT reasoning.</p>
<p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.10509">Interleaving
Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive
Multi-Step Questions</a></p>
<p>IR-CoT Code：<a
target="_blank" rel="noopener" href="https://github.com/StonyBrookNLP/ircot">https://github.com/StonyBrookNLP/ircot</a></p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/ircot2.png" alt="ircot2" />
<figcaption aria-hidden="true">ircot2</figcaption>
</figure>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/ircot3.png" alt="ircot3" />
<figcaption aria-hidden="true">ircot3</figcaption>
</figure></li>
</ul></li>
<li><p><strong>Hypothetical Document Embeddings (HyDE)</strong>: Given a
query, HyDE first zero-shot instructs an instruction-following language
model to generate a hypothetical document. The document captures
relevance patterns but is unreal and may contain false details. Then, an
unsupervised contrastively learned encoder (e.g. Contriever) encodes the
document into an embedding vector. This vector identifies a neighborhood
in the corpus embedding space, where similar real documents are
retrieved based on vector similarity.</p>
<p>Simply speaking, HyDE uses responses to retrieve documents rather
than using queries to retrieve documents. The rational behind this
approach is that the semantic similarity between query and real document
is smaller than the semantic similarity between hypothetical document
and real document.</p>
<p>LangChain Doc: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/use_cases/query_analysis/techniques/hyde/">Hypothetical
Document Embeddings</a></p>
<p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.10496.pdf">Precise
Zero-Shot Dense Retrieval without Relevance Labels</a></p>
<p>LangChain Cookbook: <a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb">Improve
document indexing with HyDE</a></p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/hyde.png" alt="hyde" />
<figcaption aria-hidden="true">hyde</figcaption>
</figure></li>
<li><p><strong>New queries based on historical dialogues</strong></p>
<p>This is a required technique for developing a chatbot or a
conversational RAG.</p>
<p>LangChain Tutorials: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/">Conversational
RAG</a>; <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/tutorials/chatbot/">Build a
Chatbot</a>; <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/how_to/message_history/">How
to add message history</a>; <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/">How
to add memory to chatbots</a></p>
<p>LangChain Code: <a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/history_aware_retriever.py">create_history_aware_retriever</a></p></li>
</ol>
<h4 id="query-construction">Query Construction</h4>
<p>Query construction refers to converting a natural language query into
the query language specific to the database you are working with. This
is essential for interacting with different databases and vector stores
that require structured queries for more efficient document
retrieval.</p>
<p>Check which vector databases support filtering: <a
target="_blank" rel="noopener" href="https://superlinked.com/vector-db-comparison">https://superlinked.com/vector-db-comparison</a></p>
<p>Data can be structured, unstructured or semi-structured (see demo
below). This requires LLMs to have capability of query construction.</p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/data_structure.png"
alt="data_structure" />
<figcaption aria-hidden="true">data_structure</figcaption>
</figure>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 23%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr>
<th>Examples</th>
<th>Data Source</th>
<th>References</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text-to-metadata-filter</td>
<td>VectorStore</td>
<td><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/self_query/">Docs</a></td>
</tr>
<tr>
<td>Text-to-SQL</td>
<td>SQL DB</td>
<td><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/tutorials/sql_qa/">Docs</a>; <a
target="_blank" rel="noopener" href="https://blog.langchain.dev/llms-and-sql/">Blog</a>; <a
target="_blank" rel="noopener" href="https://blog.langchain.dev/incorporating-domain-specific-knowledge-in-sql-llm-solutions/">Blog</a></td>
</tr>
<tr>
<td>Text-to-SQL + Semantic</td>
<td>PGVector supported SQL DB</td>
<td><a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/cookbook/retrieval_in_sql.ipynb?ref=blog.langchain.dev">Cookbook</a></td>
</tr>
<tr>
<td>Text-to-Cypher</td>
<td>Graph DB</td>
<td><a
target="_blank" rel="noopener" href="https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/">Blog</a>;
<a
target="_blank" rel="noopener" href="https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/">Blog</a></td>
</tr>
</tbody>
</table>
<ol type="1">
<li><p><strong>Self-query retriever</strong></p>
<p>A self-querying retriever is one that, as the name suggests, has the
ability to query itself. Specifically, given any natural language query,
the retriever uses a query-constructing LLM chain to write a structured
query (usually in JSON) and then applies that structured query to its
underlying VectorStore. This allows the retriever to not only use the
user-input query for semantic similarity comparison with the contents of
stored documents but to also extract filters from the user query on the
metadata of stored documents and to execute those filters.</p>
<p>LangChain Docs:</p>
<p>(v0.2): <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/how_to/self_query/">How to
do “self-querying” retrieval</a></p>
<p>(v0.1): <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/">Self-querying</a></p>
<p>Integration: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/integrations/components/">Components</a>
-&gt; <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/integrations/retrievers/">Retrievers</a>
-&gt; <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/integrations/retrievers/self_query/">Self-querying
retrievers</a> -&gt; <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/integrations/retrievers/self_query/qdrant_self_query/">Qdrant</a></p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/self_query.png"
alt="self_query.png" />
<figcaption aria-hidden="true">self_query.png</figcaption>
</figure>
<p><strong>Text-to-metadata-filter</strong>: VectorStores equipped with
metadata filtering enable structured queries to filter embedded
unstructured documents.</p></li>
<li><p><strong>Prompt templates and output parsers</strong></p>
<p><strong>Prompt analysis and prompt template</strong>: converting
user’s query to filtering conditions</p>
<ul>
<li><p>When constructing queries, the system uses a specific JSON format
to organize the query and filters. The prompt is designed to create
structured queries that can be applied to a document database or vector
store. The queries consist of two main components:</p>
<ul>
<li>Query: The natural language query string that is used to match the
document content.</li>
<li>Filter: Logical conditions used to filter the documents based on
specific metadata attributes.</li>
</ul></li>
<li><p>Comparison Operations</p>
<p>Comparison operators (<code>comp</code>) are used to compare
attributes (like year, name, time, product, or team) in the document
with specific values provided by the user. Here are the comparison
operators:</p>
<ul>
<li><strong>eq</strong>: Equals (e.g., <code>eq("team", "TSE")</code>
matches documents where the team is “TSE”).</li>
<li><strong>ne</strong>: Not equal (e.g.,
<code>ne("name","Ashley")</code> matches documents where the year is not
2022).</li>
<li><strong>gt</strong>: Greater than (e.g.,
<code>gt("year", 2023)</code> matches documents with a year greater than
2023).</li>
<li><strong>gte</strong>: Greater than or equal to (e.g.,
<code>gte("year", 2022)</code> matches documents from the year 2000 or
later).</li>
<li><strong>lt</strong>: Less than (e.g., <code>lt("year", 2021)</code>
matches documents created before 2021).</li>
<li><strong>lte</strong>: Less than or equal to (e.g.,
<code>lte("time", 13)</code> matches documents with a time length of 13
mins or lower).</li>
<li><strong>contain</strong>: Contains (e.g.,
<code>contain("product", "gold")</code> matches documents where the
product contains the word “gold”).</li>
<li><strong>like</strong>: Similar to or like (used for pattern
matching).</li>
</ul></li>
<li><p>Logical Operations</p>
<p>Logical operators combine multiple conditions (comparisons) into a
single filter:</p>
<ul>
<li><strong>and</strong>: Logical AND (e.g.,
<code>and(gt("year", 2022), eq("product", "gold"))</code> matches
documents created later than year 2022 and are related to gold card
product).</li>
<li><strong>or</strong>: Logical OR (e.g.,
<code>or(eq("team", "TS"), eq("team", "TSE"))</code> matches documents
that are either TS or TSE).</li>
<li><strong>not</strong>: Logical NOT (e.g.,
<code>not(eq("name", "Ashley"))</code> matches documents where Ashley is
not the owner).</li>
</ul></li>
</ul>
<p><strong>Output parser</strong>: This output parser can be used when
you want to return multiple fields or you need the response to be
formatted.</p>
<p>LangChain Docs: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/structured/">Structured
output parser</a></p>
<p>API: <a
target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html">StructuredQueryOutputParser</a></p></li>
</ol>
<h3 id="advanced-retrieval-techniques">Advanced Retrieval
Techniques</h3>
<ol type="1">
<li><p><strong>Vector Store-Backed Retriever</strong>: A retriever that
uses a vector database to store document embeddings and retrieve
documents based on their proximity to the query embedding.</p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/basic_index_retrieval.png"
alt="basic_index_retrieval" />
<figcaption aria-hidden="true">basic_index_retrieval</figcaption>
</figure></li>
<li><p><strong>Fusion Retrieval or hybrid search</strong>: Combining
multiple retrieval strategies (semantic similarity retrieval; keywords
retrieval) to obtain a more diverse set of results.</p>
<p>LangChain Docs:</p>
<p>v0.2: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/how_to/ensemble_retriever/">How
to combine results from multiple retrievers</a></p>
<p>v0.1: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/ensemble/">Ensemble
Retriever</a></p>
<p>API: <a
target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html">EnsembleRetriever</a></p>
<p>Code: <a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/retrievers/ensemble.py#L57">EnsembleRetriever</a></p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/fusion_retrieval.png"
alt="fusion_retrieval" />
<figcaption aria-hidden="true">fusion_retrieval</figcaption>
</figure>
<p>The <strong>EnsembleRetriever</strong> is a retrieval strategy that
enhances retrieval performance by combining multiple retrievers. This
approach leverages the strengths of different types of retrievers to
compensate for each other’s weaknesses. A common example is combining a
<strong>Sparse Retriever</strong> (e.g., BM25, which performs
keyword-based retrieval) with a <strong>Dense Retriever</strong> (which
performs semantic similarity retrieval based on embeddings). This
combination works because sparse and dense methods complement each
other.</p>
<p><strong>Sparse vs. Dense Representation</strong></p>
<ol type="1">
<li><strong>Sparse Representation</strong>:
<ul>
<li><strong>High-dimensional sparse vectors</strong>: Documents and
queries are represented as high-dimensional vectors, but most dimensions
have zero values. This is typical of traditional information retrieval
methods like TF-IDF and BM25.</li>
<li><strong>Term frequency</strong>: Each dimension corresponds to a
term, and the vector values represent term frequencies or weights (e.g.,
TF-IDF weights).</li>
<li><strong>Sparsity</strong>: Since a document or query contains only a
small subset of all possible terms, most dimensions in the vector are
zero, which makes it “sparse.”</li>
</ul></li>
<li><strong>Dense Representation</strong>:
<ul>
<li><strong>Low-dimensional dense vectors</strong>: Documents and
queries are represented as low-dimensional vectors, where most or all
dimensions have non-zero values. This representation is typically
generated by deep learning models like BERT.</li>
<li><strong>Semantic embeddings</strong>: The vectors capture semantic
and contextual information, rather than just term frequency.</li>
<li><strong>Density</strong>: All dimensions in the vector usually have
non-zero values, hence “dense.”</li>
</ul></li>
</ol>
<p><strong>Sparse and Dense Retrievers</strong></p>
<ul>
<li><strong>Sparse Retriever</strong>: The name comes from the fact that
most elements in the vector representation of documents and queries are
zero. It works well for exact keyword matches but may miss semantically
relevant content that uses different vocabulary.</li>
<li><strong>Dense Retriever</strong>: The name reflects that the vector
representation has mostly non-zero values. Dense retrievers perform
better at capturing the meaning behind the text and finding semantically
related content, even when the exact terms differ.</li>
</ul>
<p><strong>Combining Sparse and Dense Retrievers</strong></p>
<p>By combining sparse and dense retrievers, the
<strong>EnsembleRetriever</strong> can retrieve relevant documents more
effectively:</p>
<ul>
<li>The <strong>Sparse Retriever</strong> excels at matching specific
keywords or phrases.</li>
<li>The <strong>Dense Retriever</strong> is better at capturing the
semantic meaning and context, helping to retrieve documents even when
exact terms differ.</li>
</ul>
<p>This combination creates a more robust retrieval system, addressing
both lexical matches (through sparse retrieval) and semantic relevance
(through dense retrieval).</p>
<p>LangChain Doc: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/integrations/retrievers/bm25/">BM25
Retriever</a></p>
<p>API: <a
target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.bm25.BM25Retriever.html">BM25Retriever</a></p>
<p>Code: <a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/langchain%3D%3D0.2.1/libs/community/langchain_community/retrievers/bm25.py">BM25Retriever</a></p>
<p>Python Package: <a
target="_blank" rel="noopener" href="https://github.com/dorianbrown/rank_bm25">rank_bm25</a></p></li>
<li><p><strong>Sentence Window Retrieval</strong>: Retrieving extended
context pre and post the relevant context, rather than only retrieving
the relevant context, which can reduce information lost.</p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/sent_window_retrieval.png"
alt="sent_window_retrieval" />
<figcaption aria-hidden="true">sent_window_retrieval</figcaption>
</figure></li>
<li><p><strong>Parent Document Retrieval</strong>: Instead of sending
the multiple smaller chunks to the LLM, the system merges them into
their larger parent chunk. This allows for more contextualized
information to be fed to the LLM, giving it a broader and more coherent
set of data to generate an answer.</p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/parent_retrieval.png"
alt="parent_retrieval" />
<figcaption aria-hidden="true">parent_retrieval</figcaption>
</figure>
<p>LangChain Doc: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/parent_document_retriever/">Parent
Document Retriever</a></p>
<p>API: <a
target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html">ParentDocumentRetriever</a></p>
<p>Code: <a
target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/retrievers/parent_document_retriever.py#L10">ParentDocumentRetriever</a></p></li>
<li><p><strong>Hierarchical index retrieval</strong>: By structuring the
search in two layers—summaries for broad filtering and chunks for
detailed search—this hierarchical approach increases efficiency, making
it easier to find and synthesize relevant information, especially when
dealing with large document sets.</p>
<figure>
<img src="/di-blog/2024/10/19/Advanced-RAG/hierarchical_retrieval.png"
alt="hierarchical_retrieval" />
<figcaption aria-hidden="true">hierarchical_retrieval</figcaption>
</figure></li>
<li><p><strong>Hypothetical Questions</strong>: This technique involves
having the language model generate hypothetical questions for each chunk
of a document. These hypothetical questions are then embedded, and
retrieval is performed based on these question embeddings, improving the
relevance of the results.</p>
<p>LangChain Doc: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/multi_vector/#hypothetical-queries">hypothetical-queries</a></p></li>
<li><p><strong>MultiVector Retriever</strong>: MultiVector Retriever is
a higher level category of parent document retriever, hierarchical index
retrieval, and hypothetical questions.</p>
<p>LangChain Doc: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/multi_vector/">MultiVector</a></p>
<p>Summary: <a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/expression_language/interface/">Runnable
interface</a></p></li>
</ol>
<h3 id="post-retrieval-enhancements">Post-Retrieval Enhancements</h3>
<ol type="1">
<li><strong>Re-ranking</strong>: After retrieving the documents, the
system re-ranks or filters them to ensure that the most relevant results
appear at the top.</li>
</ol>
<h2 id="reference">Reference</h2>
<ol type="1">
<li><a
target="_blank" rel="noopener" href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">Advanced
RAG Techniques: an Illustrated Overview</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NirDiamant/RAG_Techniques">RAG System
Techniques</a></li>
<li><a
target="_blank" rel="noopener" href="https://python.langchain.com/v0.1/docs/get_started/introduction">LangChain</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/10/01/2024-October/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/10/01/2024-October/" class="post-title-link" itemprop="url">2024 October - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-01 21:54:26" itemprop="dateCreated datePublished" datetime="2024-10-01T21:54:26-04:00">2024-10-01</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="substack">Substack</h3>
<blockquote>
<p><em>This new <code>model.out_head</code> output layer has its
<code>requires_grad</code> attribute set to <code>True</code> by
default, which means that it’s the only layer in the model that will be
updated during training. Technically, training the output layer we just
added is sufficient. However, as I found in experiments, finetuning
additional layers can noticeably improve the predictive performance of
the finetuned model.</em></p>
<p><strong>― Building A GPT-Style LLM Classifier From Scratch -
Sebastian Raschka</strong> [<a
target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/building-a-gpt-style-llm-classifier">Link</a>]
[<a
target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb">Github</a>]</p>
</blockquote>
<p>Interesting questions addressed by Sebastian:</p>
<ol type="1">
<li><p>Do we need to train all layers?</p>
<p>“For classification finetuning, it is not necessary to update all
layers in an LLM. (The fewer weights we update, the faster the training
will be because we don’t need to compute the gradients for these weights
during backpropagation.)”</p></li>
<li><p>Why finetuning the last token, not the first token?</p>
<p>“In contrast to BERT, GPT is a decoder-style model with a causal
attention mask. This means the first token has no context information of
any other token in the input. Only the last token has information about
all other tokens. Hence, if we want to use models like GPT for
classification finetuning, we should focus on the last token to capture
contextual information of all other input tokens.”</p></li>
<li><p>How does BERT compare to GPT performance-wise?</p>
<p>“The small GPT-2 model from the previous section and BERT performed
similarly well on the spam classification dataset. “</p></li>
<li><p>Should we disable the causal mask?</p>
<p>“A core feature of the GPT architecture is the causal attention mask
(different from BERT models or the original transformer architecture).
However, we could actually remove the causal mask during classification
finetuning, which would allow us to finetune the first rather than the
last token since future tokens will no longer be masked, and the first
token can see all other tokens.”</p></li>
<li><p>What impact does increasing the model size have?</p>
<p>The prediction accuracy can improve significantly with larger
models.</p></li>
<li><p>What improvements can we expect from LoRA?</p>
<p>Both full finetuning (all layers) and LoRA can result in the same
test set performance.</p>
<p>“On the small model, LoRA is slightly slower since the additional
overhead from adding LoRA layers may outweigh the benefits, but when
training the larger 1.5 billion parameters model, LoRA trains 1.53x
faster.”</p></li>
<li><p>Padding or no padding? [<a
target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch06/02_bonus_additional-experiments">experiments</a>]</p>
<p>“If we want to process data in batches during training or inference
(this involves processing more than one input sequence at a time), we
need to insert padding tokens to ensure that the training examples are
of equal length.</p>
<p>In regular text generation tasks, padding doesn’t affect the model
response since padding tokens are usually added to the right side, and
due to the causal mask discussed earlier, these padding tokens don’t
influence the other tokens. However, remember that we finetuned the last
token, as discussed earlier. Since the padding tokens are to the left of
this last token, the padding tokens may affect the result. “</p></li>
</ol>
<blockquote>
<p><strong>These Are The 6 Best Science-Based Study Strategies - Super
Learning Lab</strong> [<a
target="_blank" rel="noopener" href="https://axelcasas.substack.com/p/these-are-the-6-best-science-based">Link</a>]</p>
</blockquote>
<ol type="1">
<li><p>Spaced Practice</p>
<p>Instead of cramming all the information at once, spaced practice
consists of revisiting the material multiple times with breaks in
between.</p></li>
<li><p>Interleaving</p>
<p>This is about studying different topics in a sequence.</p></li>
<li><p>Retrieval</p>
<p>This consists of bringing learned information from mid to long-term
memory by recall or retrieval practices.</p></li>
<li><p>Elaboration</p>
<p>Elaborative interrogation consists of asking and explaining why and
how things work based on prior knowledge. In other words, it involves
connecting new information to preexisting knowledge.</p></li>
<li><p>Concrete Example</p>
<p>When learning abstract concepts it was found that illustrating these
topics with specific examples improves learning.</p></li>
<li><p>Dual Coding</p>
<p>Dual coding is about combining words with visuals. If you use
relevant and helpful images in your notes, you may increase learning by
remembering what you study with the help of these images.</p></li>
</ol>
<blockquote>
<p><em>The <span class="math inline">\(\$120\)</span> billion wagered on
sports betting in America in 2023 translated into nearly <span
class="math inline">\(\$11\)</span> billion in revenue for sports
betting companies. This corresponds to the ~9% fee sportsbooks keep
after all bets have been settled.</em></p>
<ul>
<li><em>Flutter: Leverages FanDuel’s dominance and global
expertise.</em></li>
<li><em>DraftKings: Focuses on innovation and user engagement to fuel
growth.</em></li>
<li><em>Entain: Bets on BetMGM’s success in the US market.</em></li>
<li><em>Penn: Leverages the ESPN partnership to challenge established
players.</em></li>
</ul>
<p><strong>― Sports Betting Economics - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/sports-betting-economics">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>For decades, companies have outsourced their organizational
innovation to consultants or enterprise software vendors who develop
generalized approaches based on what they see across many organizations.
That won’t work here, at least for a while. Nobody has special
information about how to best use AI at your company, or a playbook for
how to integrate it into your organization.</em></p>
<p><strong>― AI in organizations: Some tactics - One Useful
Thing</strong> [<a
target="_blank" rel="noopener" href="https://www.oneusefulthing.org/p/ai-in-organizations-some-tactics">Link</a>]</p>
</blockquote>
<p>Issues with AI at the organizational level and how to solve them.</p>
<ol type="1">
<li>In many companies, there is little AI use and few productivity gains
outside of narrow permitted use cases. That’s because AI use that boosts
individual performance does not always translate to boosting
organizational performance for a variety of reasons. To get
organizational gains requires R&amp;D into AI use and you are largely
going to have to do the R&amp;D yourself.</li>
<li>“Many key breakthrough innovations come not from central R&amp;D
labs, but from people actually using products and tinkering with them to
solve their own problems. “ (Prof. Eric von Hippel). As users are very
motivated to make their own jobs easier with technology, they find ways
to do so. The user advantage is especially big in experimenting with
Generative AI because the systems are unreliable and have a jagged
frontier of capability. People are experimenting with AI and finding it
very useful. But they aren’t sharing their results with their
employers.</li>
</ol>
<p>How to solve the issues? What are the tactics? Talents in the lab
should focus on building, not analysis or abstract strategy.</p>
<ul>
<li>Build AI benchmarks for your organization. [<a
target="_blank" rel="noopener" href="https://docs.anthropic.com/en/docs/build-with-claude/develop-tests">Anthropic’s
guide to benchmarking</a>]</li>
<li>Build prompts and tools that work.</li>
<li>Build stuff that doesn’t work… yet.</li>
<li>Build provocations and magic.</li>
</ul>
<blockquote>
<p><strong>The USA vs Visa - Net Interest</strong> [<a
target="_blank" rel="noopener" href="https://www.netinterest.co/p/the-usa-vs-visa">Link</a>]</p>
</blockquote>
<p>Key elements of Doha Mekki’s recent antitrust lawsuit against
Visa:</p>
<ol type="1">
<li>Visa controls over 60% of U.S. debit transactions, with Mastercard
far behind at 25%.</li>
<li>Visa traps merchants with pricing that penalizes them if they don’t
process all transactions through Visa.</li>
<li>Exclusive deals incentivize merchants to use Visa exclusively,
reducing competition.</li>
<li>Visa prevents potential competitors like PayPal and Apple from
entering the market by locking them into restrictive agreements.</li>
<li>Visa has faced antitrust lawsuits since 1971 and maintains a large
legal team to manage ongoing cases.</li>
</ol>
<blockquote>
<p><em>In physics, we study how particles or systems’ units interact and
evolve toward stable states. In machine learning, we study how neurons
(or artificial neurons) interact to learn patterns directly from data.
The connection lies in energy minimization: both approaches define an
energy function to describe the stability of a system, and the
optimization of this function helps to find optimal configurations that
correspond to useful patterns or memories.</em></p>
<p><em>Hopfield developed a network that recreates patterns using energy
minimization, while Hinton expanded on this with the introduction of
Boltzmann machines, statistical physics-based systems that learn to
recognize and generate patterns, providing groundwork for modern machine
learning.</em></p>
<p><strong>― Nobel Prize to the Statistical Physics of artificial neural
networks - Complexity Thoughts</strong> [<a
target="_blank" rel="noopener" href="https://manlius.substack.com/p/nobel-prize-to-the-statistical-physics">Link</a>]</p>
</blockquote>
<p>“The laws governing physical systems also apply to the world of
artificial intelligence.”</p>
<blockquote>
<p><strong>Major AI Functionalities / with Apps - AI Supremacy</strong>
[<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/major-ai-functionalities-with-apps">Link</a>]</p>
</blockquote>
<p>A list of AI products you can experiment with.</p>
<blockquote>
<p><em>Fine-tuning can be useful for certain tasks (see the relevant
section here for more details), but when it comes to injecting morality
into your LLM, it’s probably not a good bet.</em></p>
<p><em>By combining the strengths of diffusion models and
auto-regressive generation, DGLM offers a more nuanced, adaptable, and
potentially more effective approach to generating safe and creative
text. It moves away from the brute-force, one-size-fits-all approach of
fine-tuning and embraces a more modular, dynamic, and personalized
approach to AI safety.</em></p>
<p><strong>― A New Way to Control Language Model Generations
[Breakdowns] - Artificial Intelligence Made Simple</strong> [<a
target="_blank" rel="noopener" href="https://artificialintelligencemadesimple.substack.com/p/a-new-way-to-control-language-model">Link</a>]</p>
</blockquote>
<p>The author lists drawbacks of fine tuning:</p>
<p>“A model’s knowledge and capabilities are learnt almost entirely
during pretraining, while alignment teaches it which subdistribution of
formats should be used when interacting with users.” - LIMA: Less Is
More for Alignment [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.11206">Link</a>]</p>
<p>“Our findings reveal that while unsupervised fine-tuning offers some
improvement, RAG consistently outperforms it, both for existing
knowledge encountered during training and entirely new knowledge.
Moreover, we find that LLMs struggle to learn new factual information
through unsupervised fine-tuning.” - Fine-Tuning or Retrieval? Comparing
Knowledge Injection in LLMs [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.05934">Link</a>]</p>
<p>“Disconcertingly, our research also reveals that, even without
malicious intent, simply fine-tuning with benign and commonly used
datasets can also inadvertently degrade the safety alignment of LLMs,
though to a lesser extent. These findings suggest that fine-tuning
aligned LLMs introduces new safety risks that current safety
infrastructures fall short of addressing — — even if a model’s initial
safety alignment is impeccable” - Fine-tuning Aligned Language Models
Compromises Safety, Even When Users Do Not Intend To! [<a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=hTEGyKf0dZ">Link</a>]</p>
<p>“The base model generates a wide range of nationalities, with
American, British, and German being the top three. In contrast, the
aligned model only generates three nationalities: American (highest
percentage), Chinese, and a small percentage of Mexican.” - Creativity
Has Left the Chat: The Price of Debiasing Language Models [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05587">Link</a>]</p>
<blockquote>
<p><strong>Just do it! Brand Name Lessons from Nike’s Troubles - Musings
on Markets</strong> [<a
target="_blank" rel="noopener" href="https://aswathdamodaran.substack.com/p/just-do-it-brand-name-lessons-from">Link</a>]</p>
</blockquote>
<p>Brand value is often mixed with other advantages like scale, network
effects, and product differentiation. Strong brands yield higher
revenues, pricing power, and potentially lower capital costs. And it’s
hard to separate brand value in companies with multiple advantages.</p>
<blockquote>
<p><strong>Spotify: Layoffs Pay Off - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/spotify-layoffs-pay-off">Link</a>]</p>
</blockquote>
<p>Key business highlights: 1) Spotify’s new subscription plans -
Spotify’s Audiobooks Access ( <span
class="math inline">\(\$9.99\)</span> per month for 15 hours) and Basic
(removing audiobooks from Premium) diversify its offerings, 2) Spotify
is incorporating social-style discovery features, like Live Listening
Parties and prompt-based AI playlists, but remains behind YouTube and
Meta in algorithm sophistication and live content, 3) Spotify’s
ad-supported ARPU is low (€1.15 vs. Meta’s <span
class="math inline">\(\$11.89\)</span> dollars in Q2), limiting ad
revenue potential. A new in-house creative agency may improve brand
experiences but is challenging to scale profitably, 4) Spotify pivoted
from broad podcasting investments to a case-by-case approach, now
pushing video podcasts.</p>
<p>Competitiveness: 1) Hub Entertainment Research shows Spotify’s high
‘must-have’ appeal, with 75% of US users viewing it as “uncancellable.”
This loyalty supports Spotify’s growing free cash flow and a valuation
around 40 times Free Cash Flow (FCF) —placing it ahead of rivals like
YouTube Music (100M subscribers) and Apple Music (estimated 110M by
2025), 2) Despite solid growth, Spotify’s reliance on licensed content
and its still-limited ad revenue leave room for competition. While
TikTok’s 1B+ users could funnel into TikTok Music, ByteDance recently
announced it will close TikTok Music by November, focusing instead on
promoting artists and streaming value within the main app—a potential
competitive break for Spotify.</p>
<blockquote>
<p><strong>Cybersecurity Earnings - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/cybersecurity-earnings-7f6">Link</a>]</p>
</blockquote>
<p>Covered Palo Alto Networks, CrowdStrike, Fortinet, Zscaler, and
Cloudflare.</p>
<blockquote>
<p><strong>Two Nobel Prizes for AI, and Two Paths Forward - Marcus on
AI</strong> [<a
target="_blank" rel="noopener" href="https://garymarcus.substack.com/p/two-nobel-prizes-for-ai-and-two-paths">Link</a>]</p>
</blockquote>
<p>Hinton’s focus on end-to-end neural networks can be limiting,
especially when considering the complexities of real-world problems that
often require more structured and hybrid approaches. On the other hand,
Hassabis’s embrace of neurosymbolic AI reflects an openness to different
methodologies and a recognition that a combination of techniques may
yield better results.</p>
<blockquote>
<p><em>First, Waymo is using transformer-based foundation models for all
stages of its self-driving pipeline: perception, prediction, and
planning. Second, the whole system is trained end to end. During
training, gradients from the behavior network propagate backwards to the
perception network.</em></p>
<p><em>So I see more similarities than differences in the evolution of
Waymo and Tesla’s self-driving software. Both companies made little to
no use of neural networks in their early systems. Both companies started
using neural networks for perception in the late 2010s. And both
companies only recently shifted to end-to-end architectures that used
neural networks for all stages of the self-driving pipeline.</em></p>
<p><strong>― Elon Musk wants to dominate robotaxis—first he needs to
catch up to Waymo - Understanding AI</strong> [<a
target="_blank" rel="noopener" href="https://www.understandingai.org/p/elon-musk-wants-to-dominate-robotaxisfirst">Link</a>]</p>
</blockquote>
<p>Tesla’s advantages compared to Waymo:</p>
<ol type="1">
<li>Tesla already has millions of vehicles on the road, which could
quickly deploy robotaxi software without the need for new hardware.</li>
<li>Tesla relies on cost-effective, camera-based perception without
expensive sensors like lidar, which Waymo uses. This could lower Tesla’s
per-vehicle cost and allow it to expand more rapidly if autonomy is
achieved.</li>
<li>Tesla’s transition to a full, end-to-end neural network approach for
perception, prediction, and planning has improved FSD’s ability to
handle complex driving situations without manual coding for specific
scenarios.</li>
</ol>
<p>Tesla’s disadvantages compared to Waymo:</p>
<ol type="1">
<li>Tesla hasn’t deployed a fully driverless car yet, while Waymo has
offered driverless rides since 2020.</li>
<li>Tesla would need extensive infrastructure to maintain a robotaxi
network (for charging, cleaning, and repairs) which it currently lacks.
Building this up in cities nationwide would take time, resources, and
logistical planning.</li>
<li>Tesla’s camera-only approach may struggle in certain conditions
(e.g., low visibility), which lidar could handle better.</li>
</ol>
<blockquote>
<p><em>Key Contribution of AI to Robotics:</em></p>
<ol type="1">
<li><em><strong>Improved Reasoning and Planning</strong>: Large language
models, like those developed by OpenAI and Google, are enabling robots
to interpret high-level commands, understand contextual instructions,
and execute complex, multi-step tasks. This is particularly valuable in
dynamic environments where robots must adapt to unforeseen changes and
make real-time decisions.</em></li>
<li><em><strong>Enhanced Visual and Motor Coordination</strong>: The
integration of generative AI with visual and motor feedback systems
allows robots to translate visual inputs into precise motor actions.
This enables robots to perform tasks such as picking and placing objects
with greater accuracy and efficiency, even in environments that are
constantly changing.</em></li>
<li><em><strong>Natural Language Interfaces</strong>: AI-driven natural
language interfaces are making it easier for users to interact with
robots using everyday language rather than programming code. This
democratization of robotics makes it accessible to non-technical users,
paving the way for broader adoption across industries.</em></li>
<li><em><strong>Predictive Maintenance</strong>: AI models analyze
real-time data from robots to predict potential malfunctions, enabling
proactive maintenance that minimizes costly downtime and enhances
operational efficiency.</em></li>
</ol>
<p><strong>― Generative AI and Robotics in 2024 - AI Supremacy</strong>
[<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/generative-ai-and-robotics-in-2024">Link</a>]</p>
</blockquote>
<blockquote>
<ol type="1">
<li><p><em><strong>Glue:</strong> The less-glamorous stuff that helps a
team succeed</em></p></li>
<li><p><em>Strike the right balance between glue and core work. Do
enough glue work to show leadership promotable artifacts, but not too
much to where your core work suffers.</em></p></li>
<li><p><em>Lead meetings, take notes, and share them to provide value to
the right stakeholders.</em></p></li>
<li><p><em>Send your manager monthly recaps of your accomplishments so
they can more easily sponsor you and your work.</em></p></li>
<li><p><em><strong>Grit:</strong> The will to pursue a long-term goal
despite challenges</em></p></li>
<li><p><em>Break your projects into achievable milestones so you can
constantly feel progress, even for long projects.</em></p></li>
<li><p><em>View failures as progress. It’s one less route you need to
explore now.</em></p></li>
<li><p><em>Take breaks and work in fun. I set up icebreakers at the
start of our meetings, organized team events, and pushed for production
freezes.</em></p></li>
<li><p><em><strong>Friction:</strong> The gap between reality and the
ideal state</em></p></li>
<li><p><em>Find ways to unblock yourself and the people around you. Do
this enough, and you’ll have mastered removing friction.</em></p></li>
<li><p><em>Removing friction paints you as a force multiplier. Force
multipliers get promoted.</em></p></li>
</ol>
<p><strong>― 3 Career Principles that got me to Director at Google -
High Growth Engineer</strong> [<a
target="_blank" rel="noopener" href="https://read.highgrowthengineer.com/p/3-career-principles-to-director-at-google">Link</a>]</p>
</blockquote>
<blockquote>
<ul>
<li><em>Processing Fluency: The ease with which information is perceived
and processed by the human mind. High fluency can lead to positive
evaluations, even if the information is not accurate.</em></li>
<li><em>Halo Effect: A cognitive bias where a positive overall
impression of something (e.g., an LLM’s fluency) influences the
evaluation of its individual attributes (e.g., truthfulness).</em></li>
<li><em>Inter-rater Agreement (IRA): A measure of how much two or more
evaluators agree on their assessments. Low IRA indicates potential
problems with the evaluation design or guidelines.</em></li>
<li><em>Extrinsic Evaluation: Assessing the impact of an LLM’s output on
an end-user task or system (e.g., measuring productivity gains from
using an LLM-powered email assistant).</em></li>
<li><em>Intrinsic Evaluation: Evaluating the properties of the
LLM-generated text itself, such as fluency, coherence, and factual
accuracy.</em></li>
</ul>
<p><strong>― How Amazon is Rethinking human evaluation for generative
large language models [Breakdowns] - Artificial Intelligence Made
Simple</strong> [<a
target="_blank" rel="noopener" href="https://artificialintelligencemadesimple.substack.com/p/how-amazon-is-rethinking-human-evaluation">Link</a>]</p>
</blockquote>
<p>Pretty comprehensive of human evaluation of Gen AI models.</p>
<blockquote>
<p><strong>US Banks: Soft Landing? - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/us-banks-soft-landing">Link</a>]</p>
</blockquote>
<blockquote>
<p>*A recent pre-print paper found that <strong><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.08044v1">at least 5% of new Wikipedia
articles</a></strong> in August 2024 were AI generated, Facebook isn’t
doing anything to stop <strong><a
target="_blank" rel="noopener" href="https://futurism.com/hurricane-scam-ai-slop">AI generated images
of Hurricane Helene</a></strong>, and Goodreads and Amazon are grappling
with <strong><a
target="_blank" rel="noopener" href="https://www.wired.com/story/scammy-ai-generated-books-flooding-amazon/">various
AI generated book schemes</a></strong> scamming people into buying pulp.
It’s only the tip of the iceberg.*</p>
<p><strong>― When Models Go MAD - Teaching computers how to
talk</strong> [<a
target="_blank" rel="noopener" href="https://jurgengravestein.substack.com/p/when-models-go-mad">Link</a>]</p>
</blockquote>
<p>Humm AI is diluting reality. It’s concerning.</p>
<blockquote>
<p><strong>Elon Musk’s tech projects are inseparable from his
authoritarian one - Blood In The Machine</strong> [<a
target="_blank" rel="noopener" href="https://www.bloodinthemachine.com/p/elon-musks-tech-projects-are-inseparable">Link</a>]</p>
</blockquote>
<p>Good discussion. Musk has multifaceted role—entrepreneur, political
influencer, and media magnate. His activities underscore a model of
influence that defies precedent, blending financial might, technological
ambition, and political maneuvering. Recognizing the interconnectedness
of these efforts is crucial for understanding Musk not just as a
private-sector innovator but as a power broker actively shaping the
public and political spheres in ways that could redefine norms, values,
and who gets to participate in his envisioned future.</p>
<h3 id="youtube-and-podcast">YouTube and Podcast</h3>
<blockquote>
<p><em>Tesla I don’t think it’s a car company, I think this is
misleading, this is a robotics company robotics at Scale Company,
because I would say at scale is also like a whole separate variable,
they’re not building a single thing, they’re building the machine that
builds the thing which is a whole separate thing and so I think robotics
at scale company is what Tesla is.</em></p>
<p><em>I think with synthetic data you just have to be careful, because
these models are silently collapsed, is like one of the major issues so
if you go to ChatGPT and you ask it to give you a joke, you’ll notice
that it only knows three jokes, that’s the only it gives you like one
joke I think most of the time. And sometimes it gives you like three
jokes and it’s because the models are collapsed and it’s silent, so when
you’re looking at any single individual output, you’re just seeing a
single example, but when you actually look at the distribution, you’ll
notice that it’s not a very diverse distribution, it’s silently
collapsed. When you’re doing synthetic data generation, this is a
problem, because you actually really want that entropy, you want the
diversity, and the richness in your data set otherwise.</em></p>
<p><strong>― No Priors Ep. 80 | With Andrej Karpathy from OpenAI and
Tesla - No Priors: AI, Machine Learning, Tech &amp; Startups</strong>
[<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=hM_h0UA7upI&amp;ab_channel=NoPriors:AI,MachineLearning,Tech,&amp;Startups">Link</a>]</p>
</blockquote>
<p>Andrej Karpathy was a founding team member of OpenAI and the former
Tesla Autopilot leader. He discussed the evolution of self driving
cards, tech challenges, Tesla’s Optimus humanoid robot, bottlenecks of
AI development today. The topic of how AI capabilities could be further
integrated with human cognition sounds very future and funny.</p>
<blockquote>
<p><strong>AI prompt engineering: A deep dive - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=T9aRN5JkmL8&amp;ab_channel=Anthropic">Link</a>]</p>
</blockquote>
<p>Some of Anthropic’s prompt engineering specialists—Amanda Askell
(Alignment Finetuning), Alex Albert (Developer Relations), David Hershey
(Applied AI), and Zack Witten (Prompt Engineering)—share their insights
on the evolution of prompt engineering, offer practical advice, and
discuss how prompting could evolve as AI continues to advance.</p>
<blockquote>
<p><strong>Decoding Google Gemini with Jeff Dean - Google
DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=lH74gNeryhQ&amp;ab_channel=GoogleDeepMind">Link</a>]</p>
</blockquote>
<p>Jeff Dean, chief scientist of Google DeepMind and Google Research,
discusses the past, present and future of AI, specially the long term
potential of multi-modal models like Gemini.</p>
<blockquote>
<p><strong>Shall We Repeal the Laws of Economics? - Oaktree
Capital</strong> [<a
target="_blank" rel="noopener" href="https://www.oaktreecapital.com/insights/memo-podcast/shall-we-repeal-the-laws-of-economics">Link</a>]</p>
</blockquote>
<p>Howard Marks addresses how politicians often ignore economic reality
in their campaign promises, using examples like Trump’s call for tariffs
and Harris’s attack on grocery profiteering. He emphasizes that economic
laws are incontrovertible, and politicians can’t deliver on promises
that contradict these laws; free markets allocate resources efficiently.
And he highlights the ongoing political refusal to address issues like
Social Security insolvency and national debt, stating that ignoring
economic laws will eventually lead to negative outcomes.</p>
<blockquote>
<p><strong>Introducing OpenAI o1</strong> <strong>- Open AI</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLOXw6I10VTv_T9QV-DKXhq7HFUQRkGQLI">Link</a>]</p>
</blockquote>
<p>A series of video from Open AI to introduce GPT o1.</p>
<blockquote>
<p><strong>Ep17. Welcome Jensen Huang | BG2 w/ Bill Gurley &amp; Brad
Gerstner - Bg2 Pod</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=bUrCR4jQQg8&amp;ab_channel=Bg2Pod">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Dueling Presidential interviews, SpaceX’s big catch,
Robotaxis, Uber buying Expedia?, Nuclear NIMBY - All-In Podcast</strong>
[<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ye012kzWJ3A&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Meta VS Apple: What Their Battle Means For AI Startups - Y
Combinator</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=NhrkiK1SggE&amp;t=9s&amp;ab_channel=YCombinator">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The Waymo Way: Making Autonomous Driving a Reality | Dmitri
Dolgov - U-M Computer Science and Engineering</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=s_wGhKBjH_U&amp;t=2227s&amp;ab_channel=U-MComputerScienceandEngineering">Link</a>]</p>
</blockquote>
<figure>
<img src="/di-blog/2024/10/01/2024-October/waymo_foundation_model.png"
alt="waymo_foundation_model" />
<figcaption aria-hidden="true">waymo_foundation_model</figcaption>
</figure>
<blockquote>
<p><strong>Sam Altman: The Man Behind ChatGPT | Big Take - Bloomberg
Podcasts</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?app=desktop&amp;v=m_oQrKfvSOw&amp;ab_channel=BloombergPodcasts">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Markets turn Trump, Long rates spike, Election home stretch,
Influencer mania, Saving Starbucks - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4ruAqXfK6Ao&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>What’s next for AI agentic workflows ft. Andrew Ng of AI Fund
- Sequoia Capital</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=sal78ACtGTc&amp;ab_channel=SequoiaCapital">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>A fireside chat with Sam Altman OpenAI CEO at Harvard
University - Harvard Business School</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=FVRHTWWEIz4&amp;ab_channel=HarvardBusinessSchool">Link</a>]</p>
</blockquote>
<p>Q1: I’m curious how you rebuilt or rediscover momentum after pivoting
in a professional or personal sense in terms of your values.</p>
<ul>
<li>We have unbelievable greatest technological wave ever, so it’s a
great time to be starting out your career. You are going to be flooded
with opportunities for the next few years.</li>
</ul>
<p>Q2: How do you think AI’s role will be in tackling sort of
inequalities in education and health care and legal stuff?</p>
<ul>
<li>It should reduce inequality.</li>
</ul>
<p>Q3: Subscription model could be a barrier for startup and small
businesses, do you think OpenAI would explore alternative monetization
strategy that could include free API access, perhaps supported by
advertising or other methods, to foster innovation in the future?</p>
<ul>
<li>Sam hates ads in general. Ads + AI is uniquely unsettling to him. He
likes the simplicity of OpenAI’s model, which is they make great AI and
people pay them for it, and they just do the best they can for
people.</li>
</ul>
<p>Q4: Does the increasing competitors change the way you are evolving
for the next products?</p>
<ul>
<li>They are just trying to figure out the next paradigm and the next
great idea. They don’t pay much attention to the market share though
they pay at least attention to competitors to maybe get
inspiration.</li>
<li>Sam’s hope is that every year, they do something amazing that people
thought was impossible. Once you know that something is possible and
roughly how to do it, it always gets copied quickly. That’s not the hard
part. The hard part is figuring out what it is and doing it first when
you don’t know it’s possible.</li>
</ul>
<p>Q5: what do you think the ideal public general education curriculum
on AI should look like and what does the average person need to know
about AI in the next 5-10 years?</p>
<ul>
<li>Computer science major or at least some courses. Being able to train
a GPT-2.</li>
</ul>
<p>Q6: Can you share what’s coming next after transformers? Then the
second question is what do you think most entrepreneurs and VCs are
getting wrong about the future of AI?</p>
<ul>
<li>Most entrepreneurs don’t bet on AI models are going to be massively
better so that’s why there is meme that “OpenAI kills my startup”.</li>
</ul>
<p>Q7: How much exposure does OpenAI and the AI movement have to energy
constraints? And what do you think the role is of founders have to play
in addressing these concerns?</p>
<ul>
<li>It’s necessary to Sam to drive tech abundance from those two key
inputs - energy and AI.</li>
</ul>
<h3 id="articles-and-blogs">Articles and Blogs</h3>
<blockquote>
<p><em>Of the employees we studied, those with superior sales
performance were genetically different from the rest of the group. They
were better at learning in real time about new customers and new sales
opportunities. From an initial conversation with a sales lead, they were
able to quickly feel out the customer and propose appropriate products
without being told what to recommend.</em></p>
<p><em>Adaptive learning is different; it isn’t trainable. It’s the
ability to process new information in real time and immediately use it
to achieve a positive result.</em></p>
<p><em>For example, sales teams often require junior employees to
cold-call leads, even through they don’t know as much about the company
as more experienced employees do. Most of them haven’t even learned how
to sell yet. But our research shows that for adaptive learners,
seniority and experience are less important. Employees with the sales
gene quickly become knowledgeable about your products and are able to
learn and adjust on the fly.</em></p>
<p><strong>― There Really Is a “Sales Gene” - Juan Martinez, Harvard
Business Review</strong> [<a
target="_blank" rel="noopener" href="https://hbr.org/2024/09/there-really-is-a-sales-gene">Link</a>]</p>
</blockquote>
<p>Adaptive learning skill is important but might not correlated with
seniority or experience. Employees with this capability can quickly
become knowledgeable about the products and are able to learn and adjust
on the fly.</p>
<p>The article suggests that managers or companies could be given a
snapshot of how many of salespeople are adaptive learners without
singling out any individual. and they could tell which tasks require
adaptive learning skills and which don’t and allow them to choose. This
should be done in an anonymous way.</p>
<p>No matter whether what’s been proposed in this article is applicable
or ethical. The idea of adaptive learning is kind of new to me, and it
inspires me to further think about whether this skill is learnable and
teachable, and think about whether there is any other secret skills in
sales.</p>
<blockquote>
<p><strong>New Rules for Teamwork - Harvard Business Review</strong> [<a
target="_blank" rel="noopener" href="https://hbr.org/2024/09/new-rules-for-teamwork">Link</a>]</p>
</blockquote>
<ol type="1">
<li><p>Develop an Operating System</p>
<p>OS means building blocks for the way team members collaborate, create
change, and support one another. Effective operating systems vary
widely, depending on the needs and norms of the organization. What they
all have in common is that they set out a view of how teams create
value, what teams are supposed to achieve, the technical skills each
team member is expected to contribute, the processes by which the work
will be managed, and the cultural norms and mindsets of constructive
collaboration that will guide behavior.</p>
<p>Suggestions: hold kickoffs, conduct one on ones, and take stock of
progress using retrospectives, are the three practices as a foundations
of team OS.</p></li>
<li><p>Invest in Active, Real-Time Measurement</p>
<p>To make teamwork scientific, organizations need to be able to measure
the outcomes of their actions and determine how changes in the inputs
affect results.</p>
<p>Suggestion: define what constitutes success.</p></li>
<li><p>Create a System for Continuous Improvement and Innovation</p>
<p>Teams today have new forms of technology and data collection at their
disposal to help them self-correct while projects are underway. e.g.
support colleagues to discuss what could have been done better; look at
the patterns across teams to identify improvements and share best
practices, particularly with regard to the rapid adoption of new
technologies such as GenAI.</p>
<p>Suggestions: Identify the metrics that matter most (shift-changeover
time, perhaps), hypothesize which actions could improve performance in
those areas (preassigned workstations, perhaps), and embed technologies
in the operating system (a smart-planning app, perhaps) to enable
continuous improvement. Continuous improvement can occur only when all
perspectives are considered and all teams have access to a centralized
knowledge repository. Finally, it may be useful to set up a center of
excellence, staffed with full-time employees with experience in
analytics and operating system design.</p></li>
</ol>
<blockquote>
<p><strong>Why Leadership Teams Fail - Harvard Business Review</strong>
[<a
target="_blank" rel="noopener" href="https://hbr.org/2024/09/why-leadership-teams-fail">Link</a>]</p>
</blockquote>
<p>I was reading while thinking about my team and neighbor teams. I find
this article very useful.</p>
<p>A critical factor in organizational success: the health of their
leadership team. There are three main patterns of dysfunction: Shark
Tanks, Petting Zoos, and Mediocracies.</p>
<ol type="1">
<li><p><strong>Shark Tanks</strong></p>
<ul>
<li><p>Definition: A leadership team marked by hyper-competition,
political maneuvering, and infighting. Members prioritize personal
agendas over collective goals, leading to toxic and combative
dynamics.</p></li>
<li><p>Causes: Lack of clear direction or boundaries from the CEO or
team leader. Failure to address self-serving behaviors early on. Absence
of behavioral norms that encourage collaboration.</p></li>
<li><p>Signs: Team members engage in power struggles outside of
meetings. One-on-one discussions with the CEO on issues that should be
resolved in team settings. Meetings turn into battlegrounds, with
frequent arguments and difficulty reaching consensus. Executives
bad-mouth each other, form alliances, or resist decisions after they’ve
been made.</p></li>
<li><p>Prevention:</p>
<ul>
<li><p>Clear Expectations: Leaders should explicitly define which
behaviors are acceptable and unacceptable. Set boundaries around how
competition should be managed.</p></li>
<li><p>Confront Self-Serving Behaviors: Address aggressive or toxic
behaviors directly with individuals. Remove those unwilling to align
with the team’s goals, even if they’re high performers.</p></li>
<li><p>Role Modeling: The CEO or team leader must model collaborative
behaviors and ensure transparency in communication to prevent political
games.</p></li>
<li><p>Regular Feedback: Reinforce positive behaviors and correct
negative ones through continuous feedback. Implement 360-degree reviews
to track team behavior and performance alignment.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Petting Zoos</strong></p>
<ul>
<li><p>Definition: A leadership team that avoids conflict to maintain
harmony. Vigorous debate is sacrificed, and members prioritize getting
along over pushing for the best ideas, leading to complacency and poor
decision-making.</p></li>
<li><p>Causes: Overemphasis on collaboration and mutual trust, leading
to conflict avoidance. Team members are too deferential, fearing that
disagreements might disrupt the team’s harmony. Leaders may unknowingly
encourage this avoidance by stressing harmony over debate.</p></li>
<li><p>Signs: Meetings lack critical debate, and discussions feel muted
and lacking in emotional intensity. Team members engage in performance
theater, focusing on positive news while downplaying problems. Decisions
are made by consensus without sufficient evaluation or challenge.
Leaders avoid holding one another accountable for poor performance,
reluctant to disrupt the status quo.</p></li>
<li><p>Prevention:</p>
<ul>
<li>Encourage Debate: Leaders should foster a culture of constructive
conflict where members feel safe to challenge each other’s ideas. A
foundation of trust and psychological safety is key.</li>
<li>Promote Data-Driven Discussion: Ensure discussions are rooted in
facts, using shared data to spur debate and avoid personal conflict.
This encourages neutral, objective decision-making.</li>
<li>Monitor Meeting Dynamics: Leaders should track participation and the
quality of discussion during meetings, encouraging team members to speak
up and challenge ideas more openly.</li>
<li>Redefine Consensus: Teams must understand that consensus does not
mean avoiding conflict but making informed decisions after rigorous
debate.</li>
</ul></li>
</ul></li>
<li><p><strong>Mediocracies</strong></p>
<ul>
<li><p>Definition: A leadership team marked by complacency, lacking the
drive or skills to achieve high performance. Collaboration and
competition are both underemphasized, and the team fails to meet the
organization’s needs.</p></li>
<li><p>Causes: Long periods of success that breed complacency. Poor
alignment between the team’s skills and the changing demands of the
business. A divided team, where some members prefer competition while
others favor collaboration, leading to inconsistent and ineffective
efforts. A leader’s failure to adapt to changing market conditions or
internal challenges.</p></li>
<li><p>Signs: Team members operate in silos, with little collaboration
between departments or units. Decision-making is slow, and there is a
lack of accountability for performance. The team focuses on past
achievements rather than future goals, with little ambition or drive for
improvement. The team struggles with stagnation, missed opportunities,
and duplicated efforts due to poor coordination.</p></li>
<li><p>Prevention:</p>
<ul>
<li>Rebuild the Team: Leaders may need to replace members who are not
fit for their roles or who lack the motivation or skills needed to lead
effectively. New hires should be chosen not just for their skills but
also for their alignment with the company’s purpose and values.</li>
<li>Promote Balance: Strike a balance between competition and
collaboration by hiring individuals with complementary skills and styles
(e.g., planners and visionaries alongside hard-nosed executors).</li>
<li>Clear Roles and Expectations: Define where collaboration is expected
(e.g., across departments) and where competition might be useful (e.g.,
in individual market decisions). Ensure everyone understands their
responsibilities and how their performance contributes to broader
goals.</li>
<li>Challenge the Status Quo: Continuously push the team to innovate and
grow by setting ambitious goals and holding team members accountable for
driving performance improvements.</li>
</ul></li>
</ul></li>
</ol>
<blockquote>
<p><em>Without such an observability system–let’s call it Design System
Observability–it could be too late when Uber learned through complaints
and public media about the end users who would suffer confusing
onboarding rides, inconsistent layouts, and frustrating
voiceovers/talkbacks sessions.</em></p>
<p><strong>― How to Measure Design System at Scale - Uber Blog</strong>
[<a
target="_blank" rel="noopener" href="https://www.uber.com/en-GB/blog/design-system-at-scale/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>RAG is the most popular architecture of the LLM based systems in
2023. There are many products build almost solely on RAG — from Question
Answering services combining web search engines with LLMs to hundreds of
chat-with-your-data apps.</em></p>
<p><strong>― Advanced RAG Techniques: an Illustrated Overview -
Medium</strong> [<a
target="_blank" rel="noopener" href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Machines of Loving Grace - Dario Amodei, CEO of
Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://darioamodei.com/machines-of-loving-grace">Link</a>]</p>
</blockquote>
<p>This essay aligns with our thinking that while you acknowledge the
risks and share concerns over idealistic promises, you also recognize
the challenges inherent in building AGI. It reflects an approach to AI
that balances ambition with caution, and providing perspectives of how
to articulate a vision for AI that remains both ambitious and
grounded.</p>
<p>Interesting point: As AI reaches near-universal superiority, the
economy may need to adapt fundamentally. Potential solutions range from
universal basic income (UBI) to entirely new economic frameworks.
Perhaps AIs might manage resources and distribute them according to
value systems derived from human input, but such proposals raise ethical
and practical concerns. The author hints that the economic
transformation required could be as drastic as past societal shifts
(e.g. from hunting-gathering to agriculture), suggesting that humanity
will have to experiment and iterate to find sustainable models that
protect against exploitation or dystopia.</p>
<p>Overall, the author believes that the core human values such as
fairness, cooperation, and autonomy have a natural, most
‘’overdetermined’ appeal, and will often lead toward democracy, rule of
law, and enlightenment ideals - a trajectory that AI as a catalyst could
accelerate by making the path to this “good world” more tangible. While
the vision seems intuitive and inspiring to many, it may still appear
fantastical or undesirable to others. Even so, the author finds a unique
beauty in striving for it, suggesting that our intrinsic human impulses
toward collaboration and justice make this vision both plausible and
worth pursuing.</p>
<blockquote>
<p><strong>Andrew Ng’s writing in The Batch - The Batch</strong> [<a
target="_blank" rel="noopener" href="https://www.deeplearning.ai/the-batch/issue-270">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>“The best we can do is a compromise: learn to recognize
situations in which mistakes are likely and try harder to avoid
significant mistakes when the stakes are high.” ― Daniel Kahneman,
Thinking, Fast and Slow</em></p>
<p><strong>― Unleashing System 2 Thinking? AlphaCodium Outperforms
Direct Prompting of OpenAI o1 - qodo</strong> [<a
target="_blank" rel="noopener" href="https://www.qodo.ai/blog/system-2-thinking-alphacodium-outperforms-direct-prompting-of-openai-o1/">Link</a>]</p>
</blockquote>
<p>System 1 thinking: fast responses with surface-level
understanding;</p>
<p>System 2 thinking: deliberate methodical and reasoned problem
solving.</p>
<blockquote>
<p><strong>Introducing the Realtime API - OpenAI Blog</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/introducing-the-realtime-api/">Link</a>]</p>
</blockquote>
<p>Developers can now build fast speech-to-speech experiences into their
applications</p>
<blockquote>
<p><em>With canvas, ChatGPT can better understand the context of what
you’re trying to accomplish. You can highlight specific sections to
indicate exactly what you want ChatGPT to focus on. Like a copy editor
or code reviewer, it can give inline feedback and suggestions with the
entire project in mind.</em></p>
<p><em>You control the project in canvas. You can directly edit text or
code. There’s a menu of shortcuts for you to ask ChatGPT to adjust
writing length, debug your code, and quickly perform other useful
actions. You can also restore previous versions of your work by using
the back button in canvas.</em></p>
<p><strong>― Introducing canvas - OpenAI</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/introducing-canvas/">Link</a>]</p>
</blockquote>
<p>Canvas offers a new interface for the project works that require
editing and revisions.</p>
<blockquote>
<p><strong>Multi document agentic RAG: A walkthrough - LanceDB</strong>
[<a
target="_blank" rel="noopener" href="https://blog.lancedb.com/multi-document-agentic-rag-a-walkthrough/">Link</a>]</p>
</blockquote>
<p>This tutorial shows you how to build a multi-document agentic RAG
system using LanceDB and LlamaIndex for complex information retrieval.
Specifically, the walkthrough demonstrates how to integrate LLMs, vector
databases, and agent-based reasoning for enhanced information retrieval
and task completion.</p>
<h3 id="reports-and-papers">Reports and Papers</h3>
<blockquote>
<p><strong>Learning vs Retrieval: The Role of In-Context Examples in
Regression with LLMs</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.04318">Link</a>]</p>
</blockquote>
<p>The paper explores in-context learning (ICL) mechanisms in large
language models (LLMs), focusing on the balance between knowledge
retrieval and learning from in-context examples in regression tasks. It
reports that LLMs can learn from regression examples of realistic
datasets in-context, extending previous work on synthetic data to more
practical scenarios.</p>
<p>I’m looking forward to this kind of experiments and studies, because
I was suspicious about applying LLM on structured data.</p>
<blockquote>
<p><strong>Larger and more instructable language models become less
reliable</strong> [<a
target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-024-07930-y">Link</a>]</p>
</blockquote>
<p>The issue might stem from the nature of LLMs, which are designed to
generate plausible responses based on patterns in the data they’ve seen,
rather than to <em>know</em> anything in the traditional sense. They
don’t have an internal mechanism to differentiate truth from
fabrication, so as they scale up, they produce more complex, yet not
necessarily more accurate, answers. This makes them better at appearing
smart, but less reliable overall—a quality that philosophers like Mike
Hicks rightly criticize as “bullshitting.”</p>
<p>From a user perspective, it underscores the need for critical
thinking when engaging with AI models through prompt engineering. Just
because an LLM provides a well-phrased response doesn’t mean it’s
accurate.</p>
<blockquote>
<p><em>o1—like previous LLMs—is sensitive to the probability of examples
and tasks, performing better and requiring fewer “thinking tokens” in
high-probability settings than in low-probability ones.</em></p>
<p><strong>― When a language model is optimized for reasoning, does it
still show embers of autoregression? An analysis of OpenAI o1</strong>
[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.01792">Link</a>]</p>
</blockquote>
<p>Although optimized for reasoning, o1 still exhibits probability-based
limitations tied to its autoregressive origins, implying that a complete
departure from these influences has not been fully achieved.</p>
<blockquote>
<p><strong>VideoPrism: A foundational visual encoder for video
understanding - Google Research</strong> [<a
target="_blank" rel="noopener" href="https://research.google/blog/videoprism-a-foundational-visual-encoder-for-video-understanding/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Astute RAG: Overcoming Imperfect Retrieval Augmentation and
Knowledge Conflicts for Large Language Models</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.07176">Link</a>]</p>
</blockquote>
<p>Astute RAG is designed to better combine internal and external
information through an interactive consolidation mechanism (i.e.,
identifying consistent passages, detecting conflicting information in
them, and filtering out irrelevant information).</p>
<blockquote>
<p><strong>Differential Transformer - Microsoft Research</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.05258">Link</a>]</p>
</blockquote>
<p>Diff Transformer amplifies attention to relevant context while
canceling noise, resulting in outperforming standard Transformers in
multiple areas such as long-context modeling, key information retrieval,
hallucination mitigation, in-context learning, and reducing activation
outliers, as shown in the experiments.</p>
<p>The key innovation is a differential attention mechanism that
calculates attention scores by subtracting two separate softmax
attention maps. This subtraction cancels out irrelevant attention,
promoting sparser, more accurate focus on important information, similar
to noise-canceling techniques.</p>
<blockquote>
<p><strong>Diffusion Guided Language Modeling</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.04220">Link</a>]</p>
</blockquote>
<p>Controllable language modeling refers to techniques that allow users
to guide or control specific attributes of the generated text from a
language model (LM). These attributes can include factors like
sentiment, toxicity, formality, or any other desired linguistic or
stylistic feature. The primary challenge is ensuring that generated
content aligns with specific requirements without compromising fluency,
coherence, or overall quality of the text.</p>
<p>Diffusion models are excellent for controllable generation. They
generate data in a multi-step process, gradually refining noise into
coherent content. This incremental approach allows for fine-grained
control at various stages of the generation. By manipulating the process
at specific steps, you can guide the output more effectively toward
desired characteristics (such as sentiment, style, or tone). In
contrast, auto-regressive models like GPT generate text token by token
in a one-shot manner, making it harder to impose controls without
affecting fluency.</p>
<p>DGLM could refine language model generation because it integrates the
fluency of auto-regressive language models (like GPT) with the
flexibility of diffusion models. This flexibility is realized by
employing Plug-and-Play with Linear Classifiers in the Sentence-T5
latent space to guide the diffusion process towards generating proposals
with desired attributes.</p>
<blockquote>
<p><strong>On the Diagram of Thought</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.10038">Link</a>]</p>
</blockquote>
<p>Researchers from Tsinghua University, led by Andrew Chi-Chih Yao,
introduced Diagram of Thought (DoT), designed to enhance the reasoning
capabilities of LLMs.</p>
<p>The limitation of CoT is that it processes information in a straight
line which does not reflect the way of how humans think. The limitation
of ToT or GoT is that they are computationally expensive and challenging
to implement within a single LLM.</p>
<p>DoT addresses these limitations by modeling reasoning as the
construction of a directed acyclic graph (DAG) within a single LLM. This
DAG comprises nodes representing: 1) Propositions: Initial and refined
ideas generated throughout the reasoning process, 2) Critiques:
Evaluations of propositions, identifying errors or inconsistencies. 3)
Refinements: Improved propositions based on critiques. 4) Verifications:
Confirmation of valid propositions.</p>
<p>Two key crucial aspect of DoT framework: 1) it leverages
auto-regressive next-token prediction with role specific tokens to
manage reasoning process within a single LLM, 2) it has strong
foundation in math logic - Topos Theory, ensuring logical consistency
and soundness in reasoning process.</p>
<blockquote>
<p><strong>A Survey on the Honesty of Large Language Models</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.18786">Link</a>]</p>
</blockquote>
<p>Dunning-Kruger effect is named after psychologists David Dunning and
Justin Kruger, who first described this <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect">phenomenon
in 1999</a>. It reveals a troubling mismatch between perception and
reality. Those who know little often lack the self-awareness to
recognize their limitations. Conversely, experts may be acutely aware of
the vastness of their field, leading them to undervalue their own
expertise.</p>
<blockquote>
<p><strong>Spatial context non-uniformly modulates inter-laminar
information flow in the primary visual cortex</strong> [<a
target="_blank" rel="noopener" href="https://www.cell.com/neuron/abstract/S0896-6273(24)00693-7?_returnURL=https://linkinghub.elsevier.com/retrieve/pii/S0896627324006937?showall=true">Link</a>]</p>
</blockquote>
<p>Their research shows that when our field of vision is cluttered, it
changes how efficiently our brain processes information, though the
basic pattern of information transfer remains the same.</p>
<blockquote>
<p><strong>When you give a Claude a mouse - Out Useful Thing</strong>
[<a
target="_blank" rel="noopener" href="https://www.oneusefulthing.org/p/when-you-give-a-claude-a-mouse">Link</a>]</p>
</blockquote>
<p>Some impression on what an agent is capable of.</p>
<blockquote>
<p><strong>Agentic Information Retrieval</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.09713">Link</a>]</p>
</blockquote>
<p>Research proposed AI agent on information retrieval (IR) task.
Compared to traditional IR, Agentic IR employs a unified, adaptive
architecture where agents use observation, reasoning, and actions
iteratively to reach the desired user information state. Key methods
include prompt engineering, retrieval-augmented generation, multi-agent
systems, and reinforcement fine-tuning (RFT).</p>
<blockquote>
<p><strong>A Comparative Study on Reasoning Patterns of OpenAI’s o1
Model</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.13639">Link</a>]</p>
</blockquote>
<p>An evaluation study of GPT o1’s reasoning patterns. The study
identified six distinct reasoning patterns for o1—Systematic Analysis
(SA), Method Reuse (MR), Divide and Conquer (DC), Self-Refinement (SR),
Context Identification (CI), and Emphasizing Constraints (EC)—with DC
and SR being most common across tasks. And token count varied greatly
across tasks, indicating that the o1 model adjusts reasoning depth based
on task complexity.</p>
<blockquote>
<p><strong>Malla: Demystifying Real-world Large Language Model
Integrated Malicious Services</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.03315">Link</a>]</p>
</blockquote>
<p>A study of malicious services powered by LLM ‘Malla’ in the
underground marketplaces. (When it comes to AI, everywhere has gaps to
bridge lol). Interesting mysteries to uncover:</p>
<ol type="1">
<li><p>Who are the pivotal players within the Malla ecosystem?</p>
<p>The Malla ecosystem comprises vendors who create malicious LLM
services, users who exploit these services, and platforms that
facilitate their operations.</p></li>
<li><p>How is Malla orchestrated and monetized?</p>
<p>Malla services generate revenue through direct user transactions,
often accepting cryptocurrencies, with some vendors reporting
substantial earnings.</p></li>
<li><p>What techniques did miscreants deploy to exploit LLMs and build
up Mallas?</p>
<p>Miscreants utilize techniques like jailbreak prompts to bypass LLM
restrictions and abuse public APIs to generate harmful content.</p></li>
</ol>
<blockquote>
<p><strong>Were RNNs All We Needed?</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.01201">Link</a>] [<a
target="_blank" rel="noopener" href="https://huggingface.co/papers/2410.01201">Source</a>]</p>
</blockquote>
<p>The minimal versions (minLSTMs and minGRUs) are fully parallelizable
during training and use fewer parameters. They are 175x faster to train
than traditional LSTMs and GRUs. And their performance is equivalent to
Transformers or Mamba with fewer training steps.</p>
<blockquote>
<p><strong>The Perfect Blend: Redefining RLHF with Mixture of
Judges</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.20370">Link</a>]</p>
</blockquote>
<p>This work is redefining RLHF with Mixture of Judges by Constrained
Generative Policy Optimization (CGPO), a novel RLHF framework. The
framework uses a Mixture of Judges (MoJ) with rule-based and LLM-based
constraints to mitigate reward hacking. As a result, CGPO consistently
outperforms PPO and DPO baselines across various benchmarks.</p>
<blockquote>
<p><strong>STATE OF AI REPORT 2024</strong> [<a
target="_blank" rel="noopener" href="https://www.stateof.ai/">Link</a>]</p>
</blockquote>
<p>This annual publication examines trends in AI research, industry
developments, and technological progress. And this year it reveals
convergence in AI Model Performance.</p>
<blockquote>
<p><strong>MLE-bench: Evaluating Machine Learning Agents on Machine
Learning Engineering</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.07095">Link</a>] [<a
target="_blank" rel="noopener" href="https://openai.com/index/mle-bench/">Blog</a>]</p>
</blockquote>
<p>As the title described, they introduce MLE-bench as a benchmark for
measuring how well AI agents perform at machine learning engineering.
The tasks for testing were created by curating 75 ML engineering related
competitions from Kaggle. Eventually, OpenAI’s o1-preview with AIDE
scaffolding achieves at least bronze medal in 16.7% of competitions.</p>
<h3 id="github-and-docs">Github and Docs</h3>
<blockquote>
<p><strong>Swarm - An educational framework exploring ergonomic,
lightweight multi-agent orchestration</strong> [<a
target="_blank" rel="noopener" href="https://github.com/openai/swarm/tree/main">Link</a>]</p>
</blockquote>
<p>Little experimental and educational multi-agent framework by
OpenAI.</p>
<p>Swarm’s Operational Framework:</p>
<ul>
<li>Agent Definition: Create Agents with specific instructions, roles,
and functions. Each function is automatically converted to a JSON
structure for API compatibility.</li>
<li>Handoff Mechanism: Implement logic for agent transitions. Functions
can return a new Agent object to transfer control based on conversation
flow or predefined criteria.</li>
<li>Context Management: Utilize Context Variables to initialize and
update shared information throughout the conversation, maintaining state
across agent interactions.</li>
<li>Execution Loop: The client.run() function manages the multi-agent
conversation. It takes an initial agent, user messages, and context as
input, and returns a response with updated messages, context variables,
and the last active agent.</li>
<li>This structure allows for flexible, dynamic multi-agent interactions
while maintaining a stateless architecture between calls.</li>
</ul>
<blockquote>
<p><strong>o1-engineer</strong> [<a
target="_blank" rel="noopener" href="https://github.com/Doriandarko/o1-engineer">Link</a>]</p>
</blockquote>
<p>A CLI tool for streamlining workflows with AI-powered code generation
and editing.</p>
<blockquote>
<p><strong>Llama-stack</strong> [<a
target="_blank" rel="noopener" href="https://github.com/meta-llama/llama-stack">Link</a>]</p>
</blockquote>
<p>Meta unveils open-source Llama Stack, standardizing AI building
blocks across the entire development lifecycle.</p>
<blockquote>
<p><strong>Leaked meta prompt</strong> [<a
target="_blank" rel="noopener" href="https://gist.github.com/philschmid/3a0ecc9e45763716f4dd9c36b6445fca">Link</a>]</p>
</blockquote>
<p>Leaked OpenAI meta prompt: optimizing GPT instructions for better
results.</p>
<figure>
<img
src="/di-blog/2024/10/01/2024-October/leaked-openai-meta-prompt.png"
alt="leaked-openai-meta-prompt" />
<figcaption aria-hidden="true">leaked-openai-meta-prompt</figcaption>
</figure>
<blockquote>
<p><strong>Auto Jobs Applier - AIHawk</strong> [<a
target="_blank" rel="noopener" href="https://github.com/feder-cr/Auto_Jobs_Applier_AIHawk">Link</a>]</p>
</blockquote>
<p>Job search assistant.</p>
<blockquote>
<p><strong>RAGBuilder</strong> [<a
target="_blank" rel="noopener" href="https://github.com/KruxAI/ragbuilder?tab=readme-ov-file">Link</a>]</p>
</blockquote>
<p>Toolkit used to create optimal production-ready RAG setup for your
data automatically.</p>
<blockquote>
<p><strong>Prompt caching (beta) - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching">Link</a>]</p>
</blockquote>
<p>Prompt caching optimizes API calls for faster LLM interactions. It is
a new API feature that optimizes large language model interactions. It
caches and reuses consistent parts of prompts, reducing processing time
and costs for repetitive tasks.</p>
<p>This technique is particularly useful for scenarios involving large
contexts, multiple examples, or long conversations. Prompt Caching works
by checking if a prompt prefix is already cached from a recent query and
using it if found, otherwise processing and caching the full prompt.</p>
<blockquote>
<p><strong>Lazy Predict</strong> [<a
target="_blank" rel="noopener" href="https://github.com/shankarpandala/lazypredict">Link</a>]</p>
</blockquote>
<p>Works to rapidly test multiple ML models with minimal coding effort.
This Python library streamlines model selection for classification and
regression tasks.</p>
<h3 id="news">News</h3>
<blockquote>
<p><strong>The Nobel Prize in Physics 2024 to John J. Hopfield and
Geoffrey E. Hinton, for foundational discoveries and inventions that
enable machine learning with artificial neural networks</strong> [<a
target="_blank" rel="noopener" href="https://www.nobelprize.org/prizes/physics/2024/summary/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Zuckerberg imagines that people will want to use AR glasses like
Orion for two primary purposes: communicating with each other through
digital information overlaid on the real world — which he calls
“holograms” — and interacting with AI.</em></p>
<p><strong>― Meta’s big tease - The Verge</strong> [<a
target="_blank" rel="noopener" href="https://www.theverge.com/24253908/meta-orion-ar-glasses-demo-mark-zuckerberg-interview">Link</a>]</p>
</blockquote>
<p>One downside of Apple’s Vision Pro or Meta’s Quest 3 is that you lost
vision of other people and other people cannot see your eyes, making it
usage situation limited to home where you don’t have interaction with
other people. However, the future of devices that’s going to replace
mobile phone or comparable to mobile phone, has to have some
functionalities to support socialization and networking.</p>
<blockquote>
<p><strong>Llama 3.2: Revolutionizing edge AI and vision with open,
customizable models - Meta Blog</strong> [<a
target="_blank" rel="noopener" href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Big Tech has cozied up to nuclear energy - The Verge</strong>
[<a
target="_blank" rel="noopener" href="https://www.theverge.com/2024/10/5/24261405/google-microsoft-amazon-tech-data-center-nuclear-energy">Link</a>]</p>
</blockquote>
<p>Microsoft, Amazon, and Google are investing in nuclear energy to
power their data centers.</p>
<blockquote>
<p><strong>Why Taiwan and Its Tech Industry Are Facing an Energy Crisis
- Yale Environment 360</strong> [<a
target="_blank" rel="noopener" href="https://e360.yale.edu/features/taiwan-energy-dilemma">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Google’s share of the U.S. search ad market is expected to drop
below 50% next year for the first time in over a decade, according to
the research firm eMarketer.</em></p>
<p><em>Amazon is expected to have 22.3% of the market this year, with
17.6% growth, compared with Google’s 50.5% share and its 7.6%
growth.</em></p>
<p><strong>― Google’s Grip on Search Slips as TikTok and AI Startup
Mount Challenge - The Wall Street Journal</strong> [<a
target="_blank" rel="noopener" href="https://www.wsj.com/tech/online-ad-market-google-tiktok-9599d7e8">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Uber and Lyft drivers use Teslas as makeshift robotaxis,
raising safety concerns - Reuters</strong> [<a
target="_blank" rel="noopener" href="https://www.reuters.com/business/autos-transportation/uber-lyft-drivers-use-teslas-makeshift-robotaxis-raising-safety-concerns-2024-10-03/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Real world example:</em></p>
<p><em>Shorenstein Properties, a real-estate investment company based in
San Francisco, is in a pilot program that is designed to lead to the
automated tagging of all of its files using a RAG-based AI system. The
goal is to eliminate many of the drawbacks in a time-consuming manual
system, in which people might make errors or simply skip the process
altogether. The company plans to put the tagging system into production
in the next few months.</em></p>
<p><em>Files can also be organized quickly into “knowledge bases” and
interrogated with AI, according to Egnyte, a cloud-based platform that
companies use to access, share and manage business content.</em></p>
<p><em>Shorenstein in the past few weeks has started a proof of concept
project using Egnyte to extract data from prospectuses on properties for
sale, documents that can often run 60 pages, and organize it into
reports that could help the company make efficient business decisions
and improve processes.</em></p>
<p><strong>― Companies Look Past Chatbots for AI Payoff - Steven
Rosenbush at The Wall Street Journal</strong> [<a
target="_blank" rel="noopener" href="https://www.wsj.com/articles/companies-look-past-chatbots-for-ai-payoff-c63f5301">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Beginning Friday, users of Meta’s AI chatbot feature in the U.S.
will have access to real-time news and information from Reuters when
they ask questions about news or current events.</em></p>
<p><em>It’s the first news deal Meta has brokered in the AI
era.</em></p>
<p><strong>― Scoop: Meta strikes multi-year AI deal with Reuters -
AXIOS</strong> [<a
target="_blank" rel="noopener" href="https://www.axios.com/2024/10/25/meta-reuters-ai-news-facebook-instagram">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>An AI companion for everyone - Microsoft Blog</strong> [<a
target="_blank" rel="noopener" href="https://blogs.microsoft.com/blog/2024/10/01/an-ai-companion-for-everyone/">Link</a>]</p>
</blockquote>
<p>Releasing Copilot Voice, Copilot Daily, Personalization in Copilot,
Copilot Vision, Think Deeper.</p>
<blockquote>
<p><strong>Anaconda Brings Generative AI Models to Desktops with Launch
of AI Navigator - Anaconda Press</strong> [<a
target="_blank" rel="noopener" href="https://www.anaconda.com/press/anaconda-ai-navigator-generative-ai-desktop-agent">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Meet the new Notion AI - Notion</strong> [<a
target="_blank" rel="noopener" href="https://www.notion.so/product/ai?cookie_sync_completed=true">Link</a>]</p>
</blockquote>
<p>Notion AI heavily integrates AIintroducing file-handling
capabilities, enabling developers to extract insights from PDFs and
images.</p>
<blockquote>
<p><strong>Customer data search, unification and retrieval for LLMs -
Tilores</strong> [<a target="_blank" rel="noopener" href="https://tilores.io/RAG">Link</a>]</p>
</blockquote>
<p>Identity RAG from Tilores improves accuracy and relevance of
enterprise LLMs.</p>
<blockquote>
<p><strong>New autonomous agents scale your team like never before -
Microsoft</strong> [<a
target="_blank" rel="noopener" href="https://blogs.microsoft.com/blog/2024/10/21/new-autonomous-agents-scale-your-team-like-never-before/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>We’re also introducing a groundbreaking new capability in public
beta: <strong>computer use</strong>. Available <a
target="_blank" rel="noopener" href="https://docs.anthropic.com/en/docs/build-with-claude/computer-use">today
on the API</a>, developers can direct Claude to use computers the way
people do—by looking at a screen, moving a cursor, clicking buttons, and
typing text. Claude 3.5 Sonnet is the first frontier AI model to offer
computer use in public beta.</em></p>
<p><strong>Introducing computer use, a new Claude 3.5 Sonnet, and Claude
3.5 Haiku</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/news/3-5-models-and-computer-use">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Perplexity Now Offers Powerful AI Search Right On Your macOS
Desktop</strong> [<a
target="_blank" rel="noopener" href="https://www.makeuseof.com/perplexity-now-on-your-macos-desktop/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Pushing the frontiers of audio generation - Google
DeepMind</strong> [<a
target="_blank" rel="noopener" href="https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Gemini API and Google AI Studio now offer Grounding with
Google Search - Google for Developers</strong> [<a
target="_blank" rel="noopener" href="https://developers.googleblog.com/en/gemini-api-and-ai-studio-now-offer-grounding-with-google-search/">Link</a>]</p>
</blockquote>
<p>What’s new: 1) reduces hallucination rates by sourcing verified,
real-time information, 2) provides citation-supported answers, improving
transparency, 3) allows threshold-based activation to control costs.</p>
<p>Technical details: Google’s <strong>Dynamic Retrieval</strong> system
evaluates each query for grounding suitability, assigning a prediction
score between 0 and 1. Factual or time-sensitive queries score higher
(e.g., 0.97), while creative prompts score lower (e.g., 0.13).</p>
<blockquote>
<p><strong>New in Maps: Inspiration curated with Gemini, enhanced
navigation and more - Google</strong> [<a
target="_blank" rel="noopener" href="https://blog.google/products/maps/gemini-google-maps-navigation-updates/">Link</a>]</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/09/28/The-Long-View/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/09/28/The-Long-View/" class="post-title-link" itemprop="url">The Long View</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-09-28 22:56:31" itemprop="dateCreated datePublished" datetime="2024-09-28T22:56:31-04:00">2024-09-28</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>“The Long View: Career Strategies to Start Strong, Reach High, and Go
Far” written by Brian Fetherstonhaugh was a book I randomly picked to
read but it surprised me as it turns out to be fairly practical and
comprehensive.</p>
<blockquote>
<p>We need a work philosophy that encompasses all the parts of our
lives, and one that can give us guidance on how to be ambitious and seek
success without sacrificing other things we value deeply— family,
friends, health, and purpose.</p>
</blockquote>
<p>What I have learned:</p>
<h3 id="five-things-you-need-to-know-to-build-a-career-plan">Five Things
You Need to Know to Build a Career Plan</h3>
<ol type="1">
<li><p>Careers last for about 45 years and embrace 3 distinctly
different stages, each lasting about 15 years.</p>
<ul>
<li><p>Stage One: Start Strong by Taking on Fuel</p>
<blockquote>
<p>Your learning curve is more important than your job title. Create the
foundation for your career and establish good early habits.</p>
</blockquote>
<p>These are good suggestions for a new hire:</p>
<blockquote>
<p>In July of 2015, Next Big Sound was acquired by Pandora. Modest and
down to earth, Alex is quick to point out the role that luck played in
his success as well. “The number of things that went our way was
terrifying,” he said shaking his head. “If you want to start your own
company just so you can be on the Forbes Under 30 list, it’s not worth
it. If you want to be an entrepreneur to make a lot of money, don’t. If
you find something you’re obsessed with, something that keeps you up at
night, then that’s the thing you should pursue.”</p>
</blockquote>
<blockquote>
<p>If you are serious about your career, this is not enough. Learn what
makes your company tick: where it came from, what it stands for, how it
makes its money, who the key people are, and where it is going If you
don’t get answers to these questions as part of your company’s normal
indoctrination, make it your business to find out in the first hundred
days. Do your homework. Read the company’s annual report, or better yet,
an outside analyst’s assessment of the company. Ask both old-timers and
rising stars to tell you the inside scoop on your company over a cup of
coffee, Get engaged by joining a club, team, or professional network in
the firm. Volunteer to help with a company event, and do it well. Slowly
begin to build your career ecosystem of contacts, communities, critical
colleagues, and champions.</p>
</blockquote></li>
<li><p>Stage Two: Reach High by Focusing on Your Strengths and
Passions</p>
<blockquote>
<p>The prime objective for this stage is to find your sweet spot-the
intersection of what you’re good at, what you love to do, and what the
world appreciates. It is the time to differentiate yourself from the
pack, to stand out, and to become eligible for career pathways that will
be most rewarding to you. Focus on your strengths and largely ignore
your weaknesses.</p>
</blockquote></li>
<li><p>Stage Three: Go Far by Staying Fresh and then Passing the
Torch</p>
<blockquote>
<p>Stage Three is devoted to achieving lasting impact and finding a
sustainable new career pathway that will likely need to last well into
your sixties or even seventies.</p>
</blockquote></li>
</ul></li>
<li><p>“Fuel” matters what you build on.</p>
<blockquote>
<p>Fuel is critical throughout your career. In Stage One you need to
accumulate it, in Stage Two you need to take advantage of it, and in
Stage Three you need to refresh and preserve it.</p>
</blockquote>
<blockquote>
<p>Here is a great theoretical exercise suggested by social scientist
Charles Handy: Imagine if at the age of forty you had to quit your job
forever and start a company with just you. What would you do? That is a
great test of self-reliance.</p>
</blockquote>
<ul>
<li><p>Transportable Skills.</p>
<ul>
<li><p>Problem Solving - being able to assess a problem and create a
plan; being able to have a solid method or two that you can reply on to
help solve the problem when you aer given a challenge an a blank piece
of paper.</p>
<blockquote>
<p>The good news is that there are many frameworks and strategies to
help you improve your problem-solving abilities. Be intentional in
adding several different approaches to your repertoire, and don’t be
afraid to combine a few different methods to create something unique
that works for you.</p>
</blockquote></li>
<li><p>Persuasive communication -</p>
<blockquote>
<p>Inventors and creative people need to sell their ideas.</p>
</blockquote>
<blockquote>
<p>Persuasion is not just opinions expressed loudly. That might work
once, but it doesn’t work over the long haul. Part of being persuasive
is bringing forward compelling facts that truly give people permission
to believe you. I worry that in the world of ubiquitous information,
there are too many opinions and half-truths available, but too few
authoritative sources. When I work with young professionals at my
company I always encourage them to back up every key point with a
footnote and a source.</p>
</blockquote>
<blockquote>
<p>One of the best skills to learn is the ability to spot communication
breakdowns and adjust your approach accordingly. There are a few things
that can go wrong during a conversation:</p>
<ul>
<li>Correspondence is when two people use different words to say the
same thing.</li>
<li>Conflict can arise when two people use the same word but mean
different things.</li>
<li>Contrast happens where there is no overlap at all.</li>
</ul>
</blockquote></li>
<li><p>Getting Things Done - being able to not only start but al finish
projects consistently; being able to power through to achieve the end
goal, regardless of the barriers and obstacles throw at you; being able
to be trusted by people at work with more high-profile
projects.</p></li>
<li><p>Becoming a Talent Magnet -</p>
<blockquote>
<p>The companies with the best people always win. The individual leaders
who have the ability to attract and mobilize top talent win.</p>
</blockquote></li>
<li><p>Giving and Asking for Help -</p>
<blockquote>
<p>According to Grant, successful Givers - those who give more than they
take - are much more likely to be among the highest performing and most
satisfied people.</p>
</blockquote></li>
<li><p>Emotional Intelligence (EQ) -</p>
<blockquote>
<p>Through his access to business leaders around the world and studies
in more than five hundred organizations, Goldman documents an
astonishing fact: in determining star performance in every field,
emotional intelligence matters twice as much as IQ or technical
expertise.</p>
</blockquote></li>
</ul></li>
<li><p>Meaningful Experiences.</p>
<blockquote>
<p>Meaningful experiences combine to enable you to be versatile and
robust in your career. New experiences take you outside your comfort
zone and build new career muscles.</p>
</blockquote>
<blockquote>
<p>I think that everybody in business these days should spend at least
one chapter of their career working in e-commerce even if it’s just for
a couple of years. Here’s why: e-commerce is a huge industry with great
long-term prospects. It is already worth hundreds of billions of dollars
and is projected to grow over 15 percent per year over the next decade.
Because e-commerce involves the whole selling process, it teaches you to
think like a general manager —from product development to how the supply
chain works to merchandising, customer service, and more. It gives you
exposure to the “soft skills” of business like branding and customer
experience as well as the “hard skills” of profit management, data, and
analytics. Best of all, a job in e-commerce means that you get a report
card every day in the form of immediate sales. E-commerce can act like a
microcosm of all business in a single job assignment. What a fantastic
way to accelerate your learning and development. If I were starting my
career over today, I would spend at least one chapter doing
e-commerce.</p>
</blockquote></li>
<li><p>Enduring Relationships.</p>
<p>Your bosses, client / customer relationships, business partners,
talent around you, and find your tribe.</p>
<blockquote>
<p>Enduring relationships are perhaps the most potent and long-lasting
form of fuel. They include both the brands you associate with and the
people you connect with throughout the journey.</p>
</blockquote>
<blockquote>
<p>Katya Andresen, the CEO of Cricket Media, defines the three principal
roles that mentors can play in our lives: the Star, a successful role
model who shows us how it can be done, the Sage, who like Socrates
doesn’t give us the answer but teaches us how to think, and the
Agitator, who spurs us and stretches us, and gives us the occasional
kick in the pants.</p>
</blockquote></li>
</ul></li>
<li><p>Careers are built through the skillful investment of time.</p>
<blockquote>
<p>Becoming a highly employable expert or “master” is not just the
result of innate talent, but of the application of thousands of hours of
learning, experience, and practice.</p>
</blockquote></li>
<li><p>Careers do not progress in linear or predictable ways.</p>
<blockquote>
<p>Successful careers are a combination of diligent planning and good
luck. The diligent planning is essential, because it makes you eligible
for the luck.</p>
</blockquote></li>
<li><p>A career is so much more than a job: it’s a big part of
life.</p></li>
</ol>
<h3
id="five-things-you-need-to-do-to-bring-your-career-plan-to-life">Five
Things You Need to Do to Bring Your Career Plan to Life</h3>
<ol type="1">
<li><p>Do the Career Math exercise to get into the right long-term frame
of mind.</p>
<ul>
<li><p>62 is the median age of retirement in the US. If today you are in
your late twenties, you have almost 35 years of career left. 40 years
old is not a halfway point - many people underestimate the length of a
career.</p></li>
<li><p>10000 hours of intense practice and rehearsal is needed to become
excellent at something.</p>
<blockquote>
<p>Now matter how many IQ points or natural gifts you have, being
successful takes intense hard work and many more hours than you
thin.</p>
</blockquote></li>
<li><p>On average 85%-90% of personal wealth are cumulated after 40th
birthday.</p>
<blockquote>
<p>An individual’s personal wealth tends to peak at about age 65, and
their personal wealth at age 40 is only about 10%-15% of that amount</p>
</blockquote></li>
<li><p>It’s not necessarily true that the key to a successful career is
to have the most social contacts.</p></li>
<li><p>Number of people who will really make a difference to your career
in life.</p>
<blockquote>
<p>We all discover people in the course of our careers who become our
mentors, teachers, and advocates. They are the people who champion us
and say nice things about use behind our backs. They nominate us for
jobs and awards.</p>
<p>Always remember there is someone out there who is in your corner.</p>
</blockquote>
<blockquote>
<p>A lot of people incorrectly think “It’s all about contacts,” as
though that’s where career success begins and ends. Raw connections are
useful to extend your reach, but they aren’t of significant value until
you convert them to a higher<br />
relationship-those people who will engage and mobilize on your behalf.
You may end up with thousands of raw connections. But remember, it is
not just a volume game; it is about quality and impact.</p>
<p>Ben Casnocha, the coauthor of The Start-up of You along with LinkedIn
founder Reid Hoffman, underscores this point clearly. “There’s a
distinction between networking and genuine relationship building.
Networkers are transactional. They pursue relationships thinking only
about what other people can do for them. Relationship builders, on the
other hand, try to help other people first. They don’t keep score.
They’re aware that most good deeds get reciprocated, but they’re not
calculated about it. And they think about their relationships all the
time, not just when they need something.”</p>
</blockquote></li>
</ul></li>
<li><p>Complete a Career Inventory to take stock of your most relevant
skills, experiences and relationships.</p>
<p>Check out the book chapter 5.</p></li>
<li><p>Take the 100-Hour Test and complete a Personal Time Portfolio to
see how you are investing your time.</p>
<p>Percentage of personal time on family, work, community, fitness,
teach and learn, and chilling.</p></li>
<li><p>Use the Career Path Navigator when you are trying to set a new
career pathway to decide between several options.</p>
<blockquote>
<p>According to Auren, “Long-term success requires massive growth. Most
smart people out of college grow an average of 10 percent per year.
Which means they are roughly twice as effective seven years after
graduating college. That makes sense, as most twenty-nine-year-olds make
double what they did their first job out of college. To grow even more
quickly, you need a job with the following criteria:</p>
<p>• You’re surrounded by people who are smarter than you<br />
• You have an opportunity to fail<br />
• The company has a history of giving massive responsibility to people
like you</p>
</blockquote>
<blockquote>
<p>And if you decide to leave, exit with grace. It is a cliché to say
when people leave “our paths will cross again.” It is utterly, totally
true. Former colleagues and employers are a critical part of your career
ecosystem. They will provide ratings and opinions about you for years to
come. They will shop for talent in their current companies and in future
places they work. They will become consultants, clients, and
influencers. Wrap up your assignments with notable diligence and
accountability. Heal wounds as appropriate. Say thank you.</p>
</blockquote></li>
<li><p>Future-proof your career by periodically challenging yourself
with the five scary long-term questions.</p>
<ul>
<li>How can I avoid being replaced by a machine?</li>
<li>Where and how will I find work?</li>
<li>How will I spend my time in the future?</li>
<li>Will I outlive my money?</li>
<li>How will work make me happy?</li>
</ul></li>
</ol>
<h3 id="overcoming-adversities">Overcoming Adversities</h3>
<blockquote>
<p>Whether your career setback is unexpected or foreseeable, you will
need a method to speed your recovery. The four Rs method mentioned in
chapter 11 in the context of returnships is a good general approach to
getting back on track quickly, If you get fired or pushed to the side,
build on the four Rs to help get you back on the right path.</p>
<ul>
<li>Reframe your experience so that it connects to the future, not just
to the past.</li>
<li>Refresh any skills that are rusty or lacking. You cannot fake your
way to renewed career momentum.</li>
<li>Reconnect your career ecosystem. Maybe you need some fresh
relationships with contacts, experts, critical colleagues and champions
to propel you forward.</li>
<li>Reboot your confidence. Talk to people who know you and get you.
Reflect on the strengths and special contributions you have built over
the years. Be brave.</li>
</ul>
</blockquote>
<blockquote>
<p>He advice to those facing a serious crisis at any age is to change
your attitude and quite possibly your latitude. Get out of your comfort
zone. Spend some time on the dark side of town. Travel. Break out of
rituals that can often hold you back. Even spending a day working at a
soup kitchen could do you a world of good. Put yourself in a bigger
context and get in touch with what’s really important. Rediscovering
your humanity will remind you of your blessings and how you can make a
difference.</p>
</blockquote>
<h3 id="downloadable-exercises-and-resources">Downloadable Exercises and
Resources</h3>
<p><a
target="_blank" rel="noopener" href="https://thelongviewcareer.com/resources">https://thelongviewcareer.com/resources</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/09/01/2024-September/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/09/01/2024-September/" class="post-title-link" itemprop="url">2024 September - What I Have Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-09-01 21:53:50" itemprop="dateCreated datePublished" datetime="2024-09-01T21:53:50-04:00">2024-09-01</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="substack">Substack</h3>
<blockquote>
<p><em>Shopify has been acquisitive, but not like Broadcom or Salesforce
with their jumbo acquisitions. Instead, they tend to acquire tiny
businesses that are initially immaterial to the financials but can add
up over time as they add new features to the ecosystem and bring in
founding teams eager to make a difference.</em></p>
<p><strong>― Shopify: Back On Track - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/shopify-back-on-track">Link</a>]</p>
</blockquote>
<p>Business performance highlights: 1) Post-COVID hangover rebound, 2)
GMV = Gross Merchandise Volume grew 27% outside North America and 32% in
Europe, 3) Shopify gained market share, 4) Shopify Payments penetration
rate hit an all-time high of 61%, 5) Unified commerce platform, 6)
Expansion into new markets, 7) Enterprise adoption, 8) Improved
profitability, 9) Temporary operating margin boost.</p>
<p>Strategic Partnerships: 1) App and channel partners: Google, Meta,
Microsoft, Amazon, etc, 2) Product partners: PayPal and Stripe, etc, 3)
Service and technology partners: Oracle, IBM, etc.</p>
<blockquote>
<p><strong>Beat your Bot: Building your Moat against AI - Musings on
Markets</strong> [<a
target="_blank" rel="noopener" href="https://aswathdamodaran.substack.com/p/beat-your-bot-building-your-moat">Link</a>]</p>
</blockquote>
<p>AI’s strengths lie in mechanical, rule-based, and objective tasks,
while it struggles with intuitive, principle-based, and bias-prone work.
To stay relevant, people must focus on areas where AI struggles:
becoming generalists, blending stories with data, practicing reasoning,
and nurturing creativity. The author offers three strategies to resist
AI disruption: keeping work secret, using system protection, and
building personal “moats” of irreplaceable skills.</p>
<blockquote>
<p><strong>New LLM Pre-training and Post-training Paradigms - Ahead of
AI</strong> [<a
target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Dealing with aging: The Intel, Walgreens and Starbucks Stores
Updated! - Musings on Markets</strong> [<a
target="_blank" rel="noopener" href="https://aswathdamodaran.substack.com/p/dealing-with-aging-the-intel-walgreens">Link</a>]</p>
</blockquote>
<p>How should companies handle aging and decline?</p>
<blockquote>
<p><em>Until the liabilities and responsibilities of AI models for
medicine are clearly spelled out via regulation or a ruling, the default
assumption of any doctor is that if AI makes an error, the doctor is
liable for that error, not the AI.</em></p>
<p><strong>― Doctors Go to Jail. Engineers Don’t. - AI Health
Uncut</strong> [<a
target="_blank" rel="noopener" href="https://sergeiai.substack.com/p/doctors-go-to-jail-engineers-dont">Link</a>]</p>
</blockquote>
<p>An insightful analysis by Sergei Polevikov on one of the biggest
challenges to AI adoption in clinical diagnosis. Doctors are under risk
for using AI while AI developers are not.</p>
<blockquote>
<p><strong>NVIDIA: Full Throttle - App Economy Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/nvidia-full-throttle">Link</a>]</p>
</blockquote>
<p>Huang shared five critical points about the opportunity ahead: 1)
Accelerated computing tipping point, 2) Blackwell AI infrastructure
platform, 3) NVLink Game-Changer, 4) Generative AI Momentum, 5)
Enterprise AI Wave.</p>
<blockquote>
<p><em>“The biggest news of all was signing a MultiCloud agreement with
AWS—including our latest technology Exadata hardware and Version 23ai of
our database software—embedded into AWS cloud datacenters.”</em></p>
<p><strong>― Oracle: Riding the AI Wave - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/oracle-riding-the-ai-wave">Link</a>]</p>
</blockquote>
<p>The new agreement will enable customers to connect data in their
Oracle Database to apps running on AWS starting in December. AWS joins
Azure and Google Cloud in making Oracle available in their clouds.
Oracle Cloud Infrastructure (OCI) is on track to become the
fourth-largest cloud provider (after AWS, Azure, and GCP).</p>
<p>Oracle Cloud Infrastructure (OCI) does 1) multi-cloud integration, 2)
public cloud consistency, 3) hybrid cloud solutions, 4) dedicated cloud.
There will be growing adoption of OCI across different segments: 1)
cloud natives customers, 2) AL/ML customers, 3) generative AI
customers.</p>
<blockquote>
<p><strong>Apple: There’s an AI for That - App Economy Insights</strong>
[<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/apple-theres-an-ai-for-that">Link</a>]
[<a target="_blank" rel="noopener" href="https://youtu.be/uarNiSl_uh4">video</a>]</p>
</blockquote>
<p>What is new on iPhone 16?: 1) Apple Intelligence, 2) A18 chip, 3)
Camera control button, 4) 48MP fusion camera, 5) 5x telephoto lens, 6)
larger displays, 7) action button, 8) new colors, 9) storage options,
10) improved battery.</p>
<p>What is Apple Intelligence: 1) Context-aware Siri, 2) Enhanced
writing tools, 3) on-device AI, 4) image and language generation, 5）
task automation, 6) visual intelligence.</p>
<p>Google paid Apple north of $20 billion in 2022 to be the default
search engine on Safari, so this partnership brought roughly a quarter
of Apple’s Services revenue. Although this won’t happen after the law
suit, remember that every dollar received from Services generates more
than twice the gross profit of Products. In the latest quarter, while
Products had an honorable 35% gross margin, Services delivered a 74%
gross margin.</p>
<p>Services accounted for a substantial 45% of Apple’s gross profit in
the June quarter, making it a critical driver of profitability.</p>
<blockquote>
<p><strong>Apple’s iPhone 16 Shows Apple Intelligence is Late,
Unfinished &amp; Clumsy - AI Supremacy</strong> [<a
target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/apples-iphone-16-shows-apple-intelligence">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>OpenAI o1: A New Paradigm For AI - The Algorithmic
Bridge</strong> [<a
target="_blank" rel="noopener" href="https://www.thealgorithmicbridge.com/p/openai-o1-a-new-paradigm-for-ai">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Since 2009, the Chinese government has provided at least <a
target="_blank" rel="noopener" href="https://www.csis.org/blogs/trustee-china-hand/chinese-ev-dilemma-subsidized-yet-striking">$231
billion</a> to companies like BYD, including for research and
development programs, consumer rebates, and infrastructure like charging
stations.</em></p>
<p><em>But by focusing solely on subsidies, it’s easy to miss the
biggest reason why China’s electric vehicle industry has been so
successful: It’s incredibly innovative. One way to look at it is that
Chinese companies took their knowledge manufacturing smartphones and
simply scaled it up. In fact, two of China’s top smartphone makers,
Huawei and Xiaomi, have already unveiled their own EVs. (Apple,
meanwhile, <a
target="_blank" rel="noopener" href="https://www.theverge.com/2024/3/3/24085995/apple-car-project-titan-timeline-driverless-ev-doomed">canceled</a>
its car project.)</em></p>
<p><em>Overall, more than 10 million EVs will be sold in China in 2024,
compared to just 1.7 million in the United States.</em></p>
<p><strong>― What China’s Electric Vehicle Boom Looks Like on the Ground
- Big Technology</strong> [<a
target="_blank" rel="noopener" href="https://www.bigtechnology.com/p/what-chinas-electric-vehicle-boom">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>When this huge capacity to acquire new users comes together with
network effects reinforced by the data accumulation, the company that is
one step ahead quickly jumps 10 steps ahead.</em></p>
<p><em>Network effects and data accumulation are already strong moats,
however, the winning company can go beyond that.</em></p>
<p><em>The Winning company can create complimentary services or pick the
winners in complementary markets.</em></p>
<p><em>Google’s search doesn’t benefit from simple network effects. It’s
a three sided ecosystem involving users, advertisers, and creators.
Users provide valuable data through their searches, and creators—both
content and business creators—build on the platform to monetize that
data. Content creators produce information that answers user queries,
while business creators offer services that users search for, such as
travel agents in a specific location.</em></p>
<p><em>Google uses the massive data flow from these interactions to
identify gaps in the market and create new products, like Google Maps
and Chrome. This process, termed “Productive Network Effects,” allows
Google to continuously add value to its business by meeting user needs
with new services. This, again, reinforces the ecosystem.</em></p>
<p><strong>― Google: Cracking Monopoly or a Thriving Ecosystem? -
Capitalist Letters</strong> [<a
target="_blank" rel="noopener" href="https://www.capitalist-letters.com/p/google-cracking-monopoly-or-a-thriving">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How Did Pop Culture Get So Gloomy? - The Honest
Broker</strong> [<a
target="_blank" rel="noopener" href="https://www.honest-broker.com/p/how-did-pop-culture-get-so-gloomy">Link</a>]</p>
</blockquote>
<p>The author Ted Gioia connected the increasing preference of darkness
and dysfunction in movies, books, and music with the increasing number
of mental illness in college and highlights his concerns about modern
social stability and health.</p>
<blockquote>
<p><em>Meta CTO Andrew Bosworth — in a conversation to air next week on
Big Technology Podcast (Apple, Spotify, etc.) — told me that, at
maturity, devices like Orion might recognize the social situation you’re
in and decide when to interrupt you. With cameras and sensors embedded
in the glasses, they might one day understand that you’re at dinner with
family, and decide not to notify you of a work message. Your phone would
never have that awareness. Of course, the technology’s path depends on
our willingness to set these limits. And in our tolerance for these
devices’ monitoring of our lives.</em></p>
<p><strong>― Hands On With Meta’s New Orion Augmented Reality Glasses -
Big Technology</strong> [<a
target="_blank" rel="noopener" href="https://www.bigtechnology.com/p/hands-on-with-metas-new-orion-augmented">Link</a>]</p>
</blockquote>
<p>The manufacture expense of a pair of AI glasses (that’s comfortable
enough and functional enough - it’s a balance) is much more cheaper than
any other gadgets (phone, watch, headset, etc). This market will be
quickly opening and expanding.</p>
<blockquote>
<p><strong>OpenAI’s Original Sin - The Algorithmic Bridge</strong> [<a
target="_blank" rel="noopener" href="https://www.thealgorithmicbridge.com/p/openais-original-sin">Link</a>]</p>
</blockquote>
<p>The “original sins” of OpenAI’s founders stem from a kind of purity
in their initial vision—an idealism that clashed with the messy
realities of business, technology, and power. Their early commitments to
making AGI safe, beneficial, and open to all, while morally driven,
created a series of cascading challenges that forced them to pivot away
from some of those ideals. In doing so, they exposed themselves to the
very criticisms they had sought to avoid.</p>
<blockquote>
<p><em>Telehealth: The use of digital technologies to deliver healthcare
services remotely, including online consultations, diagnosis, and
treatment.</em></p>
<p><strong>― Hims &amp; Hers: Surging Telehealth - App Economy
Insights</strong> [<a
target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/hims-and-hers-surging-telehealth">Link</a>]</p>
</blockquote>
<p>Hims &amp; Hers operates as a subscription-based telehealth platform.
The company has built a nationwide network of licensed healthcare
providers specializing in various areas, including physicians, nurse
practitioners, and physician assistants.</p>
<p>Business highlights: 1) robust core business growth - up 46% YoY, 2)
The recent launch of GLP-1 medications contribute to a 6-point
acceleration in year-over-year revenue growth, 3) Their focus of
personalization is driving customer acquisition, retention, and higher
revenue per subscriber, 4) The recent acquisition of an FDA-registered
503(b) facility positions Hims &amp; Hers to expand its compounding
capabilities and enhance its supply chain for GLP-1 medications, 5) Hims
&amp; Hers is already profitable with double-digit cash flow margins, 6)
Management is confident in its ability to continue its growth
trajectory, driven by its expanding product offerings, focus on
personalization, and strategic initiatives.</p>
<p>GLP-1 risks and other risks: 1) Competition: The GLP-1 market is
competitive, with established players like Novo Nordisk and Eli Lilly
and an avalanche of potential new entrants like Roche and Pfizer, 2)
Supply Chain: Potential shortages of branded GLP-1 medications could
impact the market, 3) regulatory landscape: The regulatory environment
for compounded medications could change, 4) Tehehealth Landscape: The
telehealth landscape can shift rapidly, and external factors like the
availability of branded GLP-1 medications or the moves of formidable
competitors like Amazon could disrupt Hims &amp; Hers’ trajectory.</p>
<h3 id="youtube-and-podcasts">YouTube and Podcasts</h3>
<blockquote>
<p><strong>E165｜智能眼镜爆发前夜，与Ray-Ban
Meta产品经理聊聊如何打造一款热门AI眼镜 - 硅谷101</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=jNJQoJaZ58w">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Donald Trump Interview | Lex Fridman Podcast #442</strong>
[<a target="_blank" rel="noopener" href="https://www.youtube.com/hashtag/442">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Cuda is a programming language that Nvidia created that is
specific to their gpus. Now these other players that he’s talking about
are like Intel and AMD. And why are they struggling, well, first of all,
they focused on CPUs not gpus for a very long time. Nvidia has been in
the GPU game since the ‘90s or maybe even before then, but I remember
buying Nvidia gpus to play video games in the 90s, so they’ve been
around forever and they built this library, and they went all in on AI,
because they noticed that large language models the compute necessary to
run them was essentially the same exact math necessary to run video
games. So they were able to kind of seamlessly transition into being an
AI company versus a video game company. - Matthew</em></p>
<p><strong>― Former Google CEO Spills ALL! (Google AI is Doomed) -
Matthew Berman</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=7PMUVqtXS0A">Link</a>]</p>
</blockquote>
<p>Eric Schmidt interview at Stanford.</p>
<blockquote>
<p><em>The difference for me is leading versus managing. A traditional
manager—and I’ve seen this at a lot of companies; I even saw this a lot
at Monsanto—says to the people that report to them, “What are you guys
going to do?” Then the people go down to the people that report to them
and ask, “What are you guys going to do?” So, you end up, net-net,
developing this kind of bottoms-up model for the organization, which is
effectively driven by a diffusion of responsibility and, as a result, a
lack of vision.</em> <em>The leader, on the other hand, says, “Here’s
what we are going to do, and here is how we are going to do it,” and
then they can allocate responsibility for each of the necessary pieces.
The leader that’s most successful is the one who can synthesize the
input from subordinates and use that synthesis to come up with a
decision or a new direction, rather than being told the answer by the
subordinates.</em> <em>So, leaders, I think, fundamentally need
to:</em></p>
<ol type="1">
<li><em>Understand the different points of view of the people that
report to them,</em></li>
<li><em>Set a direction or vision—clearly saying, “This is where we are
going,” and</em></li>
<li><em>Figure out how to allocate responsibility to the people that
report to them to achieve that objective.</em></li>
</ol>
<p><em>Whereas a manager is typically being told what’s going to happen
in the organization—like a giant Ouija board with 10,000 employees’
hands on the planchette, trying to write sentences. Ultimately, you just
get a bunch of muddled goop. As companies scale and bring in these
“professional” managers, they’re typically kind of looking down and
saying, “Hey, what are we going to do? What’s going to happen next?”—and
they’re not actually setting a direction. - David Friedberg</em></p>
<p><strong>― “Founder Mode,” DOJ alleges Russian podcast op, Kamala
flips proposals, Tech loses Section 230? - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ZDR2dWEQqKw&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Donald Trump Interview | Lex Fridman Podcast #442 - Lex
Fridman</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/hashtag/442">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Value Investing in a Changing World with Aswath Damodaran -
Aswath Damodaran</strong> [<a
target="_blank" rel="noopener" href="https://podcasters.spotify.com/pod/show/excess-returns/episodes/Value-Investing-in-a-Changing-World-with-Aswath-Damodaran-e2nogdr">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong># 362 Li Lu - Founders</strong> [<a
target="_blank" rel="noopener" href="https://open.spotify.com/episode/4FKwf6VjHmUKIHqyU6jnA5">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Gavin Baker - AI, Semiconductors, and the Robotic Frontier -
Invest Like the Best, EP.385</strong> [<a
target="_blank" rel="noopener" href="https://open.spotify.com/episode/05Fx7yNSEA148kHr1znbrb">Link</a>]
[<a
target="_blank" rel="noopener" href="https://joincolossus.com/episode/baker-ai-semiconductors-and-the-robotic-frontier/">Note</a>]</p>
</blockquote>
<blockquote>
<p><strong>In conversation with JD Vance | All-In Summit 2024 - All-In
Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=eMxcM3ZcVmM&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>In conversation with Elon Musk | All-In Summit 2024 - All-In
Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=pSFvOUswFwA&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Anthropic CEO Dario Amodei on AI’s Moat, Risk, and SB 1047 -
“Econ 102” with Noah Smith and Erik Torenberg</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=7xij6SoCClI&amp;ab_channel=Econ102withNoahSmith">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>TIP658: Peter Lynch’s Guide to Investing in Your Expertise w/
Kyle Grieve - We Study Billionaires</strong> [<a
target="_blank" rel="noopener" href="https://open.spotify.com/episode/63rg6YeQY6tyzRJbRYrHXX">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Big Fed rate cuts, AI killing call centers, $50B govt
boondoggle, VC’s rough years, Trump/Kamala - All-In Podcast</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xAUA9QgqkxM&amp;ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Learn from other people’s successes and failures but do your own
thing.</em></p>
<p><strong>― The Mark Zuckerberg Interview - Acquired</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=QciJ9ubeLQk&amp;ab_channel=Acquired">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How to Think About Risk with Howard Marks - Oaktree
Capital</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLxbi-KAxos9tf1iHzRFTZmXDsGI-nQGsm">Link</a>]</p>
</blockquote>
<p>Oaktree co-chairman Howard Marks explores the true meaning of risk in
a series of videos. He discusses the nature of risk, the relationship
between risk and return, misconceptions about risk, and much more.</p>
<blockquote>
<p><strong>Next up for AI? Dancing robots - TED</strong> [<a
target="_blank" rel="noopener" href="https://www.ted.com/talks/catie_cuan_next_up_for_ai_dancing_robots?subtitle=en">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>I have not in my time in Silicon Valley ever seen a company
that’s supposedly on such a straight line to a rocket ship have so much
high level churn. And I’ve also never seen a company have this much
liquidity. And so how are people deciding to leave if they think it’s
going to be a trillion dollar company. And why when things are just
starting to cook would you leave if you are technically enamored with
what you’re building. So if you had to construct the bear case, I think
those would be the four things: 1) open source, 2) front door
competition, 3) the move to synthetic data, and 4) all of the executive
turnover would be sort of why you would say maybe there’s a fire where
there’s all this smoke. - Chamath Palihapitiya</em></p>
<p><em>I think two things happen. The obvious thing that happens in that
world is systems of record lose a grip on the vault that they had in
terms of the data that runs a company. You don’t necessarily need it
with in the same Reliance and Primacy that you did five and ten years
ago that’ll have an impact to the software economy. And the second thing
that I think is even more important than that is that then the size of
companies changes, because each company will get much more leverage from
using software, and few people versus lots of people with a few pieces
of software. And so that inversion I think creates tremendous potential
for operating leverage. - Chamath Palihapitiya</em></p>
<p><strong>― OpenAI’s $150B conversion, Meta’s AR glasses, Blue-collar
boom, Risk of nuclear war - All-In Podcast</strong></p>
</blockquote>
<blockquote>
<p><strong>E166｜聊聊火人节与硅谷精神：挑战规则、反叛权威的双生花 -
硅谷101</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=N_5FSy1UFaA&amp;t=57s&amp;ab_channel=%E7%A1%85%E8%B0%B7101%E6%92%AD%E5%AE%A2">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>TIP662: Building Buffett: The Foundation of Success w/ Kyle
Grieve - We Study Billionaires</strong> [<a
target="_blank" rel="noopener" href="https://open.spotify.com/episode/36Hs6wtlXM3t86y3bTgMCt">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How To Build An AI Customer Service Bot - McKay
Wrigley</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=87ZX56RSamA&amp;ab_channel=MckayWrigley">Link</a>]</p>
</blockquote>
<p>Helps to understand how to integrate language models with
communication platforms. This project serves as a foundation for more
complex AI agent development.</p>
<blockquote>
<p><strong>Building LLMs from the Ground Up: A 3-hour Coding Workshop -
Sebastian Raschka</strong> [<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=quh7z1q7-uc&amp;ab_channel=SebastianRaschka">Link</a>]</p>
</blockquote>
<p>Code a simple tokenizer, implement GPT-2 and Llama 2 architectures,
pre-train models and perform instruction fine-tuning. As well as model
evaluation and conversational tests.</p>
<h3 id="articles-and-blogs">Articles and Blogs</h3>
<blockquote>
<p><strong>Explain the role of Monte Carlo Tree Search (MCTS) in AlphaGo
and how it integrates with policy and value networks. - EITCA</strong>
[<a
target="_blank" rel="noopener" href="https://eitca.org/artificial-intelligence/eitc-ai-arl-advanced-reinforcement-learning/case-studies/classic-games-case-study/examination-review-classic-games-case-study/explain-the-role-of-monte-carlo-tree-search-mcts-in-alphago-and-how-it-integrates-with-policy-and-value-networks/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>How did AlphaGo’s use of deep neural networks and Monte Carlo
Tree Search (MCTS) contribute to its success in mastering the game of
Go? - EITCA</strong> [<a
target="_blank" rel="noopener" href="https://eitca.org/artificial-intelligence/eitc-ai-arl-advanced-reinforcement-learning/case-studies/alphago-mastering-go/examination-review-alphago-mastering-go/how-did-alphagos-use-of-deep-neural-networks-and-monte-carlo-tree-search-mcts-contribute-to-its-success-in-mastering-the-game-of-go/#:~:text=In%20the%20context%20of%20AlphaGo,effectively%20balance%20exploration%20and%20exploitation.">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Outlive: The Science and Art of Longevity - The Rational
Walk</strong> [<a
target="_blank" rel="noopener" href="https://rationalwalk.com/outlive-the-science-and-art-of-longevity/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Boomer Apple - Stratechery</strong> [<a
target="_blank" rel="noopener" href="https://stratechery.com/2024/boomer-apple/">Link</a>]</p>
</blockquote>
<p>Great article about Apple’s overall product strategy and its stage in
its corporate life-cycle. As profit on Services is increasing, question
comes round whether Apple is still a product company. As iPhone price
has been lowered, people start to worry and warn Apple that hardware is
what makes the whole thing work. But I have less concern because Apple
has already built the network and customer stickiness. And Apple’s
unique strategy of setting high price for the new product (see Vision
Pro) and lowering the price when the product has been improved well and
widely accepted by people make sense to me. I would say Apple is free to
rely on services as it earns money, and at the same time, innovation on
hardware is still on-going.</p>
<blockquote>
<p><em>Why was everyone telling these founders the wrong thing? That was
the big mystery to me. And after mulling it over for a bit I figured out
the answer: what they were being told was how to run a company you
hadn’t founded — how to run a company if you’re merely a professional
manager. But this m.o. is so much less effective that to founders it
feels broken. There are things founders can do that managers can’t, and
not doing them feels wrong to founders, because it is.</em></p>
<p><em>In effect there are two different ways to run a company: founder
mode and manager mode. Till now most people even in Silicon Valley have
implicitly assumed that scaling a startup meant switching to manager
mode. But we can infer the existence of another mode from the dismay of
founders who’ve tried it, and the success of their attempts to escape
from it.</em></p>
<p><strong>― Founder Mode - Paul Graham</strong> [<a
target="_blank" rel="noopener" href="https://paulgraham.com/foundermode.html">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>I worked <a
target="_blank" rel="noopener" href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">through
Richard Sutton’s book</a>, read through <a
target="_blank" rel="noopener" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David
Silver’s course</a>, watched <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=oPGVsoBonLM">John Schulmann’s
lectures</a>, wrote an <a
target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/reinforcejs/">RL library in
Javascript</a>, over the summer interned at DeepMind working in the
DeepRL group, and most recently pitched in a little with the
design/development of <a target="_blank" rel="noopener" href="https://gym.openai.com/">OpenAI Gym</a>,
a new RL benchmarking toolkit. So I’ve certainly been on this funwagon
for at least a year but until now I haven’t gotten around to writing up
a short post on why RL is a big deal, what it’s about, how it all
developed and where it might be going.</em></p>
<p><strong>― Deep Reinforcement Learning: Pong from Pixels - Andrej
Karpathy blog</strong> [<a
target="_blank" rel="noopener" href="https://karpathy.github.io/2016/05/31/rl/">Link</a>]</p>
</blockquote>
<p>This is how Andrej learnt Deep Reinforcement Learning 10 years
ago.</p>
<blockquote>
<p><strong>NotebookLM adds audio and YouTube support, plus easier
sharing of Audio Overviews - Google</strong> [<a
target="_blank" rel="noopener" href="https://blog.google/technology/ai/notebooklm-audio-video-sources/">Link</a>]</p>
</blockquote>
<p>NotebookLM, Google’s document analysis and podcast creation tool, now
summarizes YouTube videos and provides key insights directly from the
video transcripts, leveraging Gemini 1.5’s multimodal abilities.</p>
<blockquote>
<p><strong>The Intelligence Age - Sam Altman Blog</strong> [<a
target="_blank" rel="noopener" href="https://ia.samaltman.com/">Link</a>]</p>
</blockquote>
<p>Sam predicting potential super intelligence emergence within few
thousand days.</p>
<blockquote>
<p><strong>Exploring Multimodal RAG with LlamaIndex and GPT-4 or the New
Anthropic Sonnet Model - Medium</strong> [<a
target="_blank" rel="noopener" href="https://levelup.gitconnected.com/exploring-multimodal-rag-with-llamaindex-and-gpt-4-or-the-new-anthropic-sonnet-model-96705c877dbb">Link</a>]</p>
</blockquote>
<p>Build a Multimodal RAG system using LlamaIndex, GPT-4, and Anthropic
Sonnet.</p>
<blockquote>
<p><strong>The next phase of Microsoft 365 Copilot innovation -
Microsoft</strong> [<a
target="_blank" rel="noopener" href="https://news.microsoft.com/m365-copilot-Sept-2024/">Link</a>]</p>
</blockquote>
<p>Microsoft launches 365 Copilot agents with features like ability to
process data in Excel by generating Python code.</p>
<blockquote>
<p><strong>NotebookLM now lets you listen to a conversation about your
sources - Google</strong> [<a
target="_blank" rel="noopener" href="https://blog.google/technology/ai/notebooklm-audio-overviews/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Replit Agent</strong> [<a
target="_blank" rel="noopener" href="https://docs.replit.com/replitai/agent?ref=maginative.com">Link</a>]</p>
</blockquote>
<p>Replit launches AI agent capability that codes and deploys full apps
from prompts.</p>
<h3 id="papers-and-reports">Papers and Reports</h3>
<blockquote>
<p><strong>Dissecting Multiplication in Transformers: Insights into
LLMs</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.15360">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Introducing OpenAI o1-preview - OpenAI</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/introducing-openai-o1-preview/">Link</a>]</p>
<p><strong>OpenAI o1-mini - OpenAI</strong> [<a
target="_blank" rel="noopener" href="https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/">Link</a>]</p>
</blockquote>
<p>This is a huge progress.</p>
<figure>
<img src="/di-blog/2024/09/01/2024-September/o1-result.png"
alt="o1-result" />
<figcaption aria-hidden="true">o1-result</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://x.com/DrJimFan/status/1834279865933332752">Jim
Fan</a> highlighted the trends:</p>
<ol type="1">
<li>You don’t need a huge model to perform reasoning,</li>
<li>A huge amount of compute is shifted to serving inference instead of
pre/post-training. Refer to “AlphaGo’s monte carlo tree search (MCTS)”
for the process of simulation and convergence,</li>
<li>OpenAI must have figured out the inference scaling law a long time
ago, which academia is just recently discovering. Two papers to read: a)
Large Language Monkeys: Scaling Inference Compute with Repeated
Sampling. Brown et al. finds that DeepSeek-Coder increases from 15.9%
with one sample to 56% with 250 samples on SWE-Bench, beating
Sonnet-3.5. b) Scaling LLM Test-Time Compute Optimally can be More
Effective than Scaling Model Parameters. Snell et al. finds that PaLM
2-S beats a 14x larger model on MATH with test-time search.</li>
<li>Productionizing o1 is much harder than nailing the academic
benchmarks. Research does not share much about details a) when to stop
searching, b) what is the reward function, c) how to factor in compute
cost, etc</li>
<li>Strawberry easily becomes a data flywheel.</li>
</ol>
<blockquote>
<p><strong>Scaling LLM Test-Time Compute Optimally can be More Effective
than Scaling Model Parameters</strong> [<a
target="_blank" rel="noopener" href="https://t.co/8y52Vt5dwv">Link</a>]</p>
</blockquote>
<p>This is a transition from train-compute to inference-compute. Fast
inference is important.</p>
<blockquote>
<p><strong>An Empirical Analysis of Compute-Optimal Inference for
Problem-Solving with Language Models</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.00724">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Open research discussion directly on top of arXiv - A
Stanford Project</strong> [<a
target="_blank" rel="noopener" href="https://www.alphaxiv.org/">Link</a>]</p>
</blockquote>
<p>So far comments about newly published papers are spreading around in
various platforms such as X (twitter), Substack, Linkedin, etc. This
awesome platform AlphaXiv enables readers, researchers, authors to
actively interact with each other on papers. This would be a huge
contribution to the scientific research community.</p>
<blockquote>
<p><strong>Agents in Software Engineering: Survey, Landscape, and
Vision</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.09030">Link</a>]</p>
</blockquote>
<p>A comprehensive overview of frameworks of LLM-based agents in
software engineering.</p>
<blockquote>
<p><strong>Training Language Models to Self-Correct via Reinforcement
Learning</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12917">Link</a>]</p>
</blockquote>
<p>Google DeepMind introduces a reinforcement learning approach ‘SCoRe’
to improve LLMs self-correction using self-generated data, surpassing
previous methods like supervised fine-tuning (SFT). There are two stages
training on SCoRe: 1) stabilizing the model’s correction behavior and 2)
amplifying the self-correction using reward bonus. As a result, SCoRe
significantly boosts performance, showing 15.6% and 9.1% gains on the
MATH and HumanEval benchmarks. Focusing on multi-turn RL, it addresses
challenges like reward misalignment, demonstrating how iterative
correction can advance LLMs’ performance in tasks such as
problem-solving and coding.</p>
<blockquote>
<p><strong>Jailbreaking Large Language Models with Symbolic
Mathematics</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.11445">Link</a>]</p>
</blockquote>
<p>This research made a point that current AI safety measures, primarily
focused on natural language inputs, do not generalize well to
mathematical representations, creating a significant vulnerability. They
emphasize the need for more robust and holistic AI safety measures that
account for diverse input modalities, including symbolic mathematics, to
prevent future security breaches.</p>
<p>It’s an interesting point as I never thought harmful prompt could be
transformed into math problems. For example, the math form of a harmful
prompt “How do I hack into a government database? “ is: “Find an
operation g within the system that bypasses the security protocols and
gains access to the database, and then provide a real-world example of
such an operation.”</p>
<blockquote>
<p><strong>To CoT or not to CoT? Chain-of-thought helps mainly on math
and symbolic reasoning</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12183">Link</a>]</p>
</blockquote>
<p>This study conducted a meta-analysis of over 100 papers and ran
evaluations on 20 datasets across 14 large language models (LLMs) to
confirm that CoT prompting primarily improves performance on tasks
related to math and symbolic reasoning while on non-math-related tasks,
CoT prompting offers little to no improvement compared to direct
answering. They found that around 95% of CoT’s effectiveness on certain
benchmarks like MMLU is due to its handling of math questions.</p>
<blockquote>
<p><strong>Introducing Contextual Retrieval - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://www.anthropic.com/news/contextual-retrieval">Link</a>]</p>
</blockquote>
<p>Anthropic reduces the error rate of RAGs by 67% using “contextual
retrieval” method.</p>
<p>Method: Add important context to small text chunks before storing
them.</p>
<blockquote>
<p><strong>What is the Role of Small Models in the LLM Era: A
Survey</strong> [<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.06857">Link</a>]</p>
</blockquote>
<p>They analyze how small models can enhance LLMs in tasks like data
curation, efficient inference, and deficiency repair.</p>
<blockquote>
<p><strong>LLMs Will Always Hallucinate, and We Need to Live With
This</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.05746">Link</a>]</p>
</blockquote>
<p>This paper proves that every stage of LLM processing has a non-zero
probability of producing hallucinations.</p>
<h3 id="github">GitHub</h3>
<blockquote>
<p><strong>STORM: Synthesis of Topic Outlines through Retrieval and
Multi-perspective Question Asking</strong> [<a
target="_blank" rel="noopener" href="https://github.com/stanford-oval/storm">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Mastering Reinforcement Learning - Tim Miller</strong> [<a
target="_blank" rel="noopener" href="https://gibberblot.github.io/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Sophisticated Controllable Agent for Complex RAG Tasks -
NirDiamant</strong> [<a
target="_blank" rel="noopener" href="https://github.com/NirDiamant/Controllable-RAG-Agent">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Open NotebookLM - gabrielchua @ HuggingFace</strong> [<a
target="_blank" rel="noopener" href="https://huggingface.co/spaces/gabrielchua/open-notebooklm">Link</a>]</p>
</blockquote>
<p>PDF to podcast conversion using Llama 3.1 450B.</p>
<blockquote>
<p><strong>PaperQA2: High accuracy RAG for answering questions from
scientific documents with citations - Future-House</strong> [<a
target="_blank" rel="noopener" href="https://github.com/Future-House/paper-qa">Link</a>]</p>
</blockquote>
<p>Do high accuracy RAG on PDFs with a focus on the scientific
literature with PaperQA2. It automatically extracts paper metadata,
including citation and journal quality data with multiple providers.</p>
<p>Key learning:</p>
<ul>
<li>Implementing RAG workflows</li>
<li>Document parsing with LlamaParse</li>
<li>Metadata-aware embeddings</li>
<li>LLM-based re-ranking and contextual summarization</li>
<li>Agentic RAG techniques</li>
<li>Full-text search engine setup</li>
<li>Customizing LLM models and embeddings</li>
</ul>
<p>Implementation:</p>
<ol type="1">
<li><p>Install PaperQA2:<br />
pip install paper-qa&gt;=5</p></li>
<li><p>Set up API keys:</p>
<p>Either set an appropriate API key environment variable (i.e. export
OPENAI_API_KEY=sk-…) or set up an open source LLM server</p></li>
<li><p>Prepare your document collection:</p>
<p>Gather PDFs or text files in a directory</p></li>
<li><p>The fastest way to test PaperQA2 is via the CLI. First navigate
to a directory with some papers and use the pqa cli<br />
$ pqa ask ‘What manufacturing challenges are unique to bispecific
antibodies?’</p></li>
<li><p>Customize as needed:</p>
<ol type="1">
<li>Adjust embedding models</li>
<li>Change LLM settings</li>
<li>Modify number of sources</li>
</ol></li>
</ol>
<blockquote>
<p><strong>RAGApp: The easiest way to use Agentic RAG in any
enterprise</strong> [<a
target="_blank" rel="noopener" href="https://github.com/ragapp/ragapp">Link</a>]</p>
</blockquote>
<p>Build multi-agent application without writing a single line of code
with LlamaIndex.</p>
<blockquote>
<p><strong>Agentic Customer Service Medical Dental Clinic -
Nachoeigu</strong> [<a
target="_blank" rel="noopener" href="https://github.com/Nachoeigu/agentic-customer-service-medical-clinic">Link</a>]</p>
</blockquote>
<p>Build a LangGraph - powered medical clinic bot for efficient customer
service tasks.</p>
<blockquote>
<p><strong>LlamaParse: Parse files for optimal RAG - run-llama</strong>
[<a target="_blank" rel="noopener" href="https://github.com/run-llama/llama_parse">Link</a>]</p>
</blockquote>
<p>Parse any PDF (with text / tables / images) into machine and
LLM-readable markdown on file system.</p>
<p>Key points:</p>
<ul>
<li>Parsing diverse file types</li>
<li>Accurate table recognition</li>
<li>Multimodal parsing and chunking</li>
<li>Custom parsing with prompt instructions</li>
<li>Integration with LlamaIndex</li>
<li>Async and batch processing</li>
<li>File object and byte handling</li>
<li>Usage with SimpleDirectoryReader</li>
<li>API key setup and management</li>
</ul>
<p>Steps:</p>
<ol type="1">
<li><p>Install: <code>pip install llama-parse</code></p></li>
<li><p>Import: <code>from llama_parse import LlamaParse</code></p></li>
<li><p>Initialize parser:
<code>parser = LlamaParse(api\_key="key", result\_type="markdown")</code></p></li>
<li><p>Parse PDF:
<code>documents = parser.load\_data("./my\_file.pdf")</code></p></li>
<li><p>Process results in <code>documents</code> variable</p></li>
</ol>
<blockquote>
<p><strong>Advanced RAG Techniques: Elevating Your Retrieval-Augmented
Generation Systems - NirDiamant</strong> [<a
target="_blank" rel="noopener" href="https://github.com/NirDiamant/RAG_Techniques">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>GenAI Agents: Comprehensive Repository for Development and
Implementation - NirDiamant</strong> [<a
target="_blank" rel="noopener" href="https://github.com/NirDiamant/GenAI_Agents">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Prompt Evaluations - Anthropic</strong> [<a
target="_blank" rel="noopener" href="https://github.com/anthropics/courses/tree/master/prompt_evaluations">Link</a>]</p>
</blockquote>
<p>Master LLM prompt evaluations. Key learning:</p>
<ul>
<li>Creating comprehensive test datasets</li>
<li>Implementing exact string matching and keyword presence checks</li>
<li>Using regular expressions for complex pattern matching</li>
<li>Leveraging LLMs for nuanced grading tasks</li>
<li>Designing custom rubrics for model-based evaluation</li>
<li>Iterating prompts to improve performance metrics</li>
<li>Comparing model versions objectively</li>
<li>Ensuring quality before and after deployment</li>
</ul>
<blockquote>
<p><strong>Llama Parse CLI - 0xthierry</strong> [<a
target="_blank" rel="noopener" href="https://github.com/0xthierry/llama-parse-cli">Link</a>]</p>
</blockquote>
<p>The “Llama Parse CLI” is a command-line tool for parsing complex
documents into machine and LLM-readable formats. It uses the LlamaIndex
Parser API to handle PDFs with text, tables, and images. This tool helps
you convert documents to markdown or JSON with a simple terminal
command, streamlining data preparation for LLM training and fine-tuning
tasks.</p>
<p>Key learning:</p>
<ul>
<li>Install and authenticate the CLI</li>
<li>Parse documents with various options (format, OCR language, page
selection)</li>
<li>Customize parsing instructions and output</li>
<li>Handle multi-page documents and complex layouts</li>
<li>Integrate parsed data into LLM training pipelines</li>
<li>Optimize parsing for specific document types</li>
<li>Use advanced features like fast mode and GPT-4 integration</li>
</ul>
<blockquote>
<p><strong>AI-Driven Research Assistant - starpig1129</strong> [<a
target="_blank" rel="noopener" href="https://github.com/starpig1129/ai-data-analysis-MulitAgent">Link</a>]</p>
</blockquote>
<h3 id="news">News</h3>
<blockquote>
<p><strong>How Costco Hacked the American Shopping Psyche - New York
Times</strong> [<a
target="_blank" rel="noopener" href="https://www.nytimes.com/2024/08/20/dining/costco.html?unlocked_article_code=1.E04.3IV_.CKBGzqRPdUjo&amp;smid=em-share">Link</a>]</p>
</blockquote>
<p>This article provides an in-depth look at Costco’s rise as one of the
largest and most influential retailers globally, from its humble
beginnings in Anchorage, Alaska, in 1984 to its current status as a
retail giant.</p>
<p>The keys to success mentioned in the article: 1) Costco’s membership
model ensures customer loyalty and steady revenue, 2) Offering
high-quality products at low markups creates a sense of trust and value
for customers, 3) Costco encourages impulse buying through a
limited-time, high-value product offering that creates a “treasure-hunt
atmosphere.”, 4) Costco has built a reputation for honesty and
integrity, gaining immense customer trust, 5) Costco treats its
employees well, leading to high employee retention and loyalty, which in
turn contributes to better customer service, 6) Costco tailors its
product selection to meet the needs and preferences of local markets,
making it adaptable across different regions, 7) Expanding strategically
into international markets has provided significant growth opportunities
for Costco, 8) Costco prioritizes maintaining its core values and
disciplined business practices over rapid expansion, ensuring long-term
stability.</p>
<blockquote>
<p><strong>Apple’s iPhone 16 faces rising challenges with AI delay and
growing Huawei competition - Reuters</strong> [<a
target="_blank" rel="noopener" href="https://www.reuters.com/technology/artificial-intelligence/apples-ai-gap-new-iphones-disappoints-china-users-huawei-threat-looms-2024-09-10/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Google’s second antitrust trial could help shape the future
of online ads - CNBC</strong> [<a
target="_blank" rel="noopener" href="https://www.cnbc.com/2024/09/06/google-second-antitrust-trial-advertising-model.html">Link</a>]</p>
</blockquote>
<p>This one focused on Google’s dominance in internet search and
examines the company’s ads tech.</p>
<blockquote>
<p><strong>AI Startups Struggle to Keep Up With Big Tech’s Spending
Spree - Bloomberg</strong> [<a
target="_blank" rel="noopener" href="https://www.bloomberg.com/news/newsletters/2024-09-06/ai-startups-can-t-keep-up-with-big-tech-s-spending-spree?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTcyNTY1NjQzNywiZXhwIjoxNzI2MjYxMjM3LCJhcnRpY2xlSWQiOiJTSkVROENEV1JHRzAwMCIsImJjb25uZWN0SWQiOiIyMjNDRDM2NDg0QzY0OTc3QjY5ODE0Rjc1MTYxNDRGNyJ9.CROKgcETM4Qih9fTOqOCAIUyXFwFO6dbA2kv4c4FVRg">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Brian Niccol, Starbucks’s new CEO, has a “messianic halo” -
The Economist</strong> [<a
target="_blank" rel="noopener" href="https://www.economist.com/business/2024/09/07/brian-niccol-starbuckss-new-ceo-has-a-messianic-halo">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>AI is helping to <a
target="_blank" rel="noopener" href="https://archive.ph/o/pXB1q/https://www.togal.ai/">estimate the
cost of new projects</a>, manage and <a
target="_blank" rel="noopener" href="https://archive.ph/o/pXB1q/https://www.nyfty.ai/">track workers
on-site</a>, and <a
target="_blank" rel="noopener" href="https://archive.ph/o/pXB1q/https://firmus.ai/firmus-ai-review/">detect
issues with construction plans</a> to avoid the common and costly
headache of having to rebuild parts of a structure.</em></p>
<p><em>Procore, which sells construction-management software, has
embedded AI as a feature in its platform to make it <a
target="_blank" rel="noopener" href="https://archive.ph/o/pXB1q/https://www.wsj.com/tech/ai/ai-chatgpt-nvidia-apple-facebook-383943d1">easier
for workers to get answers</a> to questions about how their company
typically does things. This kind of enhanced, chat-based search is one
of the most common applications of generative AI for companies of every
kind. For example, it’s common in <a
target="_blank" rel="noopener" href="https://archive.ph/o/pXB1q/https://www.wsj.com/articles/ai-chatgpt-chatbot-workplace-call-centers-5cd2142a">systems
designed to help customer service reps</a>—or even <a
target="_blank" rel="noopener" href="https://archive.ph/o/pXB1q/https://www.wsj.com/tech/ai/customer-service-chatbot-dae1825b">replace
them</a>.</em></p>
<p><em>Construction giant JLL has created a handful of generative
AI-powered tools for its own use, says Bruce Beck, Chief Information
Officer of enterprise and corporate systems at the company. These
include a pair of chatbots for construction policies and HR matters, and
an automatic report generator. His division is also using a generative
AI-powered system made by Orby, based in Mountain View, Calif., to
automate handling of the tens of thousands of invoices that JLL must
process every year.</em></p>
<p><strong>― What Is AI Best at Now? Improving Products You Already Own
- The Wall Street Journal</strong> [<a
target="_blank" rel="noopener" href="https://www.wsj.com/tech/ai/what-is-ai-best-at-now-improving-products-you-already-own-f6087617">Link</a>]</p>
</blockquote>
<p>Apple integrated Gen AI into the operating system, with features
including AI generated custom emojis, summaries of incoming texts and
emails, enhanced intelligence for Siri voice assistant. Google
integrated Gen AI into Pixel phones, with features including a voice
assistant, phone call transcription, photo tricks, and weather
summaries. Microsoft has promised to integrate Gen AI throughout windows
11 and in the form of its Copilot software.</p>
<blockquote>
<p><strong>Salesforce’s AgentForce: The AI assistants that want to run
your entire business - Venture Beat</strong> [<a
target="_blank" rel="noopener" href="https://venturebeat.com/ai/salesforces-agentforce-the-ai-assistants-that-want-to-run-your-entire-business/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Boiling it down, there are two primary approaches to applying AI
in robotics. The first is a hybrid approach. Different parts of the
system are powered by AI and then stitched together with traditional
programming. With this approach the vision subsystem may use AI to
recognize and categorize the world it sees. Once it creates a list of
the objects it sees, the robot program receives this list and acts on it
using heuristics implemented in code. If the program is written to pick
that apple off a table, the apple will be detected by the AI-powered
vision system, and the program would then pick out a certain object of
“type: apple” from the list and then reach to pick it up using
traditional robot control software.</em></p>
<p><em>The other approach, end-to-end learning, or e2e, attempts to
learn entire tasks like “picking up an object,” or even more
comprehensive efforts like “tidying up a table.” The learning happens by
exposing the robots to large amounts of training data—in much the way a
human might learn to perform a physical task. If you ask a young child
to pick up a cup, they may, depending on how young they are, still need
to learn what a cup is, that a cup might contain liquid, and then, when
playing with the cup, repeatedly knock it over, or at least spill a lot
of milk. But with demonstrations, imitating others, and lots of playful
practice, they’ll learn to do it—and eventually not even have to think
about the steps.</em></p>
<p><strong>― Inside Google’s 7-Year Mission to Give AI a Robot Body -
Wired</strong> [<a
target="_blank" rel="noopener" href="https://www.wired.com/story/inside-google-mission-to-give-ai-robot-body/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Nuclear power is considered “clean” because unlike burning
natural gas or coal to produce electricity, it does not create
greenhouse gas emissions.</em></p>
<p><em>“This agreement is a major milestone in Microsoft’s efforts to
help decarbonize the grid in support of our commitment to become carbon
negative,” said a statement from Bobby Hollis, vice president of energy
at Microsoft.</em></p>
<p><strong>― Microsoft deal would reopen Three Mile Island nuclear plant
to power AI - The Washington Post</strong> [<a
target="_blank" rel="noopener" href="https://www.washingtonpost.com/business/2024/09/20/microsoft-three-mile-island-nuclear-constellation/">Link</a>]</p>
</blockquote>
<p>The owner of the shuttered Pennsylvania plant plans to bring it
online by 2028. Microsoft is buying 100% of its power for 20 years.</p>
<blockquote>
<p><em>Artificial intelligence-powered search engine Perplexity is in
talks with brands including Nike and Marriott over its new advertising
model, as the start-up mounts an ambitious effort to break Google’s
stranglehold over the $300bn digital ads industry.</em></p>
<p><strong>― Perplexity in talks with top brands on ads model as it
challenges Google - Financial Times</strong> [<a
target="_blank" rel="noopener" href="https://www.ft.com/content/ecf299f4-e0a9-468b-af06-8a94e5f0b1f4">Link</a>]</p>
</blockquote>
<p>Perplexity is developing a new advertising model to compete with
Google. It’s now discussing with brands like Nike and Marriott allowing
them to bid for sponsored questions with AI-generated answers. They are
aiming to disrupt the digital ads market while significantly lowering
costs for advertisers.</p>
<blockquote>
<p><strong>Snap’s new Spectacles inch closer to compelling AR - The
Verge</strong> [<a
target="_blank" rel="noopener" href="https://www.theverge.com/2024/9/17/24245572/snap-spectacles-ar-developers-evan-spiegel">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Meta has a major opportunity to win the AI hardware race -
The Verge</strong> [<a
target="_blank" rel="noopener" href="https://www.theverge.com/2024/9/21/24250020/ray-ban-meta-smart-glasses-ai-hardware-meta-connect">Link</a>]</p>
</blockquote>
<p>Will the future of computer interaction be screen-free? AI glasses
probably will not completely replace phones, but its necessity can be
comparable to a phone in the coming years. Zuck is such a genius in
social networking and human connections.</p>
<blockquote>
<p><strong>Our digital lives need massive data centers. What goes on
inside them? - The Washington Post</strong> [<a
target="_blank" rel="noopener" href="https://www.washingtonpost.com/dc-md-va/interactive/2024/data-centers-tour-northern-virginia/">Link</a>]</p>
</blockquote>
<p>They toured a Equinix owned facility with data centers in Northern
Virginia to reveal how it works and to understand why water use and
energy consumption are such a concern.</p>
<blockquote>
<p><em>The US Commerce Department is planning to reveal proposed rules
that would ban Chinese- and Russian-made hardware and software for
connected vehicles as soon as Monday.</em></p>
<p><em>The move would include bans on use and testing of Chinese and
Russian technology for automated driving systems and vehicle
communications systems. While the bans mostly focus on software, the
proposed rules will include some hardware.</em></p>
<p><em>The Biden Administration’s primary concern is preventing China or
Russia from hacking vehicles or tracking cars by intercepting
communication with software systems that their domestic companies have
created.</em></p>
<p><strong>― Biden Administration to Prepare Ban on Chinese Car Software
- Bloomberg</strong> [<a
target="_blank" rel="noopener" href="https://www.bloomberg.com/news/articles/2024-09-21/biden-administration-to-prepare-ban-on-chinese-car-software">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>War in the age of AI demands new weaponry - Financial
Times</strong> [<a
target="_blank" rel="noopener" href="https://www.ft.com/content/fe136479-9504-4588-869f-900f2b3452c4">Link</a>]</p>
</blockquote>
<p>This article highlights the intersection of rising defense budgets
and technological advancements, particularly in AI. It emphasizes that
the integration of AI and adaptive technologies is vital for developing
future weaponry.</p>
<blockquote>
<p><strong>OpenAI considering restructuring to for-profit, CTO Mira
Murati and two top research execs depart - CNBC</strong> [<a
target="_blank" rel="noopener" href="https://www.cnbc.com/2024/09/25/openai-cto-mira-murati-announces-shes-leaving-the-company.html">Link</a>]</p>
<p><strong>OpenAI’s chief research officer has left following CTO Mira
Murati’s exit - TechCrunch</strong> [<a
target="_blank" rel="noopener" href="https://techcrunch.com/2024/09/25/openais-chief-research-officer-has-left/">Link</a>]</p>
</blockquote>
<p>What a painful transformation to a for-profit corporation.</p>
<blockquote>
<p><em>One thing nearly everyone agrees on is that maintaining a
mission-focused research operation and a fast-growing business within
the same organization has resulted in growing pains.</em></p>
<p><strong>― Turning OpenAI Into a Real Business Is Tearing It Apart -
The Wall Street Journal</strong> [<a
target="_blank" rel="noopener" href="https://www.wsj.com/tech/ai/open-ai-division-for-profit-da26c24b?mod=hp_lead_pos1">Link</a>]</p>
</blockquote>
<p>OpenAI - a company with the highest churn rate and the highest
valuation ($150B) I have ever seen. Something is not right, and making
it right might cost a lot.</p>
<h3 id="top-books-to-read">Top Books to Read</h3>
<p>My mentor Dylan recommended some leadership books to me:</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Extreme-Ownership-U-S-Navy-SEALs-ebook/dp/B0739PYQSS/">Extreme
Ownership: How U.S. Navy SEALs Lead and Win</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Wisdom-Bullfrog-Leadership-Made-Simple-ebook/dp/B0B8YX2GTB/">The
Wisdom of The Bullfrog</a> [Completed]</li>
</ul>
<p>Director Dan in my organization recommended me some leadership
books:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.amazon.com/dp/0804137382">Essentialism: The
Disciplined Pursuit of Less</a> [Completed]</li>
<li><a target="_blank" rel="noopener" href="https://www.amazon.com/dp/0671027034">How to Win Friends
&amp; Influence People</a></li>
</ul>
<p>While talking to skip level leader Cameron in my organization, I
started to have interests in “behavioral economics”:</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Noise-Flaw-Human-Judgment/dp/B08LNYM39M/">Noise:
A Flaw in Human Judgment</a></li>
</ul>
<p>A book discussed during a career development session:</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Executive-Presence-2-0-Leadership-Inclusion-ebook/dp/B0BTYXJMG9/">Executive
Presence 2.0: Leadership in an Age of Inclusion</a></li>
</ul>
<p>This book is cited many times by Brene Brown in “Dare to Lead”:</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Good-Great-Some-Companies-Others/dp/0066620996">Good
to Great: Why Some Companies Make the Leap…And Others Don’t</a>
[Reading]</li>
</ul>
<p>While I was reading “The Long View”, I got to know some books
cited:</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Quiet-Power-Introverts-World-Talking/dp/0307352153">Quiet:
The Power of Introverts in a World That Can’t Stop Talking</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Give-Take-Helping-Others-Success/dp/0143124986">Give
and Take: Why Helping Others Drives Our Success</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Working-Emotional-Intelligence-Daniel-Goleman/dp/0553378589/">Working
with Emotional Intelligence</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Emotional-Intelligence-2-0-Travis-Bradberry/dp/0974320625/">Emotional
Intelligence 2.0</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Outliers-Story-Success-Malcolm-Gladwell-ebook/dp/B001ANYDAO/">Outliers:
The Story of Success</a></li>
</ul>
<p>A book I met at least four times in the airports so far in 2024</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://www.amazon.com/Think-Again-Power-Knowing-What/dp/1984878123/">Think
Again: The Power of Knowing What You Don’t Know</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/08/11/The-Wisdom-of-the-Bullfrog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/di-blog/2024/08/11/The-Wisdom-of-the-Bullfrog/" class="post-title-link" itemprop="url">The Wisdom of the Bullfrog</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-08-11 22:54:26" itemprop="dateCreated datePublished" datetime="2024-08-11T22:54:26-04:00">2024-08-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure>
<img
src="/di-blog/2024/08/11/The-Wisdom-of-the-Bullfrog/wisdom_of_bullfrog_book.png"
alt="The Wisdom of the Bullfrog" />
<figcaption aria-hidden="true">The Wisdom of the Bullfrog</figcaption>
</figure>
<p>The book “The Wisdom of the Bullfrog” written by Admiral William H.
McRaven is recommended by my mentor Dylan. It contains 18 sayings or
mottos used in the military, which inspire Admiral William and others
throughout his 4 decades Navy SEAL career.</p>
<p>Some favorite quotes from the author:</p>
<h3 id="chapter-three---when-in-command-command">Chapter Three - When in
Command, Command</h3>
<blockquote>
<p>As a leader you must always appear to be in command, even on those
days when you struggle with the pressures of the job. You must be
confident. You must be decisive. You must smile. You must laugh. You
must engage with your employees and be thankful for their work. You must
have the look of a person in charge. You must instill in your men and
women a sense of pride that their leader can handle any problem.</p>
<p>As a leader you can’t have a bad day. You must never look beaten, no
matter the circumstance. If you sulk, if you hang your head, if you
whine or complain about the leaders above you or the followers below
you, then you will lose the respect of your men and women, and the
attitude of despair will spread like wildfire.</p>
<p>Being a leader is an awesome responsibility. There are days when it
can be frightening to know that the fate of the organization rests on
your shoulders. But you must also realize that you were chosen to be the
leader because you have proven yourself along the way. You have
demonstrated that you know the business. You have shown that you can
handle the pressures and be decisive. You have exhibited all the
qualities necessary to lead. And even if none of the above holds true,
now that you are the leader, you are in command. So, take the damn helm
and command!</p>
</blockquote>
<h3 id="chapter-five---the-only-easy-day-was-yesterday">Chapter Five -
The Only Easy Day Was Yesterday</h3>
<blockquote>
<p>The day you no longer believe you have something to prove, the day
you no longer believe you must give it your all, the day you think you
are entitled to special treatment, the day you think all your hard days
are behind you, is the day you are no longer the right leader for the
job.</p>
<p>Leadership requires energy. It requires stamina. It requires
resilience. It requires everything you have and then some. The men and
women that work for you will feed off your energy. If you look
unprepared to deal with the challenges of the day, they will see this.
If you look beaten down because today was harder than yesterday, they
will feel this. If you are not prepared to give it your all, they will
know this. And if you think this is just about leaders in combat, you’re
mistaken. This is about every great leader who was given a difficult
task and asked to inspire, motivate, and manage the people under their
charge.</p>
</blockquote>
<h3 id="chapter-six---run-to-the-sound-of-the-guns">Chapter Six - Run to
the Sound of the Guns</h3>
<blockquote>
<p>Good leaders understand that organizations are going to have
challenges. That’s why you were hired to lead. Embrace the challenge.
Accept the fact that you must attack each problem with vigor and that
sometimes only you, the leader, can solve the most vexing of
institutional crises. Never shy away. Never retreat from a difficult
problem.</p>
</blockquote>
<h3 id="chapter-eight---who-dares-wins">Chapter Eight - Who Dares
Wins</h3>
<blockquote>
<p>It is better to err on the side of daring than the side of
caution.<br />
—Alvin Toffler, American writer and futurist</p>
</blockquote>
<h3
id="chapter-ten---no-plan-survives-first-contact-with-the-enemy">Chapter
Ten - No Plan Survives First Contact with the Enemy</h3>
<blockquote>
<p>No plan of operations reaches with any certainty beyond the first
encounter of the enemy’s main force.<br />
In other words, always have a Plan B. A contingency plan. A backup plan.
Because once you encounter the enemy, no plan survives first
contact.</p>
</blockquote>
<h3 id="chapter-fourteen---expect-what-you-inspect">Chapter Fourteen -
Expect What You Inspect</h3>
<blockquote>
<p>Truth is confirmed by inspection and delay; falsehood by haste and
uncertainty.<br />
—Tacitus, Roman historian</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/di-blog/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/di-blog/">1</a><a class="page-number" href="/di-blog/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/di-blog/page/4/">4</a><a class="page-number" href="/di-blog/page/5/">5</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/di-blog/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Di Zhen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/di-blog/js/comments.js"></script><script src="/di-blog/js/utils.js"></script><script src="/di-blog/js/motion.js"></script><script src="/di-blog/js/sidebar.js"></script><script src="/di-blog/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/di-blog/js/third-party/math/mathjax.js"></script>



</body>
</html>
