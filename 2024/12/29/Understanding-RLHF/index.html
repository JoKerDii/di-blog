<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/di-blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/di-blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/di-blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/di-blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/di-blog/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jokerdii.github.io","root":"/di-blog/","images":"/di-blog/images","scheme":"Muse","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/di-blog/js/config.js"></script>

    <meta name="description" content="Happy New Year (&#x2F;≧▽≦)&#x2F; There is a lot to talk about Reinforcement Learning from Human Feedback (RLHF). How about starting with Reinforcement Learning (RL) basics. Warning: Extremely long article ahead">
<meta property="og:type" content="article">
<meta property="og:title" content="Understanding RLHF">
<meta property="og:url" content="https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/index.html">
<meta property="og:site_name" content="Di&#39;s Blog">
<meta property="og:description" content="Happy New Year (&#x2F;≧▽≦)&#x2F; There is a lot to talk about Reinforcement Learning from Human Feedback (RLHF). How about starting with Reinforcement Learning (RL) basics. Warning: Extremely long article ahead">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/td_learning_for_v.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/ppo_clip_algo.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/PPO_RLHF_flowchart.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/DPO_idea.png">
<meta property="article:published_time" content="2024-12-29T18:14:49.000Z">
<meta property="article:modified_time" content="2024-12-29T18:14:49.000Z">
<meta property="article:author" content="Di Zhen">
<meta property="article:tag" content="knowledge">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/td_learning_for_v.png">


<link rel="canonical" href="https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/","path":"2024/12/29/Understanding-RLHF/","title":"Understanding RLHF"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Understanding RLHF | Di's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/di-blog/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/di-blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Di's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reinforcement-learning"><span class="nav-number">2.</span> <span class="nav-text">Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#introduction"><span class="nav-number">2.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-armed-bandit-mab"><span class="nav-number">2.2.</span> <span class="nav-text">Multi-Armed Bandit (MAB)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#agent"><span class="nav-number">2.3.</span> <span class="nav-text">Agent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#long-term-goal"><span class="nav-number">2.3.1.</span> <span class="nav-text">Long Term Goal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#policy"><span class="nav-number">2.3.2.</span> <span class="nav-text">Policy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bellman-equations"><span class="nav-number">2.4.</span> <span class="nav-text">Bellman Equations*</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#state-value-functions-and-action-value-functions"><span class="nav-number">2.4.1.</span> <span class="nav-text">State-Value
Functions and Action-Value Functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#state-value-bellman-equation-and-action-value-bellman-equation"><span class="nav-number">2.4.2.</span> <span class="nav-text">State-Value
Bellman Equation and Action-Value Bellman Equation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-iteration"><span class="nav-number">2.5.</span> <span class="nav-text">Policy Iteration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#monte-carlo-method"><span class="nav-number">2.6.</span> <span class="nav-text">Monte Carlo Method</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#monte-carlo-for-policy-evaluation"><span class="nav-number">2.6.1.</span> <span class="nav-text">Monte Carlo for
Policy Evaluation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#monte-carlo-for-policy-improvement"><span class="nav-number">2.6.2.</span> <span class="nav-text">Monte Carlo for
Policy Improvement</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#main-limitation"><span class="nav-number">2.6.3.</span> <span class="nav-text">Main Limitation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#temporal-difference-learning"><span class="nav-number">2.6.4.</span> <span class="nav-text">Temporal Difference Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#from-table-to-function-approximation"><span class="nav-number">2.6.5.</span> <span class="nav-text">From table to
function approximation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ppo-prior"><span class="nav-number">3.</span> <span class="nav-text">PPO Prior</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#average-reward"><span class="nav-number">3.1.</span> <span class="nav-text">Average Reward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#differential-return"><span class="nav-number">3.2.</span> <span class="nav-text">Differential Return</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-gradient-and-reinforce"><span class="nav-number">3.3.</span> <span class="nav-text">Policy Gradient and
REINFORCE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#main-limitations-of-reinforce"><span class="nav-number">3.4.</span> <span class="nav-text">Main Limitations of
REINFORCE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#advantage-function"><span class="nav-number">3.5.</span> <span class="nav-text">Advantage Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#off-policy-policy-gradient"><span class="nav-number">3.6.</span> <span class="nav-text">Off-Policy Policy Gradient</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#trust-region-policy-optimization-trpo"><span class="nav-number">3.7.</span> <span class="nav-text">Trust Region Policy
Optimization (TRPO)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ppo"><span class="nav-number">4.</span> <span class="nav-text">PPO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ppo-objective"><span class="nav-number">4.1.</span> <span class="nav-text">PPO Objective</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ppo-usage"><span class="nav-number">4.2.</span> <span class="nav-text">PPO Usage</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#state-action-and-reward-in-the-context-of-llms"><span class="nav-number">4.2.1.</span> <span class="nav-text">State,
Action, and Reward in the Context of LLMs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#steps-of-rlhf-using-ppo"><span class="nav-number">4.2.2.</span> <span class="nav-text">Steps of RLHF Using
PPO</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rlhf-training-tricks"><span class="nav-number">4.2.3.</span> <span class="nav-text">RLHF Training Tricks</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dpo"><span class="nav-number">5.</span> <span class="nav-text">DPO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bradley-terry-and-plackett-luce-reward-model"><span class="nav-number">5.1.</span> <span class="nav-text">Bradley-Terry and
Plackett-Luce Reward Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dpo-objective"><span class="nav-number">5.2.</span> <span class="nav-text">DPO Objective</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dpo-usage"><span class="nav-number">5.3.</span> <span class="nav-text">DPO Usage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dpo-performance"><span class="nav-number">5.4.</span> <span class="nav-text">DPO Performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dpo-objective-pseudocode"><span class="nav-number">5.5.</span> <span class="nav-text">DPO Objective Pseudocode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dpo-variants"><span class="nav-number">5.6.</span> <span class="nav-text">DPO Variants</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#main-difficulties-in-rlhf"><span class="nav-number">6.</span> <span class="nav-text">Main Difficulties in RLHF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#data-collection"><span class="nav-number">6.1.</span> <span class="nav-text">Data Collection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reward-hacking"><span class="nav-number">6.2.</span> <span class="nav-text">Reward Hacking</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Di Zhen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/di-blog/archives/">
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/12/29/Understanding-RLHF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Understanding RLHF | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Understanding RLHF
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-29 13:14:49" itemprop="dateCreated datePublished" datetime="2024-12-29T13:14:49-05:00">2024-12-29</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>Happy New Year (/≧▽≦)/</p>
<p>There is a lot to talk about Reinforcement Learning from Human
Feedback (RLHF). How about starting with Reinforcement Learning (RL)
basics.</p>
<p><em>Warning: Extremely long article ahead :)</em></p>
<h2 id="overview">Overview</h2>
<p>The process of training a model using reinforcement learning from
human feedback (RLHF) involves three key steps, as outlined in the paper
titled “<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Training language
models to follow instructions with human feedback</a>” by OpenAI.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/instructGPT_overview_RLHF.png"
alt="instructGPT_overview_RLHF" />
<figcaption aria-hidden="true">instructGPT_overview_RLHF</figcaption>
</figure>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<h3 id="introduction">Introduction</h3>
<p>Reinforcement Learning (RL) is a machine learning approach where an
agent learns to make decisions by interacting with an environment to
maximize cumulative rewards.</p>
<p>The <strong>agent</strong> is the decision-maker or learner in the RL
framework. It performs actions in the environment and learns from the
feedback it receives. The <strong>environment</strong> represents
everything external to the agent that it interacts with. It provides
feedback in response to the agent’s actions. The <strong>state</strong>
is a representation of the current situation of the environment as
perceived by the agent. An <strong>action</strong> is a decision or move
taken by the agent at each step based on its policy (a mapping from
states to actions). The <strong>reward</strong> is a scalar feedback
signal provided by the environment to indicate how good or bad an action
was in achieving the agent’s goal.</p>
<p>An RL problem is typically formalized as a <strong>Markov Decision
Process (MDP)</strong>, which includes:</p>
<ul>
<li><strong>States (<span class="math inline">\(S\)</span>)</strong>:
The set of all possible situations in which the agent can find
itself.</li>
<li><strong>Actions (<span class="math inline">\(A\)</span>)</strong>:
The set of all possible moves or decisions available to the agent.</li>
<li><strong>Transition Dynamics (<span
class="math inline">\(P(s&#39;|s,a)\)</span>)</strong>: The probability
of transitioning to a new state <span
class="math inline">\(s&#39;\)</span> given the current state <span
class="math inline">\(s\)</span> and action <span
class="math inline">\(a\)</span>.</li>
<li><strong>Rewards (<span
class="math inline">\(R(s,a)\)</span>)</strong>: The immediate feedback
received after taking action <span class="math inline">\(a\)</span> in
state <span class="math inline">\(s\)</span>.</li>
<li><strong>Policy (<span
class="math inline">\(\pi(a|s)\)</span>)</strong>: A strategy that
defines how the agent selects actions based on states.</li>
</ul>
<p>The goal of RL is to find an optimal policy <span
class="math inline">\(\pi^*\)</span> that maximizes cumulative rewards
(also called return). This involves balancing short-term rewards with
long-term planning using trial-and-error interactions with the
environment.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/agent_env_rewards_RL_intro.png"
alt="agent_env_rewards_RL_intro" />
<figcaption aria-hidden="true">agent_env_rewards_RL_intro</figcaption>
</figure>
<p>The challenges arised from the <strong>nature of environment and its
dynamics</strong> are non-stationary environments, stochastic rewards,
and random states:</p>
<ul>
<li>In non-stationary environments, the dynamics of the environment
(e.g., transition probabilities or reward functions) change over time.
This forces RL agents to continuously adapt their policies, which can
lead to a drop in performance during the readjustment phase and
forgetting previously learned policies.</li>
<li>Stochastic rewards occur when the reward function is probabilistic
rather than deterministic. This introduces noise into the feedback
signal, making it harder for the agent to discern which actions truly
lead to higher rewards.</li>
<li>Random states refer to situations where the agent’s observations are
noisy or partially observable, making it harder to infer the true state
of the environment. Such randomness complicates policy learning because
the agent may need to rely on memory or belief states (e.g., Partially
Observable Markov Decision Processes, POMDPs) to make decisions. It
increases the dimensionality and complexity of the state space.</li>
</ul>
<p>The challenges related to <strong>algorithmic design and
computational feasibility</strong> are:</p>
<ul>
<li>RL algorithms require a significant amount of interaction with the
environment to learn effectively, making them data-intensive. Many RL
algorithms, particularly model-free methods like policy gradient
techniques, require a large number of samples to converge.</li>
<li>RL agents face the <strong>exploration-exploitation
dilemma</strong>, where they need to balance trying new actions to
discover potentially better rewards (<strong>Exploration</strong>) and
using known actions that yield high rewards
(<strong>Exploitation</strong>).</li>
<li>Many RL problems involve enormous state and action spaces, such as
games like Go or real-world robotics tasks. The exponential growth of
possible states and actions makes it computationally challenging for RL
algorithms to find optimal solutions.</li>
<li>Poorly designed rewards can lead to unintended behaviors (e.g., an
agent exploiting loopholes in the reward structure). Sparse or delayed
rewards make it difficult for the agent to associate its actions with
outcomes.</li>
<li>RL agents often struggle to generalize learned behaviors across
different tasks or environments. Agents trained in specific simulations
(e.g., driving simulators) may fail to perform well in real-world
scenarios due to differences in dynamics, noise, or variability.</li>
<li>RL algorithms are highly sensitive to hyperparameter choices (e.g.,
learning rate, discount factor). Poor tuning can lead to slow
convergence or failure to converge at all, making training unpredictable
and requiring significant expertise.</li>
<li>RL agents often use complex models (e.g., deep neural networks),
making their decisions difficult to interpret. This lack of transparency
is problematic in safety-critical applications like healthcare or
autonomous driving, where understanding the reasoning behind decisions
is essential.</li>
</ul>
<h3 id="multi-armed-bandit-mab">Multi-Armed Bandit (MAB)</h3>
<p>The multi-armed bandit (MAB) problem is a classic RL problem that
exemplifies the exploration-exploitation tradeoff. It provides a
simplified framework for decision-making under uncertainty.</p>
<p>Here is a simple scenario to help understand the Multi-Armed Bandit
(MAB) problem. Imagine a doctor has three types of prescription drugs to
treat a particular disease and <span class="math inline">\(N\)</span>
patients to treat. At the beginning, the doctor has no knowledge about
which drug is the most effective. The goal is to identify the
<strong>best action</strong>—the drug that can cure the highest number
of patients.</p>
<p>To achieve this goal, we can define <strong>action values</strong>
as:</p>
<p><span class="math display">\[
Q_t(a) = E[R_t \mid A_t = a],
\]</span></p>
<p>where: - <span class="math inline">\(R_t\)</span> is a random
variable representing whether a patient is cured (reward), - <span
class="math inline">\(a\)</span> is an action, which in this case
corresponds to selecting a specific type of drug for the patients.</p>
<p>The <strong>best action</strong> is the one that maximizes the
expected reward:</p>
<p><span class="math display">\[
a^* = \arg\max_a Q(a).
\]</span></p>
<p>It’s important to note that an expectation, <span
class="math inline">\(E[x]\)</span>, is typically calculated as:</p>
<p><span class="math display">\[
E[x] = \sum x p(x),
\]</span></p>
<p>where <span class="math inline">\(p(x)\)</span> represents the
probability distribution of <span class="math inline">\(x\)</span>.
However, in real-world scenarios where <span
class="math inline">\(p(x)\)</span> is unknown and data is limited, the
expectation can be approximated using sample averages:</p>
<p><span class="math display">\[
E[x] \approx \frac{\sum x}{N},
\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the total number of
observations of <span class="math inline">\(x\)</span>. This
approximation process is known as <strong>Monte Carlo
estimation</strong>.</p>
<p>The action value <span class="math inline">\(Q_t(a)\)</span> can be
estimated by <strong>Sample-Average Method</strong> using the following
formula:</p>
<p><span class="math display">\[
Q_t(a) = \frac{\text{Total rewards received when action } a \text{ was
taken before time } t}{\text{Number of times action } a \text{ was taken
before time } t}.
\]</span></p>
<p>Mathematically, this can be expressed as:</p>
<p><span class="math display">\[
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \cdot I_{A_i = a}}{\sum_{i=1}^{t-1}
I_{A_i = a}},
\]</span></p>
<p>where: - <span class="math inline">\(R_i\)</span> is the reward
received at time step <span class="math inline">\(i\)</span>, - <span
class="math inline">\(I_{A_i = a}\)</span> is an indicator function that
equals 1 if action <span class="math inline">\(a\)</span> was selected
at time step <span class="math inline">\(i\)</span> and 0 otherwise.</p>
<p>The best action can be select by <strong>Greedy Approach</strong>:
<span class="math display">\[
a^* = \arg\max_a Q(a).
\]</span> In our case as demonstrated in the diagram below, after 4
times, <span class="math inline">\(Q_t(1)=0.5, Q_t(2)=0.75,
Q_t(3)=0.25\)</span>, the best action is determined by <span
class="math inline">\(\arg \max_a Q_t(a)\)</span> which is Action <span
class="math inline">\(A_2\)</span> (<span
class="math inline">\(a=2\)</span>).</p>
<p>However, this approach has some drawbacks such as small sample size
and non-stationary environment (e.g. patients are in different
consitions). An intuitive alternative is to give Action <span
class="math inline">\(A_1\)</span> and Action <span
class="math inline">\(A_3\)</span> more opportunities. This is called
<strong>Exploration - Exploitation Tradeoff</strong>, which means to
balance trying new actions to discover potentially better rewards
(<strong>Exploration</strong>) and using known actions that yield high
rewards (<strong>Exploitation</strong>).</p>
<p>A better approach is called <strong>Epsilon-Greedy Strategy</strong>
which is a simple yet effective method for addressing the
exploration-exploitation tradeoff in RL. It involves:</p>
<ol type="1">
<li><strong>Exploration</strong>: With a probability of <span
class="math inline">\(\epsilon\)</span>, the agent chooses a random
action, allowing it to explore the environment and gather new
information.</li>
<li><strong>Exploitation</strong>: With a probability of <span
class="math inline">\(1-\epsilon\)</span>, the agent selects the action
that has the highest estimated reward (greedy action) based on its
current knowledge.</li>
</ol>
<p>In our case, let <span class="math inline">\(\epsilon =
20\%\)</span>, <span class="math inline">\(Q_t(1)\)</span> and <span
class="math inline">\(Q_t(3)\)</span> each is given <span
class="math inline">\(10\%\)</span>, and <span
class="math inline">\(Q_t(2)\)</span> is given <span
class="math inline">\(80\%\)</span>. The next round (5th) of action is
decided by random sampling <span
class="math inline">\(A_1,A_2,A_3\)</span> with probability of <span
class="math inline">\(10\%, 80\%,10\%\)</span>. If the sampled action is
<span class="math inline">\(A_1\)</span> and the reward is <span
class="math inline">\(1\)</span>, then its action value is updated to be
is <span class="math inline">\(Q_t&#39;(1) =  (0+1+1+0+1)/5
=0.6\)</span>.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/doctor_treatment_example.png"
alt="doctor_treatment_example" />
<figcaption aria-hidden="true">doctor_treatment_example</figcaption>
</figure>
<p>The pseudo code of Epsilon Greedy Approach is as follows.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize</span></span><br><span class="line"><span class="keyword">for</span> a = <span class="number">1</span> to K:</span><br><span class="line">    Q(a) &lt;- <span class="number">0</span>          <span class="comment"># Estimated value of each arm</span></span><br><span class="line">    N(a) &lt;- <span class="number">0</span>          <span class="comment"># Number of times each arm has been pulled</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Epsilon-Greedy Algorithm</span></span><br><span class="line"><span class="keyword">for</span> t = <span class="number">1</span> to num_turns:</span><br><span class="line">    <span class="keyword">with</span> probability ε:</span><br><span class="line">        A &lt;- randomly select an arm (exploration)</span><br><span class="line">    otherwise:</span><br><span class="line">        A &lt;- select the arm <span class="keyword">with</span> the highest Q(a) (exploitation)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pull the selected arm and observe reward R</span></span><br><span class="line">    R &lt;- Reward(A)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the estimates for the selected arm</span></span><br><span class="line">    N(A) &lt;- N(A) + <span class="number">1</span></span><br><span class="line">    Q(A) &lt;- Q(A) + (<span class="number">1</span> / N(A)) * (R - Q(A))  <span class="comment"># Incremental update formula</span></span><br></pre></td></tr></table></figure>
<p>Note that there is a math trick in the incremental updates
<code>Q(A) &lt;- Q(A) + (1 / N(A)) * (R - Q(A))</code>. <span
class="math display">\[
\begin{equation}
\begin{aligned}
Q_{n+1} &amp;= {1\over n}\sum^n_{i=1}R_i \space \text{ : average reward
in the n+1 iteration}\\
&amp;= {1\over n}(R_n + \sum^{n-1}_{i=1}R_i)\\
&amp;= {1\over n}(R_n + (n-1){1\over n-1}\sum^{n-1}_{i=1}R_i)\\
&amp;= {1\over n} (R_n + (n-1)Q_n)\\
&amp;= {1\over n}(R_n+ n \times Q_n - Q_n)\\
&amp;= Q_n + {1\over n} (R_n - Q_n)
\end{aligned}
\end{equation}
\]</span> The higher the <span class="math inline">\(\epsilon\)</span>,
the more opportunities given to actions, and the higher average
reward.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/epsilon_greedy_method.png"
alt="epsilon_greedy_method" />
<figcaption aria-hidden="true">epsilon_greedy_method</figcaption>
</figure>
<p>(Source: <a
target="_blank" rel="noopener" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249">Reinforcement
Learning by Sutton and Barto</a>, Chapter 2)</p>
<h3 id="agent">Agent</h3>
<h4 id="long-term-goal">Long Term Goal</h4>
<p>The goal of Agent is long-term reward <span class="math display">\[
G_t = R_{t+1}+R_{t+2}+R_{t+3}+...
\]</span> So the objective is expected reward <span
class="math inline">\(E[G_t]\)</span> <span class="math display">\[
E[G_t] = E[R_{t+1}+R_{t+2}+R_{t+3}+...]
\]</span> There are different types of agent tasks:</p>
<ul>
<li><p><strong>Episodic Task</strong>: Episodic tasks consist of
distinct episodes, where each episode has a clear beginning and end. At
the end of an episode, the environment resets to a starting
state.</p></li>
<li><p><strong>Continuing Task</strong>: Continuing tasks involve
ongoing interactions with no natural endpoint. The agent interacts with
the environment indefinitely. A key challenge in continuing tasks is
that the cumulative reward (<span class="math inline">\(E[G_t]\)</span>)
can become unbounded as time progresses. This makes it difficult to
optimize an unbounded objective directly.</p>
<p>To make the objective bounded, a <strong>discount factor</strong>
(<span class="math inline">\(\gamma\)</span>) is introduced. The
discount factor ensures that more weight is given to immediate rewards
while gradually reducing the importance of future rewards. This approach
stabilizes the optimization process. <span class="math inline">\(\gamma
\in (0,1)\)</span> is a scalar that determines how much future rewards
are discounted compared to immediate rewards. In practice, <span
class="math inline">\(\gamma\)</span> is often set close to 1 (e.g.,
0.95 or 0.98), allowing the agent to consider long-term rewards while
still prioritizing recent ones.</p>
<p>The following derivations demonstrate how discounting makes the
objective bounded. <span class="math display">\[
\begin{equation}
\begin{aligned}
G_t &amp;= \gamma R_{t+1}+\gamma^2
R_{t+2}+\gamma^3R_{t+3}+...+\gamma^{k-1} R_{t+k} ...\\
&amp;=\sum^{\infty}_{k=0}\gamma^k R_{t+k+1}\\
&amp;\leq \sum^{\infty}_{k=0} \gamma^k \times R_{max} \space \text{
,where }R_{max} = \max\{R_{t+1},R_{t+2},...R_{t+k}\}\\
&amp;=R_{max} \sum^{\infty}_{k=0} \gamma^k \\
&amp;= R_{max} {1\over 1-\gamma} &lt; \infty
\end{aligned}
\end{equation}
\]</span> The value of <span class="math inline">\(\gamma\)</span>
influences how far-sighted or short-sighted the agent is. If <span
class="math inline">\(\gamma\)</span> is large, the agent is
<strong>far-sighted</strong>, meaning it prioritizes long-term rewards
over immediate ones. If <span class="math inline">\(\gamma\)</span> is
small, the agent is <strong>short-sighted</strong>, focusing heavily on
immediate rewards while ignoring distant future outcomes.</p>
<p>The cumulative reward can also be written as follows, representing
how the current cumulative reward is determined by the next step's
reward and cumulative reward: <span class="math display">\[
\begin{equation}
\begin{aligned}
G_t &amp;= \gamma R_{t+1}+\gamma^2 R_{t+2}+...\\
&amp;=R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + ...)\\
&amp;=R_{t+1} + \gamma G_{t+1}
\end{aligned}
\end{equation}
\]</span></p></li>
</ul>
<h4 id="policy">Policy</h4>
<p>The outcome of RL is policy <span class="math inline">\(\pi\)</span>,
which is a projection or mapping from input state <span
class="math inline">\(s\)</span> to output action <span
class="math inline">\(a\)</span>.</p>
<p><strong>Deterministic Policy</strong>: A deterministic policy maps
each state to a single, specific action. In other words, given the same
state, the agent will always select the same action. Deterministic
policies may fail in environments with high uncertainty or noise, as
they do not allow for the exploration of alternative actions. <span
class="math display">\[
\pi(s)=a
\]</span> <strong>Stochastic Policy</strong>: A stochastic policy maps
each state to a probability distribution over possible actions. For a
given state, the agent selects an action based on this distribution,
meaning different actions can be chosen with varying probabilities.
Requires learning and maintaining a probability distribution over
actions, which can be computationally expensive. <span
class="math display">\[
\begin{aligned}
\pi(a|s) &amp;= P(a|s) \geq 1, \\
&amp;\text{where }P(a|s)  \text{ is the probability of selecting action
a in state s.}\\
\sum_{a\in A(s)}\pi(a|s)&amp;=1
\end{aligned}
\]</span></p>
<h3 id="bellman-equations">Bellman Equations*</h3>
<h4 id="state-value-functions-and-action-value-functions">State-Value
Functions and Action-Value Functions</h4>
<p><strong>State-Value Functions</strong>: denoted as <span
class="math inline">\(V_\pi(s)\)</span>, represents the expected
cumulative future rewards starting from a particular state <span
class="math inline">\(s\)</span> and following a specific policy <span
class="math inline">\(\pi\)</span> thereafter. It measures the
"goodness" of being in the state <span class="math inline">\(s\)</span>,
considering the long-term rewards achievable from that state under
policy <span class="math inline">\(\pi\)</span>. It does not depend on
specific actions but rather on the overall behavior dictated by the
policy. <span class="math display">\[
\begin{aligned}
V_\pi(s)=  E_\pi[G_t|S_t=s], \\
\space G_t=\sum^\infty_{k=0}\gamma^kR_{t+k+1}
\end{aligned}
\]</span> <strong>Action-Value Functions</strong>: denoted as <span
class="math inline">\(Q_\pi(s,a)\)</span>, represents the expected
cumulative future rewards starting from the state <span
class="math inline">\(s\)</span>, taking action <span
class="math inline">\(a\)</span>, and then following a specific policy
<span class="math inline">\(\pi\)</span> thereafter. It measures the
“goodness” of taking action <span class="math inline">\(a\)</span> in
state <span class="math inline">\(s\)</span>, considering both immediate
rewards and future rewards achievable under the policy <span
class="math inline">\(\pi\)</span>. It provides more granular
information than <span class="math inline">\(V(s)\)</span>, as it
evaluates specific actions rather than just states. <span
class="math display">\[
Q_\pi(s,a) = E_\pi[G_t|S_t=s, A_t=a]
\]</span> The relationship between the state-value function and the
action-value function can be expressed using the following formula:
<span class="math display">\[
V_\pi(s) =\sum_{a \in A} \pi(a|s) Q_\pi (s,a)
\]</span> This equation shows that the value of a state under policy
<span class="math inline">\(\pi\)</span>, <span
class="math inline">\(V_\pi(s)\)</span>, is the expected value of the
action-value function <span class="math inline">\(Q_\pi(s,a)\)</span>,
weighted by the probability of taking each action <span
class="math inline">\(a\)</span> in state <span
class="math inline">\(s\)</span> according to policy <span
class="math inline">\(\pi(a|s)\)</span>.</p>
<h4
id="state-value-bellman-equation-and-action-value-bellman-equation">State-Value
Bellman Equation and Action-Value Bellman Equation</h4>
<p><strong>State-Value Bellman Equation</strong>:</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/bellman_equation_huggingface.png"
alt="bellman_equation_huggingface" />
<figcaption aria-hidden="true">bellman_equation_huggingface</figcaption>
</figure>
<p>(Source: <a
target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/en/unit2/bellman-equation">The
Bellman Equation: simplify our value estimation</a>)</p>
<p>The State-Value Bellman Equation can be written in a recursive form.
<span class="math display">\[
\begin{aligned}
V_\pi(s) &amp;= E_\pi(G_t|S_t=s)\\
&amp;= E_\pi(R_{t+1}+rG_{t+1}|S_t=s)\\
&amp;=\sum_a \pi(a|s) \sum_{s&#39;}\sum_r p(s&#39;,r|s,a) [r +
V_\pi(s&#39;)]
\end{aligned}
\]</span></p>
<p>The tree structure below as an example can help understand the
recursive property of the State-Value Bellman Equation. Note that an
action <span class="math inline">\(a\)</span> does not necessarily lead
to a specific state <span class="math inline">\(s\)</span>, it can
result in multiple possible states, each with a certain probability.
These probabilities are determined by the environment, which we
typically do not have direct access to.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/bellman_equation_tree.png"
alt="bellman_equation_tree" />
<figcaption aria-hidden="true">bellman_equation_tree</figcaption>
</figure>
<p><strong>Action-Value Bellman Equation</strong>:</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/action_value_equation_huggingface.png"
alt="action_value_equation_huggingface" />
<figcaption
aria-hidden="true">action_value_equation_huggingface</figcaption>
</figure>
<p>(Source: <a
target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit2/two-types-value-based-methods?utm_source=perplexity">Two
types of value-based methods</a>)</p>
<p>The Action-Value Bellman Equation can be written in a recursive form
as well: <span class="math display">\[
\begin{aligned}
Q_\pi(s,a)&amp;= E_\pi[G_t|S_t=s, A_t=a]\\
&amp;=\sum_{s&#39;}\sum_{r}
P(s&#39;,r|s,a)[r+\gamma\sum_{a&#39;}\pi(a&#39;|s&#39;)Q_\pi(s&#39;,a&#39;)]
\end{aligned}
\]</span> The tree structure below as an example can help understand the
recursive property of the Action-Value Bellman Equation.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/action_value_bellman_equation_tree.png"
alt="action_value_bellman_equation_tree" />
<figcaption
aria-hidden="true">action_value_bellman_equation_tree</figcaption>
</figure>
<p>The main limitations of Bellman Equation:</p>
<ul>
<li>In real-world problems, the number of states can be extremely large,
requiring a separate Bellman equation for each state. This results in a
system of simultaneous nonlinear equations due to the presence of the
max operator, which can be difficult to solve.</li>
<li>Solving Bellman equations often requires iterative methods and can
demand significant computational resources. This is particularly true
when seeking high-precision approximations over many iterations.</li>
<li>In applications like the Bellman-Ford algorithm for finding shortest
paths, the presence of negative cycles can pose a problem. If a cycle
has a negative total sum of edges, it can lead to an undefined shortest
path since iterating through the cycle can indefinitely reduce the path
length.</li>
<li>The Bellman equation is inherently nonlinear because it involves
maximizing over possible actions. This nonlinearity can complicate
finding solutions, especially when dealing with large state spaces or
complex reward structures.</li>
</ul>
<h3 id="policy-iteration">Policy Iteration</h3>
<p>In RL, an <strong>optimal policy</strong> is a policy that maximizes
the expected cumulative reward (or return) for an agent across all
states in a <strong>Markov Decision Process (MDP)</strong>. This means
that the state-value function <span
class="math inline">\(v_{\pi}(s)\)</span> or the action-value function
<span class="math inline">\(q_{\pi}(s,a)\)</span> under the optimal
policy is greater than or equal to that of any other policy for all
states and actions. <span class="math display">\[
\pi^*(s) \geq \pi(s), \forall s \in \text{states}
\]</span> In finite MDPs, at least one optimal policy always exists.
However, there may be multiple optimal policies that achieve the same
maximum expected return.</p>
<p><strong>Policy Iteration</strong> is a <strong>dynamic
programming</strong> algorithm used in RL to compute the optimal policy
<span class="math inline">\(\pi^*\)</span> for a Markov Decision Process
(MDP). It alternates between two main steps: <strong>policy
evaluation</strong> and <strong>policy improvement</strong>, iteratively
refining the policy until convergence.</p>
<p>The full policy iteration algorithm pseudocode is in the figure below
(Source: <a
target="_blank" rel="noopener" href="https://lcalem.github.io/blog/2018/09/24/sutton-chap04-dp">Sutton
&amp; Barto summary chap 04 - Dynamic Programming</a>):</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/policy_iteration_pseudocode.png"
alt="policy_iteration_pseudocode" />
<figcaption aria-hidden="true">policy_iteration_pseudocode</figcaption>
</figure>
<p>Here is a detailed explanation of this policy iteration's algorithm
pseudocode.</p>
<p>Repeat steps until convergence:</p>
<ol type="1">
<li><p>Policy evaluation: keep current policy <span
class="math inline">\(\pi\)</span> fixed, find value function <span
class="math inline">\(V(\cdot)\)</span>.</p>
<p>Iterate Bellman update until values <strong>converge</strong>:</p>
<p><span class="math display">\[
V(s) \leftarrow \sum_{s&#39;,r}p(s&#39;,r|s,\pi(s))[r+\gamma V(s&#39;)]
\]</span> The Bellman operator computes future rewards but discounts
them by multiplying with <span class="math inline">\(\gamma\)</span>.
This ensures that differences in value functions become smaller with
each iteration. In other words, <strong>Bellman shrinks
distances</strong>. To see it mathematically,</p>
<p>The Bellman operator for the state-value function under a fixed
policy <span class="math inline">\(\pi\)</span> is defined as <span
class="math display">\[
V^{\pi}(s) = r(s, \pi(s)) + \gamma \sum_s P(s|s,\pi(s))V(s&#39;)
\]</span> This operator updates the value function by combining the
immediate reward and the discounted future rewards.</p>
<p>We compute the difference after applying the operator: <span
class="math display">\[
\Big|V^{\pi}_1(s)-V^{\pi}_2(s)\Big| = \Big|r(s,\pi(s))+\gamma\sum_s P
(s|s,\pi(s))V_1(s&#39;)-\Big[r(s,\pi(s))+\gamma\sum_s P
(s|s,\pi(s))V_2(s&#39;)\Big]\Big|
\]</span> Simplify by canceling out the immediate rewards, we get: <span
class="math display">\[
\Big|V^{\pi}_1(s)-V^{\pi}_2(s)\Big| = \gamma \Big|\sum_s P
(s|s,\pi(s))\Big(V_1(s&#39;)-V_2(s&#39;)\Big)\Big|
\]</span> Since <span class="math inline">\(\gamma&lt;1\)</span>, the
difference between <span class="math inline">\(V_1^{\pi}(s)\)</span> and
<span class="math inline">\(V_1^{\pi}(s)\)</span> is always smaller than
the difference between <span class="math inline">\(V_1(s&#39;)\)</span>
and <span class="math inline">\(V_1(s&#39;)\)</span>. Because Bellman
operator shrinks distances, it is a <strong>contraction mapping</strong>
and follows the contraction mapping property.</p>
<p>In summary, Policy evaluation is a contraction mapping for a fixed
policy <span class="math inline">\(\pi\)</span>. Policy evaluation
converges because it applies a contraction mapping repeatedly to compute
the value function for a fixed policy.</p></li>
<li><p>Policy improvement: find the best action for <span
class="math inline">\(V(\cdot)\)</span> via one-step lookahead.</p>
<p>During policy improvement, the current policy <span
class="math inline">\(\pi\)</span> is updated by selecting actions that
maximize the expected return for each state <span
class="math inline">\(s\)</span>.</p>
<p><span class="math inline">\(\pi(s) \leftarrow \arg \max_a
\sum_{s&#39;,r}p(s&#39;,r|s,a)[r+\gamma V(s&#39;)]\)</span></p>
<p>The intuition behind: <span class="math inline">\(V^{\pi}(s)\)</span>
measures how good it is to start from the state <span
class="math inline">\(s\)</span> and follow policy <span
class="math inline">\(\pi\)</span>. By improving the actions selected by
the policy, we ensure that the agent transits into states with higher
expected cumulative rewards. This iterative process ensures that each
new policy improves upon or equals the previous one in terms of total
expected rewards.</p></li>
</ol>
<p>Overall, the idea of Policy Iteration can be demonstrated in the
diagram below. The evaluation process usually takes a long time while
the improvement process is usually fast.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/policy_iteration_eval_impro_demo.png"
alt="policy_iteration_eval_impro_demo" />
<figcaption
aria-hidden="true">policy_iteration_eval_impro_demo</figcaption>
</figure>
<p>The <strong>Generalized Policy Iteration (GPI)</strong> method can
speed up the policy evaluation process. In GPI, policy evaluation
process can be approximate or partial (e.g., only a few iterations
instead of full convergence). Policy improvement can also be done
incrementally or partially. GPI speeds up the process of finding an
optimal policy by relaxing the strict requirements of full convergence
in policy evaluation.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/generalized_policy_iteration.png"
alt="generalized_policy_iteration" />
<figcaption aria-hidden="true">generalized_policy_iteration</figcaption>
</figure>
<h3 id="monte-carlo-method">Monte Carlo Method</h3>
<p>Monte Carlo methods are specifically designed for episodic tasks,
where each episode eventually terminates, regardless of the actions
taken by the agent. An <strong>episode</strong> refers to a complete
sequence of interactions between an agent and the environment, starting
from an initial state and ending in a terminal state.</p>
<h4 id="monte-carlo-for-policy-evaluation"><strong>Monte Carlo for
Policy Evaluation</strong></h4>
<p>Value function and policy updates occur only after an episode is
complete. By waiting for the end of an episode, Monte Carlo methods
ensure that all rewards following a state are included in the return
calculation. Monte Carlo methods learn purely from sampled experience
(episodes), without requiring knowledge of transition probabilities or
reward models. Episodes allow Monte Carlo methods to handle stochastic
environments by averaging returns across multiple episodes.</p>
<p>Here is an example of calculating the state-value functions and
action-action functions by Monte Carlo Method once 2 episodes are
completed. Given states <span class="math inline">\(S=[A,B,C,D,E],
A=[1,2,3]\)</span>, and two episodes <span
class="math inline">\(E_1,E_2\)</span>, (Note: <span
class="math inline">\(A:(1,0.4)\)</span> means state <span
class="math inline">\(A\)</span>, action <span
class="math inline">\(1\)</span>, and reward <span
class="math inline">\(0.4\)</span>) <span class="math display">\[
\begin{aligned}
&amp;E_1 = \{A:(1,0.4), B:(2,0.5), A:(2,0.6), C:(2,0.1), B:(3,0.8),
E:()\}\\
&amp;E_2 = \{B:(2,0.5), A:(1,0.6), C:(2,0.3), A:(1,0.3), C:(2,0.8),
E:()\}
\end{aligned}
\]</span></p>
<ol type="1">
<li><p>State-Value Functions Calculation</p>
<p>We can calculate <span
class="math inline">\(V(A),V(B),V(C),V(D),V(E)\)</span>.</p>
<p>e.g. there are 4 sequences starting from state <span
class="math inline">\(A\)</span>, then the state value function is:
<span class="math display">\[
\begin{aligned}
V(A) &amp;= [(0.4+\gamma 0.5 + \gamma^2 0.6 + \gamma^3 0.1 + \gamma^4
0.8)\\
&amp;+(0.6+\gamma 0.1 + \gamma^2 0.8)\\
&amp;+(0.6+\gamma 0.3 + \gamma^2 0.3 + \gamma^4 0.8)\\
&amp;+(0.3+\gamma 0.8)] / 4
\end{aligned}
\]</span></p></li>
<li><p>Action-Value Functions Calculation</p>
<p>We can calculate <span
class="math inline">\(Q(A,1),Q(B,2),\cdots\)</span>.</p>
<p>e.g. there are three sequences starting from <span
class="math inline">\(A:(1,)\)</span>, then the action value function is
<span class="math display">\[
\begin{aligned}
Q(A,1) &amp;= [(0.4+\gamma 0.5 + \gamma 0.6 + \gamma^3 0.1+ \gamma^4
0.8)\\
&amp;+(0.6+\gamma 0.3 + \gamma^2 0.3 + \gamma^3 0.8)\\
&amp;+(0.3+\gamma 0.8)]/3
\end{aligned}
\]</span></p></li>
</ol>
<p>The pseudocode for the above Monte Carlo for Policy Evaluation is as
follows (Source: <a
target="_blank" rel="noopener" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249">Reinforcement
Learning by Sutton and Barto</a>, Chapter 5):</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/montecarlo_statevalue.png"
alt="montecarlo_statevalue" />
<figcaption aria-hidden="true">montecarlo_statevalue</figcaption>
</figure>
<p>As part of the algorithm, it loops for each step of episode from the
end <span class="math inline">\(T-1\)</span> to the beginning of the
episode. This allows for dynamic programming where some values can be
stored and do not need to be re-calculated (see a simple demonstration
below).</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/montecarlo_algo_dynamic_program.png"
alt="montecarlo_algo_dynamic_program" />
<figcaption
aria-hidden="true">montecarlo_algo_dynamic_program</figcaption>
</figure>
<h4 id="monte-carlo-for-policy-improvement"><strong>Monte Carlo for
Policy Improvement</strong></h4>
<p><span class="math display">\[
\pi_{k+1}(s)=\arg \max_a q_{\pi_k}(s,a)
\]</span></p>
<p>Here is an example of updating policy by action-value function. Given
states <span class="math inline">\(S=[A,B,C,D,E], A=[1,2,3],
\gamma=0.5\)</span>, and a episode <span
class="math inline">\(E\)</span>, (Note: <span
class="math inline">\(A:(1,0.7)\)</span> means state <span
class="math inline">\(A\)</span>, action <span
class="math inline">\(1\)</span>, and reward <span
class="math inline">\(0.7\)</span>) <span class="math display">\[
E = \{A:(1,0.7), B:(1,0.4), A:(3,1.5), C:(2,0.1), B:(3,0.7),
A:(1,0.3)\}\
\]</span> Through dynamic programming, cumulative reward is <span
class="math inline">\(G_5(A,1)=0.3\)</span>, <span
class="math inline">\(G_4(B,3)=0.7+0.5*3=0.85\)</span>, <span
class="math inline">\(G_3(C,2)=0.1+0.5*0.85=0.52\)</span>, <span
class="math inline">\(G_2(A,3)=1.5+0.52*0.5=1.76\)</span>, <span
class="math inline">\(G_1(B,1)=0.4+1.76*0.5=1.28\)</span>, <span
class="math inline">\(G_0(A,1)=0.7+1.28*0.5=1.34\)</span>. We can
maintain three lists to make the algorithm work:</p>
<ul>
<li>Return matrix <span class="math inline">\(Returns(S,A)\)</span>,
dimension <span class="math inline">\((S, A)\)</span>: It stores
cumulative reward values <span
class="math inline">\(G(S=s,A=a)\)</span>. One cell can store multiple
values as the number of episodes increases.</li>
<li><span class="math inline">\(Q(S,A)\)</span> matrix: It's initialized
as random numbers at the beginning. Updated whenever the return matrix
is updated. <span class="math inline">\(Q(S,A)\)</span> is the average
value of the corresponding <span
class="math inline">\(Returns(S,A)\)</span>.</li>
<li><span class="math inline">\(\pi(s)\)</span> list: It's updating
Epsilon values by giving <span class="math inline">\(1-\epsilon\)</span>
probability to the action with highest reward for each state, according
to the updated <span class="math inline">\(Q(S,A)\)</span> matrix. It
facilitates Epsilon Greedy Algorithm.</li>
</ul>
<p>The final updating result of the above example is in the diagram
below.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/mc_action_value_update_res.png"
alt="mc_action_value_update_res" />
<figcaption aria-hidden="true">mc_action_value_update_res</figcaption>
</figure>
<p>The pseudocode for the above Monte Carlo for Policy Improvement with
action-value function is as follows (Source: <a
target="_blank" rel="noopener" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249">Reinforcement
Learning by Sutton and Barto</a>, Chapter 5):</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/monte_carlo_action_value.png"
alt="monte_carlo_action_value" />
<figcaption aria-hidden="true">monte_carlo_action_value</figcaption>
</figure>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/mc_control_epsilon_pi.png"
alt="mc_control_epsilon_pi" />
<figcaption aria-hidden="true">mc_control_epsilon_pi</figcaption>
</figure>
<h4 id="main-limitation">Main Limitation</h4>
<ul>
<li>Policy updates occur only after an episode is completed, which can
slow down learning compared to methods like Temporal Difference (TD)
learning that update incrementally after each step.</li>
<li>MC methods do not use bootstrapping (i.e., they do not update value
estimates based on other estimates). While this avoids bias, it also
means MC methods cannot leverage intermediate value estimates, leading
to slower convergence.</li>
</ul>
<h4 id="temporal-difference-learning">Temporal Difference Learning</h4>
<p>TD leanring focuses on estimating the value function of a given
policy by updating value estimates based on the difference between
successive predictions, rather than waiting for an entire episode to
conclude.</p>
<p>Given <span class="math display">\[
\begin{aligned}
V(S_t) &amp;\leftarrow V(S_t) + \alpha \Big[G_t - V(S_t)\Big]\\
G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = R_{t+1} +
\gamma G_{t+1}\\
V_{\pi}(s) &amp;= E_\pi[G_t|S_t=s] = E_\pi\Big[R_{t+1} + \gamma G_{t+1}|
S_t=s\Big] = R_{t+1} + \gamma V_\pi(S_{t+1})
\end{aligned}
\]</span> We can derive the core function of TD: <span
class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(R_{t+1}) - V(S_t)]
\]</span> The pseudocode of TD learning is as follows. (Source: <a
target="_blank" rel="noopener" href="https://lcalem.github.io/blog/2018/10/31/sutton-chap06-td">Sutton
&amp; Barto summary chap 06 - Temporal Difference Learning</a>)</p>
<figure>
<img src="/di-blog/2024/12/29/Understanding-RLHF/td_learning_for_v.png"
alt="td_learning_for_v" />
<figcaption aria-hidden="true">td_learning_for_v</figcaption>
</figure>
<h4 id="from-table-to-function-approximation"><strong>From table to
function approximation</strong></h4>
<p>Main limitations of table based methods:</p>
<ul>
<li>As the number of state variables increases, the size of the state
space grows exponentially.</li>
<li>Storing a value table becomes impractical for large or continuous
state/action spaces due to memory limitations.</li>
<li>Table-based methods treat each state (or state-action pair)
independently. They cannot generalize knowledge from one part of the
state space to another, requiring every state to be visited multiple
times for accurate learning.</li>
<li>In large state spaces, it is unlikely that an agent will visit all
relevant states/actions frequently enough to converge to an optimal
policy within a reasonable time frame.</li>
<li>Table-based methods are well-suited for small problems but fail to
scale to real-world applications such as autonomous driving, robotics,
or complex games like Go or StarCraft.</li>
</ul>
<p><strong>From tabular to parametric functions</strong>:</p>
<p>Fit a parametric function to approximate the value function <span
class="math inline">\(V(s)\)</span> which maps states <span
class="math inline">\(s\)</span> to their corresponding value estimates.
<span class="math display">\[
f(s,\theta) \approx V(s)
\]</span> where <span class="math display">\[
f(s,\theta)=w^Ts+b
\]</span> To optimize this approximation, we minimize the <strong>mean
squared error (MSE)</strong> loss between the observed value <span
class="math inline">\(v(s)\)</span> and predicted <span
class="math inline">\(\hat{v}(s,{w})\)</span> from Monte Carlo. <span
class="math inline">\(\mu(s)\)</span> is the probability distribution
over states. This loss ensures that the predicted values are as close as
possible to the observed values. <span class="math display">\[
\ell = \min \sum_s \mu(s) \Big[v_\pi(s)-\hat{v}(s,{w})\Big]^2
\]</span> The optimal <span class="math inline">\({w}\)</span> that
minimize the loss can be found by batch <strong>Gradient
Descent</strong> (<span class="math inline">\(\eta\)</span> is learning
rate). <span class="math display">\[
w \leftarrow w - \eta \nabla \ell(w)
\]</span> where <span class="math display">\[
\begin{aligned}
\nabla \ell(w) &amp;= \nabla \sum_s \mu(s) \Big[v_\pi(s) -
\hat{v}(s,{w})\Big]^2\\
&amp;= \sum_s \mu(s) \nabla \Big[v_\pi(s) - \hat{v}(s,{w})\Big]^2\\
&amp;= -2 \sum_s \mu(s)  \Big[v_\pi(s) - \hat{v}(s,{w})\Big]\nabla
\hat{v}(s,{w})
\end{aligned}
\]</span> <strong>From Gradient Descent to Stochastic Gradient
Descent</strong>:</p>
<p>While batch gradient descent computes gradients over the entire
dataset (all states), this can be computationally expensive for
large-scale problems. Instead, <strong>stochastic gradient descent
(SGD)</strong> updates the parameters incrementally using one
observation at a time. Given observations <span
class="math inline">\((S_1, v_\pi(S_1)), (S_2, v_\pi(S_2)), (S_3,
v_\pi(S_3)), ...\)</span>, SGD performs updates as follows (<span
class="math inline">\(\alpha\)</span> is learning rate). <span
class="math display">\[
\begin{aligned}
{w}_2 &amp;= {w}_1 + \alpha \Big[v_\pi(S_1) - \hat{v}(S_1,{w_1})\Big]
\nabla \hat{v}(S_1,{w}_1)\\
{w}_3 &amp;= {w}_2 + \alpha \Big[v_\pi(S_2) - \hat{v}(S_2,{w_2})\Big]
\nabla \hat{v}(S_2,{w}_2)\\
&amp;\cdots
\end{aligned}
\]</span> The algorithm of Gradient Monte Carlo is as follows. (Source:
<a
target="_blank" rel="noopener" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249">Reinforcement
Learning by Sutton and Barto</a>)</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/gradient_montecarlo_v.png"
alt="gradient_montecarlo_v" />
<figcaption aria-hidden="true">gradient_montecarlo_v</figcaption>
</figure>
<h2 id="ppo-prior">PPO Prior</h2>
<h3 id="average-reward"><strong>Average Reward</strong></h3>
<p>The average reward is an alternative to the commonly used discounted
reward framework. It measures the long-term average reward per time step
under a given policy, making it particularly suitable for continuing
tasks (non-episodic problems) where there is no natural endpoint or
terminal state.</p>
<p>The average reward framework is particularly useful for continuing
tasks, where:</p>
<ul>
<li>The task does not have a natural termination point (e.g., robot
navigation, server optimization, or industrial control systems).</li>
<li>The agent operates indefinitely, and evaluating its performance
based on long-term behavior (rather than episodic returns) is more
meaningful.</li>
</ul>
<p>The average reward for a policy is defined as: <span
class="math display">\[
r(\pi)=\lim_{h \rightarrow \infty} {1\over h}\sum^h_{t=1} E\Big[R_t |
S_0, A_{0:t-1} \sim \pi\Big]
\]</span> This simple example shows how average reward is
calculated:</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/average_reward_policy.png"
alt="average_reward_policy" />
<figcaption aria-hidden="true">average_reward_policy</figcaption>
</figure>
<h3 id="differential-return">Differential Return</h3>
<p>The differential return in RL is a concept that arises in the average
reward framework, particularly for continuing tasks. It measures the
cumulative deviation of rewards from the long-term average reward rate,
<span class="math inline">\(r(\pi)\)</span> , under a given policy <span
class="math inline">\(\pi\)</span>.</p>
<p>Differential return aligns with the goal of maximizing long-term
performance in continuing tasks by focusing on deviations from
steady-state behavior. Unlike discounted returns, differential return
does not rely on a discount factor <span
class="math inline">\(\gamma\)</span>. This avoids biases introduced by
choosing an arbitrary discount factor. It is particularly well-suited
for tasks with no natural termination, such as robotics or industrial
control systems.</p>
<p>The differential return at time step <span
class="math inline">\(t\)</span> is defined as: <span
class="math display">\[
G_t = R_{t+1} - r(\pi)+R_{t+2} - r(\pi)+R_{t+3} - r(\pi)
\]</span> Then the value functions can be rewritten as <span
class="math display">\[
\begin{aligned}
v_\pi(s) = \sum_a \pi(a|s)\sum_{r,s&#39;} p(s&#39;,r|s,a) \Big[r-r(\pi)
+v_{\pi}(s&#39;)\Big]\\
q_\pi(s,a) = \sum_{r,s&#39;} p(s&#39;,r|s,a) \Big[r-r(\pi)
+\sum_{a&#39;}\pi(a&#39;|s&#39;)q_{\pi}(s&#39;,a&#39;)\Big]
\end{aligned}
\]</span> Algorithms like Gradient Monte Carlos can be rewritten by
using this differential return.</p>
<h3 id="policy-gradient-and-reinforce">Policy Gradient and
REINFORCE</h3>
<p>Objective of policy gradient: <span class="math display">\[
\begin{aligned}
J(\theta) &amp;= \sum_{s\in S}d^{\pi}(s)V^\pi(s) = \sum_{s\in S}
d^\pi(s) \sum_{a\in A} \pi_\theta(a|s) Q^\pi(s,a)\\
d^\pi(s) &amp;= \lim_{t\rightarrow \infty}P(s_t=s|s_o,\pi_\theta)
\rightarrow \text{ converege (Markov Property)}\\
&amp;\max J(\theta): \\
&amp;\max \sum_{s\in S} d^\pi(s) V^\pi(s) \implies \theta \leftarrow
\theta + \nabla_\theta J(\theta)
\end{aligned}
\]</span> <strong>Policy gradient theorem</strong> allows us to compute
the gradient of the expected return with respect to the parameters of a
parameterized policy, enabling optimization through gradient ascent.
<span class="math display">\[
\begin{aligned}
\nabla_\theta J(\theta) &amp;= \nabla _\theta\Big[\sum_{s\in S} d^\pi(s)
\sum_{a\in A} \pi_\theta(a|s) Q^\pi(s,a)\Big]\\
&amp;\propto \sum_{s\in S} d^\pi(s) \sum_{a\in A} Q^\pi(s,a) \nabla
_\theta\pi_\theta(a|s)\\
&amp;\implies \theta \leftarrow \theta + \eta\Big[\sum_{s\in S} d^\pi(s)
\sum_{a\in A} Q^\pi(s,a) \nabla _\theta\pi_\theta(a|s) \Big]
\end{aligned}
\]</span> Since Monte Carlo involves a sampling step, which requires an
expectation form. The gradient derived above can be re-written as
follows, supporting gradient estimation. (Recall: <span
class="math inline">\((\ln x)&#39; = 1/x\)</span>) <span
class="math display">\[
\begin{aligned}
\nabla_\theta J(\theta) &amp;\propto \sum_{s\in S} d^\pi(s) \sum_{a\in
A} Q^\pi(s,a) \nabla _\theta\pi_\theta(a|s)\\
&amp;=\sum_{s\in S}d^\pi(s) \sum_{a\in A} \pi_\theta(a|s) Q^\pi(s,a)
{\nabla_{\theta} \pi_\theta(a|s)\over \pi_\theta(a|s)}\\
&amp;=E_\pi\Big[Q^\pi(s,a)\nabla_\theta \ln\pi_\theta(a|s)\Big]\\
(&amp;=E_{s \sim \pi, a \sim
\pi_\theta(a|s)}\Big[Q^\pi(s,a)\nabla_\theta \ln\pi_\theta(a|s)\Big])\\
\end{aligned}
\]</span> Given the above theorems, a <strong>Reinforce Algorithm -
Monte-Carlo Policy-Gradient algorithm</strong> is defined as follows.
(Source: <a
target="_blank" rel="noopener" href="https://lcalem.github.io/blog/2019/03/21/sutton-chap13">Sutton
&amp; Barto summary chap 13 - Policy Gradient Methods</a>)</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/montecarlo_policy_gradient_pi.png"
alt="montecarlo_policy_gradient_pi" />
<figcaption
aria-hidden="true">montecarlo_policy_gradient_pi</figcaption>
</figure>
<p>A differentiable policy ensures that small changes in <span
class="math inline">\(\theta\)</span> result in smooth changes in the
action probabilities <span
class="math inline">\(\pi(a|s,\theta)\)</span>. This is crucial for
stable and efficient learning. The <strong>softmax function</strong> is
commonly used to parameterize policies in discrete action spaces. The
softmax function is smooth and differentiable, enabling gradient-based
optimization. Softmax ensures that all actions have non-zero
probabilities, promoting exploration during training.</p>
<p>Here the differentiable policy parameterization <span
class="math inline">\(\pi(a|s,{\theta})\)</span> can be defined by <span
class="math display">\[
\pi(a|s,\theta) = {\exp(h(s,a,\theta)) \over \sum_{b\in A}
\exp(h(s,b,\theta))}
\]</span> where <span
class="math inline">\(h(s,a,\theta)=w_a^T+b_a\)</span> is a linear or
non-linear function representing the preference for action <span
class="math inline">\(a\)</span>. The denominator normalizes the
probabilities so that they sum to 1.</p>
<p>The log of the softmax function has a convenient derivative that
simplifies gradient computation: <span class="math display">\[
\nabla_\theta\ln \pi(a,s,\theta) = \nabla_\theta h(s,a,\theta) - \sum_b
\pi(b|s,\theta) \nabla_\theta h(s,b,\theta)
\]</span></p>
<h3 id="main-limitations-of-reinforce">Main Limitations of
REINFORCE</h3>
<ul>
<li>REINFORCE requires complete episodes to compute the return for each
state, as it relies on Monte Carlo estimates of the expected return.
This makes it unsuitable for continuing tasks (non-episodic
environments) where there is no clear terminal state.</li>
<li>The gradient estimates in REINFORCE have high variance because they
depend on the total return from sampled episodes, which can vary
significantly due to stochasticity in the environment and policy.</li>
<li>Since REINFORCE updates the policy only after completing an episode,
it does not make use of intermediate data. This results in poor sample
efficiency, requiring a large number of episodes to learn
effectively.</li>
<li>Unlike Temporal Difference (TD) methods, REINFORCE does not use
bootstrapping (i.e., it does not update value estimates based on other
estimates). It relies solely on complete returns from episodes.</li>
<li>The algorithm is highly sensitive to the choice of the learning
rate. A poorly chosen learning rate can lead to divergence or extremely
slow convergence.</li>
</ul>
<h3 id="advantage-function">Advantage Function</h3>
<p>Advantage function employs the idea of differential return. In the
REINFORCE algorithm, with advantage function, policy gradient can be
re-written as <span class="math display">\[
\begin{aligned}
\nabla_\theta J(\theta) &amp;=E_\pi\Big[Q^\pi(s,a)\nabla_\theta
\ln\pi_\theta(a|s)\Big]\\
&amp;=E_\pi\Big[A^\pi(s,a)\nabla_\theta \ln\pi_\theta(a|s)\Big]\\
\end{aligned}
\]</span> where <span class="math display">\[
\begin{aligned}
A^{\pi}(s,a) &amp;= Q^\pi(s,a)-V^\pi(s)\\
V^\pi(s) &amp;= \sum_{a\in A}\pi(a|s) Q(s,a)
\end{aligned}
\]</span></p>
<h3 id="off-policy-policy-gradient">Off-Policy Policy Gradient</h3>
<p>Off-policy policy gradient methods allow learning a target policy
while using data generated from a different behavior policy. By reusing
past experiences and learning from suboptimal actions, off-policy
methods can significantly improve sample efficiency. Off-policy learning
allows for better exploration strategies since it can incorporate data
from various policies, including exploratory ones.</p>
<p>The policy gradient estimate is defined as <span
class="math display">\[
\nabla_\theta J(\theta) = E_\beta\Big[{\pi_\theta(a|s)\over \beta(a|s)}
Q^\pi(s,a) \nabla _\theta \ln \pi_\theta (a|s)\Big]
\]</span> <span class="math inline">\(\beta(a|s)\)</span> refers to the
behavior policy that generates the data used for training. The behavior
policy is not necessarily the optimal policy we want to learn (the
target policy <span class="math inline">\(\pi(a|s)\)</span>). Instead,
it can be any policy that provides useful exploration of the
state-action space. <span class="math inline">\({\pi_\theta(a|s)\over
\beta(a|s)}\)</span> is the important weight for sampling.</p>
<h3 id="trust-region-policy-optimization-trpo">Trust Region Policy
Optimization (TRPO)</h3>
<p>TRPO is an advanced policy gradient method in RL designed to optimize
policies while ensuring stable and reliable updates. It addresses some
of the limitations of traditional policy gradient methods by
incorporating a trust region constraint that limits how much the policy
can change in a single update. The difference between REINFORCE and TRPO
is that TRPO uses off-policy policy gradient and advantage function, as
well as a constraint on the Jullback-Leibler (KL) divergence between the
old and new policy.</p>
<p>Recall that REINFORCE's objective of policy gradient is: <span
class="math display">\[
J(\theta) = \sum_{s\in S} d^\pi(s) \sum_{a\in A} \pi_\theta(a|s)
Q^\pi(s,a)
\]</span> The derivation of TRPO's objective of policy gradient is:
<span class="math display">\[
\begin{aligned}
J(\theta) &amp;=\sum_{s \in S} d^{\pi}(s) \sum_{a\in A} (\pi_\theta(a|s)
\hat{A}_{\theta_{old}}(s,a))\\
&amp;=\sum_{s\in S} d^{\pi_{\theta_{old}}} \sum_{a\in A} (\beta(a|s)
{\pi_\theta(a|s)\over \beta(a|s)} \hat{A}_{\theta_{old}}(s,a))\\
&amp;=E_{s\sim d^{\pi_{\theta_{old}}}, a \sim
\beta}\Big[{\pi_\theta(a|s)\over \beta(a|s)}
\hat{A}_{\theta_{old}}(s,a)\Big]\\
&amp;=E_{s\sim d^{\pi_{\theta_{old}}}, a \sim
\pi_{\theta_{old}}}\Big[{\pi_\theta(a|s)\over \pi_{\theta_{old}}(a|s)}
\hat{A}_{\theta_{old}}(s,a)\Big]\\
E_{s\sim
d^{\pi_{\theta_{old}}}}&amp;\Big[D_{KL}\Big(\pi_{\theta_{old}}(\cdot |
s)||\pi_\theta(\cdot|s)\Big)\Big] \leq \delta
\end{aligned}
\]</span> The TRPO constrained optimization is defined as <span
class="math display">\[
\begin{aligned}
&amp;\max E_{s\sim d^{\pi_{\theta_{old}}}, a \sim
\pi_{\theta_{old}}}\Big[{\pi_\theta(a|s)\over \pi_{\theta_{old}}(a|s)}
\hat{A}_{\theta_{old}}(s,a)\Big]\\
&amp; s.t. E_{s\sim
d^{\pi_{\theta_{old}}}}\Big[D_{KL}\Big(\pi_{\theta_{old}}(\cdot |
s)||\pi_\theta(\cdot|s)\Big)\Big] \leq \delta
\end{aligned}
\]</span> One of the main limitations of TRPO is that the constrained
optimization problem can be computationally intensive, especially for
large state and action spaces.</p>
<h2 id="ppo">PPO</h2>
<h3 id="ppo-objective">PPO Objective</h3>
<p>To address the computational expense of the constrained optimization
in TRPO, researchers introduced the <strong>CLIP objective</strong> in
policy gradient methods. The CLIP objective simplifies the optimization
process while maintaining stable policy updates. Below are the TRPO
objective and its corresponding CLIP version: <span
class="math display">\[
\begin{aligned}
J^{TRPO}(\theta) &amp;= E[r(\theta)\hat{A}_{\theta_{old}}(s,a)]\\
J^{CLIP}(\theta) &amp;= E[\min(r(\theta)\hat{A}_{\theta_{old}}(s,a)),
\text{clip}(r(\theta),1-\epsilon, 1+\epsilon)
\hat{A}_{\theta_{old}}(s,a)]
\end{aligned}
\]</span> where <span class="math display">\[
\begin{aligned}
&amp;r(\theta) = {\pi_{\theta (a|s)} \over \pi_{\theta_{old}(a|s)}}\\
&amp;r(\theta) \in [1-\epsilon, 1+ \epsilon], \\
&amp;i.e. 1-\epsilon \leq r(\theta) \leq 1+\epsilon
\end{aligned}
\]</span> If <span class="math inline">\(r(\theta) &gt; 1+\epsilon,
r(\theta)=1+\epsilon\)</span>. If <span class="math inline">\(r(\theta)
&lt; 1-\epsilon, r(\theta) = 1-\epsilon\)</span>.</p>
<p>The CLIP objective ensures that the policy ratio <span
class="math inline">\(r(\theta)\)</span> does not deviate too far from 1
(the old policy), thereby limiting large updates to the policy. The term
<span class="math inline">\(J^{CLIP}(\theta)\)</span> takes the minimum
of the “unclipped” objective and the “clipped” version. This prevents
overly large policy updates by removing the lower bound when <span
class="math inline">\(r(\theta)\)</span> is outside the clipping
range.</p>
<p>The <strong>Proximal Policy Optimization (PPO)</strong> algorithm
(Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06347">Proximal Policy
Optimization Algorithms</a>) extends the CLIP objective by incorporating
additional terms for value function optimization and entropy
regularization. The full PPO objective is defined as: <span
class="math display">\[
J^{PPO}(\theta) = E[J^{CLIP}(\theta) - c_1(V_\theta(s)-V_{target})^2 +
c_2 H(s,\pi_\theta(\cdot))]
\]</span> where</p>
<ul>
<li><span class="math inline">\(-(V_\theta(s) - V_{target})^2\)</span>
is the negative mean squared error (MSE), which we aim to maximize. It
minimizes the difference between the predicted value function <span
class="math inline">\(V_\theta(s)\)</span> and the target value <span
class="math inline">\(V_{target}\)</span>. The coefficient <span
class="math inline">\(c_2\)</span> controls the tradeoff between policy
optimization and value function fitting.</li>
<li><span class="math inline">\(H(s,\pi_\theta(\cdot))\)</span>
represents the entropy of the policy. Maximizing entropy encourages
exploration by preventing premature convergence to deterministic
policies. The coefficient <span class="math inline">\(c_2\)</span>
determines the weight of this entropy term.</li>
</ul>
<p>Here is a pseudocode of PPO-Clip Algorithm (Source: <a
target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">OpenAI
Spinning Up - Proximal Policy Optimization</a>)</p>
<figure>
<img src="/di-blog/2024/12/29/Understanding-RLHF/ppo_clip_algo.png"
alt="ppo_clip_algo" />
<figcaption aria-hidden="true">ppo_clip_algo</figcaption>
</figure>
<h3 id="ppo-usage">PPO Usage</h3>
<h4 id="state-action-and-reward-in-the-context-of-llms"><strong>State,
Action, and Reward in the Context of LLMs</strong></h4>
<p>In the context of LLMs, the components of reinforcement learning are
defined as follows:</p>
<ol type="1">
<li><strong>State</strong>: The state corresponds to the <strong>input
prompt</strong> or context provided to the language model. It represents
the scenario or query that requires a response.</li>
<li><strong>Action</strong>: The action is the <strong>output</strong>
generated by the language model, i.e., the response or continuation of
text based on the given state (prompt).</li>
<li><strong>Reward</strong>: The reward is a scalar value that
quantifies how well the generated response aligns with human preferences
or task objectives. It is typically derived from a <strong>reward
model</strong> trained on human feedback.</li>
<li><strong>Policy</strong>: A policy refers to the strategy or function
that maps a given state (input prompt and context) to an action (the
next token or sequence of tokens to generate). The policy governs how
the LLM generates responses and is optimized to maximize a reward
signal, such as alignment with human preferences or task-specific
objectives.</li>
</ol>
<h4 id="steps-of-rlhf-using-ppo"><strong>Steps of RLHF Using
PPO</strong></h4>
<p>The RLHF process using PPO involves three main stages:</p>
<ol type="1">
<li><p><strong>Training a Reward Model</strong>: A reward model is
trained to predict human preferences based on labeled data. Human
annotators rank multiple responses for each prompt, and this ranking
data is used to train the reward model in a supervised manner. The
reward model learns to assign higher scores to responses that align
better with human preferences.</p></li>
<li><p><strong>Fine-Tuning the LLM with PPO</strong>: After training the
reward model, PPO is used to fine-tune the LLM. The steps are as
follows:</p>
<ol type="1">
<li><p><strong>Initialize Policies</strong>: Start with a pre-trained
LLM as both the <strong>policy model</strong> (actor) and optionally as
the critic for value estimation.</p>
<ul>
<li><p>The <strong>actor</strong> is the language model that generates
responses (actions) based on input prompts (states).</p>
<p>For example: Input: “Explain quantum mechanics.” Output: “Quantum
mechanics is a branch of physics that studies particles at atomic and
subatomic scales.”</p></li>
<li><p>The <strong>critic</strong> is typically implemented as a
<strong>value function</strong>, which predicts how good a particular
response (action) is in terms of achieving long-term objectives. This
model predicts a scalar value for each token or sequence, representing
its expected reward or usefulness.</p>
<p>For example:</p>
<p>Input: “Explain quantum mechanics.” → “Quantum mechanics is…” Output:
A value score indicating how well this response aligns with human
preferences or task objectives.</p></li>
<li><p>Both the actor and critic can be initialized from the same
pre-trained LLM weights to leverage shared knowledge from pretraining.
However, their roles diverge during fine-tuning: The actor focuses on
generating responses. The critic focuses on evaluating those
responses.</p></li>
</ul></li>
<li><p><strong>Collect Rollouts</strong>: Interact with the environment
by sampling prompts from a dataset. Generate responses (actions) using
the current policy. Compute rewards for these responses using the
trained reward model.</p></li>
<li><p><strong>Compute Advantage Estimates</strong>: Use rewards from
the reward model and value estimates from the critic to compute
advantages: <span class="math display">\[
\hat{A}(s, a) = R_t + \gamma V(s_{t+1}) - V(s_t),
\]</span> where $ R_t $ is the reward from the reward model.</p></li>
<li><p><strong>Optimize Policy with PPO Objective</strong>: Optimize the
policy using PPO's clipped surrogate objective: <span
class="math display">\[
J^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r(\theta)\hat{A}(s, a),
\text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)\hat{A}(s,
a)\right)\right],
\]</span> where $ r() = $ is the probability ratio between new and old
policies.</p></li>
<li><p><strong>Update Value Function</strong>: Simultaneously update the
value function by minimizing mean squared error between predicted values
and rewards: <span class="math display">\[
\mathcal{L}_{\text{value}} = \mathbb{E}\left[(V_\theta(s) -
R_t)^2\right].
\]</span></p></li>
<li><p><strong>Repeat</strong>: Iterate over multiple epochs until
convergence, ensuring stable updates by clipping policy
changes.</p></li>
</ol></li>
<li><p><strong>Evaluation</strong>: Evaluate the fine-tuned LLM on
unseen prompts to ensure it generates outputs aligned with human
preferences. Optionally, collect additional human feedback to further
refine both the reward model and policy.</p></li>
</ol>
<p>The following diagrams summarizes the high-level RLHF process with
PPO, from preference data creation, to training a reward model, and
using reward model in an RL loop to fine tune LLM.</p>
<figure>
<img src="/di-blog/2024/12/29/Understanding-RLHF/PPO_RLHF_flowchart.png"
alt="PPO_RLHF_flowchart" />
<figcaption aria-hidden="true">PPO_RLHF_flowchart</figcaption>
</figure>
<p>The following workflow chart illustrates the more detailed training
process of RLHF with PPO. (Source: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.04964">Secrets of RLHF in Large
Language Models Part I: PPO</a>)</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/RLHF_training_realworld.png"
alt="RLHF_training_realworld" />
<figcaption aria-hidden="true">RLHF_training_realworld</figcaption>
</figure>
<h4 id="rlhf-training-tricks">RLHF Training Tricks</h4>
<p>There are practical challenges that arise during RLHF training. These
challenges stem from the inherent complexities of RL, especially when
applied to aligning LLMs with human preferences. Therefore, tricks are
essential for addressing the practical limitations of RLHF, ensuring the
training process remains efficient, stable, and aligned with human
preferences while minimizing the impact of inherent challenges in RL
systems. (Source: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.04964">Secrets of
RLHF in Large Language Models Part I: PPO</a>)</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/RLHF_training_tricks.png"
alt="RLHF_training_tricks" />
<figcaption aria-hidden="true">RLHF_training_tricks</figcaption>
</figure>
<h2 id="dpo">DPO</h2>
<h3 id="bradley-terry-and-plackett-luce-reward-model">Bradley-Terry and
Plackett-Luce Reward Model</h3>
<p>The <strong>Bradley-Terry (BT) model</strong> is a probabilistic
model used to compare pairwise preferences. It assumes that each item
(e.g., a response or completion) has an intrinsic quality score, and the
probability of one item being preferred over another depends on the
relative difference in their scores.</p>
<p>Mathematically, the probability of option <span
class="math inline">\(y_1\)</span> being preferred over option <span
class="math inline">\(y_2\)</span> is given by: <span
class="math display">\[
P(y_1 \succ y_2|x) = {\exp(r(x,y_1)) \over \exp(r(x,y_1)) +
\exp(r(x,y_2))}
\]</span> The loss of this reward model is <span class="math display">\[
L_R(r_{\phi},D) = -E_{(x,y_w,y_l)\sim D} \Big[\log
\sigma\Big(r_{\phi}(x,y_w) - r_{\phi}(x,y_l)\Big)\Big]
\]</span> However, the BT model has some limitations:</p>
<ul>
<li>It assumes transitivity in preferences (if <span
class="math inline">\(A&gt;B\)</span> and <span
class="math inline">\(B&gt;C\)</span>, then <span
class="math inline">\(A &gt;C\)</span>), which may not always hold in
real-world data.</li>
<li>It only handles pairwise comparisons and does not naturally extend
to rankings involving more than two items.</li>
</ul>
<p>The <strong>Plackett-Luce (PL) model</strong> generalizes the
Bradley-Terry model to handle rankings of multiple items, not just
pairwise comparisons. It models the probability of a ranking as a
sequence of choices. The first-ranked item is chosen based on its
relative worth compared to all other items. The second-ranked item is
chosen from the remaining items, and so on.</p>
<p>Mathematically, for a ranking <span class="math inline">\(i_1\succ
i_2 \succ ... \succ i_J\)</span>, the probability is given by: <span
class="math display">\[
P(i_1\succ i_2 \succ ... \succ i_J ) = \prod^j_{j=1}{\alpha_{i_j}\over
\sum^J_{k=j} \alpha_{i_k}}
\]</span> where <span class="math inline">\(\alpha_{i_j}\)</span> is the
worth or quality score of item <span class="math inline">\(i_j\)</span>.
The denominator normalizes over all remaining items at each step.</p>
<p>The PL model has several advantages over the BT model:</p>
<ul>
<li>Unlike BT, which only works with pairwise comparisons, PL can handle
rankings involving multiple items.</li>
<li>PL can accommodate partial rankings (e.g., ranking only the top n
items), making it more versatile in scenarios where full rankings are
unavailable.</li>
<li>When human feedback involves ranking multiple responses rather than
just picking one as better, PL captures this richer information better
than BT.</li>
</ul>
<h3 id="dpo-objective">DPO Objective</h3>
<p>The main reason why RLHF with PPO is hard is that it takes a lot of
redundant effort. Policy Model is all we need, all other efforts are not
necessary. <strong>DPO (Direct Preference Optimization)</strong> is a
novel alternative to traditional RLHF for fine-tuning LLMs. It
simplifies the RLHF process by eliminating the need for complex reward
models and RL algorithms. Instead, DPO reframes the problem of aligning
LLMs with human preferences as a classification problem using
human-labeled preference data.</p>
<p>The idea is DPO and difference between DPO and PPO are shown in the
figure below (Source: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.18290">Direct
Preference Optimization: Your Language Model is Secretly a Reward
Model</a>)</p>
<figure>
<img src="/di-blog/2024/12/29/Understanding-RLHF/DPO_idea.png"
alt="DPO_idea" />
<figcaption aria-hidden="true">DPO_idea</figcaption>
</figure>
<p>Recall the Bradley-Terry reward model: <span class="math display">\[
\begin{aligned}
P(y_1 \succ y_2|x) &amp;= {\exp(r(x,y_1)) \over \exp(r(x,y_1)) +
\exp(r(x,y_2))}\\
L_R(r_{\phi},D) &amp;= -E_{(x,y_w,y_l)\sim D} \Big[\log
\sigma\Big(r_{\phi}(x,y_w) - r_{\phi}(x,y_l)\Big)\Big]
\end{aligned}
\]</span> <strong>RLHF objective</strong> is defined as follows. Keep in
mind that no matter whether DPO or PPO is used, the objective is always
like this. <span class="math display">\[
\max_{\pi_\theta} E_{x \sim D, y \sim \pi_\theta(y|x)}\Big[r_{\phi}(x,y)
- \beta D_{KL}\big[\pi_\theta(y|x) || \pi_{ref}(y|x)\big]\Big]
\]</span> where <span class="math inline">\(\beta
D_{KL}\big[\pi_\theta(y|x) || \pi_{ref}(y|x)\big]\)</span> is a
regularization term. When applying RL to NLP, regularization is often
needed. Otherwise RL would explore every possible situation and find out
hidden tricks which deviate from a language model.</p>
<p>The optimal policy <span class="math inline">\(\pi_r(y|x)\)</span>
that maximizes the objective is <span class="math display">\[
\begin{aligned}
\pi_r(y|x) &amp;= {1\over Z(x)}\pi_{ref}(y|x)\exp\Big({1\over
\beta}r(x,y)\Big)\\
Z(x) &amp;= \sum_y \pi_{ref}(y|x) \exp\Big({1\over \beta}r(x,y)\Big)
\end{aligned}
\]</span> where <span class="math inline">\(\pi_r(y|x)\)</span> is a
probability distribution.</p>
<p>Based on this optimal policy, we can derive the reward function for
the optimal policy <span class="math display">\[
r(x,y)=\beta \log{\pi_r(y|x)\over \pi_{ref}(y|x)} + \beta \log Z(x)
\]</span> If we put this reward function in the Bradley-Terry model, we
obtain a probability of <span class="math inline">\(y_1\)</span> being
prefered to <span class="math inline">\(y_2\)</span>. <span
class="math display">\[
\begin{aligned}
P^*(y_1 \succ y_2|x) &amp;= {\exp^*(r(x,y_1)) \over \exp^*(r(x,y_1)) +
\exp^*(r(x,y_2))}\\
&amp;={\exp(\beta \log{\pi^*_r(y_1|x)\over \pi_{ref}(y_1|x)} + \beta
\log Z(x)) \over \exp(\beta \log{\pi^*_r(y_1|x)\over \pi_{ref}(y_1|x)} +
\beta \log Z(x)) + \exp(\beta \log{\pi^*_r(y_2|x)\over \pi_{ref}(y_2|x)}
+ \beta \log Z(x))}\\
&amp;={1\over 1+\exp\Big(\beta \log {\pi^*(y_2|x) \over
\pi_{ref}(y_2|x)} - \beta\log {\pi^*(y_1|x)\over
\pi_{ref}(y_1|x)}\Big)}\\
&amp;=\sigma\Big(\beta \log {\pi^*(y_1|x)\over \pi_{ref}(y_1|x)} - \beta
\log {\pi^*(y_2|x)\over \pi_{ref}(y_2|x)}\Big)\\
\end{aligned}
\]</span> With this probability, we have <strong>DPO's objective
function</strong> below. We can optimize this loss function by
<strong>Maximum Likelihood Extimation</strong>: <span
class="math display">\[
L_{DPO}(\pi_\theta; \pi_{ref}) = -E_{(x,y_w,y_l) \sim D} \Big[\log
\sigma \Big(\beta \log {\pi_{\theta}(y_w|x)\over \pi_{ref}(y_w|x)} -
\beta \log {\pi_{\theta}(y_l|x)\over \pi_{ref}(y_l|x)}\Big)\Big)\Big]
\]</span> <strong>Key Ideas of DPO Objective</strong>:</p>
<ul>
<li>DPO's objective aims to increase the likelihood of generating
preferred responses over less preferred ones. By focusing directly on
preference data, DPO eliminates the need to first fit a reward model
that predicts scalar rewards based on human preferences. This simplifies
the training pipeline and reduces computational overhead.</li>
<li>Value functions exist to help reduce the variance of the reward
model. In DPO, the value function is not involved because DPO does not
rely on a traditional RL framework, such as Actor-Critic methods.
Instead, DPO directly optimizes the policy using human preference data
as a <strong>classification task</strong>, skipping the intermediate
steps of training a reward model or estimating value functions.</li>
<li>DPO was originally designed to work with <strong>pairwise</strong>
preference data, however, recent advancements and adaptations have
extended its applicability to ranking preference data as well (e.g
RankDPO).</li>
</ul>
<p>DPO paper has provided detailed steps of deriving the gradient of the
DPO objective: (Source: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization:
Your Language Model is Secretly a Reward Model</a>)</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/dpo_gradients_derivation_paper.png"
alt="dpo_gradients_derivation_paper" />
<figcaption
aria-hidden="true">dpo_gradients_derivation_paper</figcaption>
</figure>
<p>The simplified version of the DPO gradient for better understanding
is written as follows. Intuitively, when the difference between <span
class="math inline">\(\hat{r}_{\theta}(x, y_l)\)</span> and <span
class="math inline">\(\hat{r}_{\theta}(x, y_w)\)</span> is large, the
gradient takes a larger step during optimization. Conversely, when the
difference is small, the objective is optimized with a smaller
adjustment.</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/DPO_gradient_simple_version.png"
alt="DPO_gradient_simple_version" />
<figcaption aria-hidden="true">DPO_gradient_simple_version</figcaption>
</figure>
<h3 id="dpo-usage">DPO Usage</h3>
<p>Here's how DPO is applied step by step:</p>
<p><strong>1. Initial Setup and Supervised Fine-Tuning (SFT)</strong>:
Begin by fine-tuning a pre-trained LLM using supervised learning on a
dataset that is representative of the tasks the model will perform. This
step ensures the model has a strong foundation in the relevant domain,
preparing it for preference-based optimization.</p>
<p><strong>2. Collect Preference Data</strong>: Gather human feedback in
the form of pairwise preferences or rankings. Annotators evaluate
responses generated by the model and indicate which ones they prefer.
Construct a dataset of prompts and corresponding preferred and
less-preferred responses.</p>
<p><strong>3. Iterative Rounds of DPO</strong></p>
<ul>
<li><p><strong>Sampling and Annotation</strong>: In each round, sample a
set of responses from the model for given prompts. Collect new
preference annotations based on these samples, allowing for dynamic
updates to the preference dataset. (Public preference data works as
well. Off-policy and on-policy data both work).</p></li>
<li><p><strong>Preference Optimization</strong>: Use DPO to adjust the
model's outputs based on collected preference data:</p></li>
<li><p><strong>Model Update</strong>: Fine-tune the model using this
loss function to increase the likelihood of generating preferred
responses.</p></li>
</ul>
<p><strong>4. Evaluation and Iteration</strong></p>
<ul>
<li><p><strong>Performance Assessment</strong>: After each round,
evaluate the model’s performance on new prompts to ensure it aligns with
human preferences. Use feedback from these evaluations to inform
subsequent rounds of sampling and optimization.</p></li>
<li><p><strong>Iterative Refinement</strong>: Continue this loop process
over multiple rounds, iteratively refining the model's alignment with
human preferences through continuous sampling and preference
optimization.</p></li>
</ul>
<h3 id="dpo-performance">DPO Performance</h3>
<p>(Source: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.18290">Direct Preference
Optimization: Your Language Model is Secretly a Reward Model</a>)</p>
<figure>
<img
src="/di-blog/2024/12/29/Understanding-RLHF/DPO_performance_paper.png"
alt="DPO_performance_paper" />
<figcaption aria-hidden="true">DPO_performance_paper</figcaption>
</figure>
<h3 id="dpo-objective-pseudocode">DPO Objective Pseudocode</h3>
<p><span class="math display">\[
L_{DPO}(\pi_\theta; \pi_{ref}) = -E_{(x,y_w,y_l) \sim D} \Big[\log
\sigma \Big(\beta \log {\pi_{\theta}(y_w|x)\over \pi_{ref}(y_w|x)} -
\beta \log {\pi_{\theta}(y_l|x)\over \pi_{ref}(y_l|x)}\Big)\Big)\Big]
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_loss</span>(<span class="params">pi_logps, ref_logps, yw_idxs, yl_idxs, beta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    pi_logps: policy logprobs, shape (B,)</span></span><br><span class="line"><span class="string">    ref_logps: reference model logprobs, shape (B,)</span></span><br><span class="line"><span class="string">    yw_idxs: preferred completion indices in [0, B-1], shape (T,)</span></span><br><span class="line"><span class="string">    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)</span></span><br><span class="line"><span class="string">    beta: temperature controlling strength of KL penalty</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Each pair of (yw_idxs[i], yl_idxs[i]) represents the</span></span><br><span class="line"><span class="string">    indices of a single preference pair.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]</span><br><span class="line">    ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]</span><br><span class="line"></span><br><span class="line">    pi_logratios = pi_yw_logps - pi_yl_logps</span><br><span class="line">    ref_logratios = ref_yw_logps - ref_yl_logps</span><br><span class="line"></span><br><span class="line">    losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))</span><br><span class="line">    rewards = beta * (pi_logps - ref_logps).detach()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> losses, rewards</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="dpo-variants">DPO Variants</h3>
<p>The key area of research involves developing variants of DPO and
conducting theoretical analyses to understand its limitations and
potential improvements. This includes exploring different loss functions
or optimization strategies that can be applied within the DPO
framework.</p>
<ul>
<li><p>One significant area of research focuses on refining the loss
function used in DPO. This includes exploring ways to eliminate the need
for a reference model, which can simplify the optimization process.</p>
<p>Examples:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07691">ORPO: Monolithic
Preference Optimization without Reference Model</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.14734">SimPO: Simple
Preference Optimization with a Reference-Free Reward</a></p></li>
</ul></li>
<li><p>Another key direction involves leveraging existing supervised
fine-tuning data as preference data for DPO. This strategy aims to
enhance the quality of preference data by utilizing high-quality labeled
datasets that may already exist from previous SFT processes.</p>
<p>Examples:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.08005v1">Refined Direct
Preference Optimization with Synthetic Data for Behavioral Alignment of
LLMs</a></li>
</ul></li>
</ul>
<h2 id="main-difficulties-in-rlhf">Main Difficulties in RLHF</h2>
<h3 id="data-collection"><strong>Data Collection</strong></h3>
<p>In practice, people noticed that the collection of human feedback in
the form of the preference dataset is a slow manual process that needs
to be repeated whenever alignment criteria change. And there is
increasing difficulty in annotating preference data as models become
more advanced, particularly because distinguishing between outputs
becomes more nuanced and subjective.</p>
<ul>
<li>The paper “<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.02481">CDR:
Customizable Density Ratios of Strong-over-weak LLMs for Preference
Annotation</a>” explains that as models become more advanced, it becomes
harder to identify which output is better due to subtle differences in
quality. This makes preference data annotation increasingly difficult
and subjective.</li>
<li>Another paper, “<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.14916">Improving
Context-Aware Preference Modeling for Language Models</a>,” discusses
how the underspecified nature of natural language and multidimensional
criteria make direct preference feedback difficult to interpret. This
highlights the challenge of providing consistent annotations when
outputs are highly sophisticated and nuanced.</li>
<li>“<a target="_blank" rel="noopener" href="https://www.arxiv.org/abs/2408.12799">Less for More:
Enhancing Preference Learning in Generative Language Models</a>” also
notes that ambiguity among annotators leads to inconsistently annotated
datasets, which becomes a greater issue as model outputs grow more
complex.</li>
</ul>
<h3 id="reward-hacking"><strong>Reward Hacking</strong></h3>
<p>Reward hacking is a common problem in reinforcement learning, where
the agent learns to exploit the system by maximizing its reward through
actions that deviate from the intended goal. In the context of RLHF,
reward hacking occurs when training settles in an unintended region of
the loss landscape. In this scenario, the model generates responses that
achieve high reward scores, but these responses may fail to be
meaningful or useful to the user.</p>
<p>In PPO, reward hacking occurs when the model exploits flaws or
ambiguities in the <strong>reward model</strong> to achieve high rewards
without genuinely aligning with human intentions. This is because PPO
relies on a learned reward model to guide policy updates, and any
inaccuracies or biases in this model can lead to unintended behaviors
being rewarded. PPO is particularly vulnerable to reward hacking if the
reward model is not robustly designed or if it fails to capture the true
objectives of human feedback. The iterative nature of PPO, which
involves continuous policy updates based on reward signals, can
exacerbate this issue if not carefully managed.</p>
<p>DPO avoids explicit reward modeling by directly optimizing policy
based on preference data. However, it can still encounter issues similar
to reward hacking if the preference data is <strong>biased</strong> or
if the optimization process leads to <strong>overfitting</strong>
specific patterns in the data that do not generalize well. While DPO
does not suffer from reward hacking in the traditional sense (since it
lacks a separate reward model), it can still find biased solutions that
exploit <strong>out-of-distribution responses</strong> or deviate from
intended behavior due to distribution shifts between training and
deployment contexts.</p>
<ul>
<li>The article "<a
target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">Reward
Hacking in Reinforcement Learning</a>" by Lilian Weng discusses how
reward hacking occurs when a RL agent exploits flaws or ambiguities in
the reward function to achieve high rewards without genuinely learning
the intended task. It highlights that in RLHF for language models,
reward hacking is a critical challenge, as models might learn to exploit
unit tests or mimic biases to achieve high rewards, which can hinder
real-world deployment.</li>
<li>The research "<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.10760">Scaling
Laws for Reward Model Overoptimization</a>" explores how optimizing
against reward models trained to predict human preferences can lead to
overoptimization, hindering the actual objective.
<ol type="1">
<li><strong>Impact of Policy Model Size</strong>: Holding the RM size
constant, experiments showed that larger policy models exhibited similar
overoptimization trends as smaller models, despite achieving higher
initial gold scores. This implies that their higher performance on gold
rewards does not lead to excessive optimization pressure on the RM.</li>
<li><strong>Relationship with RM Data Size</strong>: Data size had a
notable effect on RM performance and overoptimization. Models trained on
fewer than ~2,000 comparison labels showed near-chance performance, with
limited improvement in gold scores. Beyond this threshold, all RMs,
regardless of size, benefited from increased data, with larger RMs
showing greater improvements in gold rewards compared to smaller
ones.</li>
<li><strong>Scaling Laws for RM Parameters and Data Size</strong>:
Overoptimization patterns scaled smoothly with both RM parameter count
and data size. Larger RMs demonstrated better alignment with gold
rewards and less susceptibility to overoptimization when trained on
sufficient data, indicating improved robustness.</li>
<li><strong>Proxy vs. Gold Reward Trends</strong>: For small data sizes,
proxy reward scores deviated significantly from gold reward scores,
highlighting overoptimization risks. As data size increased, the gap
between proxy and gold rewards narrowed, reducing overoptimization
effects.</li>
</ol></li>
</ul>
<p>Note that the KL divergence term in the RLHF objective is intended to
prevent the policy from deviating too much from a reference model,
thereby maintaining stability during training. However, it does not
fully prevent reward hacking. Reward hacking occurs when an agent
exploits flaws or ambiguities in the reward model to achieve high
rewards without genuinely aligning with human intentions. The KL
divergence penalty does not correct these flaws in the reward model
itself, meaning that if the reward model is misaligned, the agent can
still find ways to exploit it. KL does not directly address whether the
actions align with the true objectives or desired outcomes.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/di-blog/tags/knowledge/" rel="tag"># knowledge</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/di-blog/2024/12/26/Understanding-QLoRA/" rel="prev" title="Understanding QLoRA">
                  <i class="fa fa-angle-left"></i> Understanding QLoRA
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/di-blog/2025/01/04/2025-January/" rel="next" title="2025 January - What I Have Read">
                  2025 January - What I Have Read <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Di Zhen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/di-blog/js/comments.js"></script><script src="/di-blog/js/utils.js"></script><script src="/di-blog/js/motion.js"></script><script src="/di-blog/js/sidebar.js"></script><script src="/di-blog/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/di-blog/js/third-party/math/mathjax.js"></script>



</body>
</html>
