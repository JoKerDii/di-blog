<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/di-blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/di-blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/di-blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/di-blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/di-blog/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jokerdii.github.io","root":"/di-blog/","images":"/di-blog/images","scheme":"Muse","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/di-blog/js/config.js"></script>

    <meta name="description" content="This is a collection of some common useful LangChain (v.0.3.3) practices based on my coding experience so far. LLM Application Development Landscape Nowadays, LLM applications can be classified into t">
<meta property="og:type" content="article">
<meta property="og:title" content="LangChain Common Practices">
<meta property="og:url" content="https://jokerdii.github.io/di-blog/2024/12/25/Langchain-Common-Practices/index.html">
<meta property="og:site_name" content="Di&#39;s Blog">
<meta property="og:description" content="This is a collection of some common useful LangChain (v.0.3.3) practices based on my coding experience so far. LLM Application Development Landscape Nowadays, LLM applications can be classified into t">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-12-25T10:04:50.000Z">
<meta property="article:modified_time" content="2024-12-25T10:04:50.000Z">
<meta property="article:author" content="Di Zhen">
<meta property="article:tag" content="knowledge">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jokerdii.github.io/di-blog/2024/12/25/Langchain-Common-Practices/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://jokerdii.github.io/di-blog/2024/12/25/Langchain-Common-Practices/","path":"2024/12/25/Langchain-Common-Practices/","title":"LangChain Common Practices"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LangChain Common Practices | Di's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/di-blog/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/di-blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Di's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-application-development-landscape"><span class="nav-number">1.</span> <span class="nav-text">LLM Application
Development Landscape</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chaining-a-simple-prompt"><span class="nav-number">2.</span> <span class="nav-text">Chaining a Simple Prompt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#parsing-the-output-with-a-customized-format"><span class="nav-number">3.</span> <span class="nav-text">Parsing the Output
with a Customized Format</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#data-ingestion-to-pinecone-vectorstore-rag"><span class="nav-number">4.</span> <span class="nav-text">Data Ingestion to
Pinecone Vectorstore (RAG)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#data-retrieval-from-pinecone-vectorestore-rag"><span class="nav-number">5.</span> <span class="nav-text">Data Retrieval
from Pinecone Vectorestore (RAG)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chat-with-a-pdf-rag-with-faiss"><span class="nav-number">6.</span> <span class="nav-text">Chat with a PDF (RAG with
FAISS)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#create-a-react-agent"><span class="nav-number">7.</span> <span class="nav-text">Create a ReAct Agent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#using-an-langchain-agent-for-tasks"><span class="nav-number">8.</span> <span class="nav-text">Using an LangChain Agent for
Tasks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#creating-an-react-agent-with-multiple-agents-provided-as-tools"><span class="nav-number">9.</span> <span class="nav-text">Creating
an ReAct Agent with Multiple Agents Provided as Tools</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#function-tool-calling"><span class="nav-number">10.</span> <span class="nav-text">Function &#x2F; Tool Calling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tool-calling-with-langchain"><span class="nav-number">11.</span> <span class="nav-text">Tool Calling with Langchain</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#token-limitation-handling-strategies"><span class="nav-number">12.</span> <span class="nav-text">Token Limitation Handling
Strategies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coreference-resolution"><span class="nav-number">13.</span> <span class="nav-text">Coreference Resolution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tracing-application-with-langsmith"><span class="nav-number">14.</span> <span class="nav-text">Tracing Application with
LangSmith</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#langchain-hub"><span class="nav-number">15.</span> <span class="nav-text">LangChain Hub</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#langchain-text-splitter-playground"><span class="nav-number">16.</span> <span class="nav-text">LangChain Text Splitter
Playground</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Di Zhen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/di-blog/archives/">
          <span class="site-state-item-count">47</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/12/25/Langchain-Common-Practices/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="LangChain Common Practices | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LangChain Common Practices
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-25 05:04:50" itemprop="dateCreated datePublished" datetime="2024-12-25T05:04:50-05:00">2024-12-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>This is a collection of some common useful LangChain (v.0.3.3)
practices based on my coding experience so far.</p>
<h2 id="llm-application-development-landscape">LLM Application
Development Landscape</h2>
<p>Nowadays, LLM applications can be classified into the following
categories.</p>
<ol type="1">
<li><p><strong>Simple LLM Calls</strong></p>
<p>Applications where LLMs are used directly to answer questions or
perform tasks without additional layers of complexity. The focus is on
generating responses to prompts or queries. These are straightforward
implementations, often used for tasks like content generation, question
answering, or summarization.</p>
<p>Real-world examples:</p>
<ul>
<li>ChatGPT for Q&amp;A: Users input questions, and the model directly
generates answers.</li>
<li>Copywriting Tools: Applications like Jasper AI create marketing
content, blogs, or product descriptions based on user inputs.</li>
</ul></li>
<li><p><strong>Vectorstores (RAG)</strong></p>
<p>Vectorstores are used in Retrieval-Augmented Generation (RAG)
applications, where relevant information is retrieved from a database of
embeddings (vectorized representations of text) to enhance the LLM's
responses. This allows the LLM to work with domain-specific or
proprietary knowledge not contained in its training data.</p>
<p>Real-world examples:</p>
<ul>
<li>Chatbots for Enterprises: A customer support chatbot retrieves
relevant product documentation or FAQs stored in a vectorstore to
provide accurate responses.</li>
<li>Search-Augmented Systems: Google Bard integrates real-time
information retrieval to provide up-to-date and contextually relevant
responses.</li>
</ul></li>
<li><p><strong>Agents</strong></p>
<p>Agents are LLM-driven systems that execute tasks autonomously or
semi-autonomously based on input instructions. They can make decisions,
interact with APIs, and manage workflows. Agents often use reasoning
frameworks like ReAct (Reasoning and Acting) to decide what steps to
take next.</p>
<p>Real-world examples:</p>
<ul>
<li>Zapier AI Assistant: Automates workflows by taking instructions,
analyzing data, and executing API calls or actions across
platforms.</li>
<li>LangChain Agents: Used for multi-step tasks such as filling out
forms, managing databases, or performing calculations.</li>
</ul></li>
<li><p><strong>Agents + Vectorstores</strong></p>
<p>This combines the reasoning and decision-making capabilities of
agents with the data retrieval abilities of vectorstores. These systems
can autonomously fetch relevant knowledge from vectorstores and execute
tasks, enabling advanced applications like AutoGPT. The integration
provides both reasoning depth and domain-specific accuracy.</p>
<p>Real-world examples:</p>
<ul>
<li><strong>AutoGPT:</strong> An open-source agent that can generate
business plans by researching topics, retrieving relevant information,
and autonomously completing subtasks.</li>
<li><strong>GPT Engineer:</strong> Helps developers by retrieving
relevant programming resources and autonomously generating code,
debugging, or improving software projects.</li>
</ul></li>
</ol>
<h2 id="chaining-a-simple-prompt">Chaining a Simple Prompt</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain.prompts.prompt <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_ollama <span class="keyword">import</span> ChatOllama</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define information to be incorporated to the prompt template</span></span><br><span class="line">information = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Elon Reeve Musk (/ˈiːlɒn/; EE-lon; born June 28, 1971) is a businessman and investor. </span></span><br><span class="line"><span class="string">    He is the founder, chairman, CEO, and CTO of SpaceX; angel investor, CEO, product architect and former chairman of Tesla, Inc.; owner, chairman and CTO of X Corp.; founder of the Boring Company and xAI; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. </span></span><br><span class="line"><span class="string">    He is the wealthiest person in the world, with an estimated net worth of US$232 billion as of December 2023, according to the Bloomberg Billionaires Index, and $254 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.</span></span><br><span class="line"><span class="string">    A member of the wealthy South African Musk family, Elon was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. </span></span><br><span class="line"><span class="string">    Two years later, he matriculated at Queen&#x27;s University at Kingston in Canada. Musk later transferred to the University of Pennsylvania, and received bachelor&#x27;s degrees in economics and physics. </span></span><br><span class="line"><span class="string">    He moved to California in 1995 to attend Stanford University. However, Musk dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. </span></span><br><span class="line"><span class="string">    The startup was acquired by Compaq for $307 million in 1999, and, that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal.</span></span><br><span class="line"><span class="string">    In October 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. </span></span><br><span class="line"><span class="string">    In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. </span></span><br><span class="line"><span class="string">    In 2006, Musk helped create SolarCity, a solar-energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. </span></span><br><span class="line"><span class="string">    In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. </span></span><br><span class="line"><span class="string">    The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. </span></span><br><span class="line"><span class="string">    In 2022, he acquired Twitter for $44 billion. He subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. </span></span><br><span class="line"><span class="string">    In March 2023, he founded xAI, an artificial intelligence company.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create a prompt template</span></span><br><span class="line">template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Given the information &#123;information&#125; about a person, please create:</span></span><br><span class="line"><span class="string">1. A short summary</span></span><br><span class="line"><span class="string">2. Two interesting facts about the person.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># incorporate information into prompt</span></span><br><span class="line">summary_prompt_template = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;information&quot;</span>], template=template</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create an llm</span></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model_name=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line"><span class="comment"># llm = ChatOllama(model=&quot;llama3&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create a chain</span></span><br><span class="line">chain = summary_prompt_template | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># prompt the model</span></span><br><span class="line">response = chain.invoke(<span class="built_in">input</span>=&#123;<span class="string">&quot;information&quot;</span>: information&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="parsing-the-output-with-a-customized-format">Parsing the Output
with a Customized Format</h2>
<p>Using <code>PydanticOutputParser</code> and user defined output data
structure.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.output_parsers <span class="keyword">import</span> PydanticOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.pydantic_v1 <span class="keyword">import</span> BaseModel, Field, validator</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model_name=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define your desired data structure</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Joke</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    setup: <span class="built_in">str</span> = Field(description=<span class="string">&quot;question to set up a joke&quot;</span>)</span><br><span class="line">    punchline: <span class="built_in">str</span> = Field(description=<span class="string">&quot;answer to resolve the joke&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add custom validation logic easily with Pydantic</span></span><br><span class="line"><span class="meta">    @validator(<span class="params"><span class="string">&quot;setup&quot;</span></span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">question_ends_with_question_mark</span>(<span class="params">cls, field</span>):</span><br><span class="line">        <span class="keyword">if</span> field[-<span class="number">1</span>] != <span class="string">&quot;?&quot;</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Badly formed question!&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># set up a parser + inject instructions into the prompt template.</span></span><br><span class="line">parser = PydanticOutputParser(pydantic_object=Joke)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a prompt with query and instruction</span></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    template=<span class="string">&quot;Answer the user query.\n&#123;format_instructions&#125;\n&#123;query&#125;\n&quot;</span>,</span><br><span class="line">    input_variables=[<span class="string">&quot;query&quot;</span>],</span><br><span class="line">    partial_variables=&#123;<span class="string">&quot;format_instructions&quot;</span>: parser.get_format_instructions()&#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#a query intended to prompt a language model to populate the data structure</span></span><br><span class="line">prompt_and_model = prompt | llm</span><br><span class="line">output = prompt_and_model.invoke(&#123;<span class="string">&quot;query&quot;</span>: <span class="string">&quot;Tell me a joke.&quot;</span>&#125;)</span><br><span class="line">parser.invoke(output)</span><br></pre></td></tr></table></figure>
<h2 id="data-ingestion-to-pinecone-vectorstore-rag">Data Ingestion to
Pinecone Vectorstore (RAG)</h2>
<p>Using <code>TextLoader</code>, <code>CharacterTextSplitter</code>,
<code>OpenAIEmbeddings</code>, and <a
target="_blank" rel="noopener" href="https://app.pinecone.io/">Pinecone</a> vector database.</p>
<p>Please refer to <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/#text-splitters">LangChain
text splitter techniques</a> ;<a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/character_text_splitter/">text
Split by character</a>; <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/embed_text/">text
embedding models</a> for more details.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_pinecone <span class="keyword">import</span> PineconeVectorStore</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;doc1.txt&quot;</span>)</span><br><span class="line">document = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># split data</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">texts = text_splitter.split_documents(document)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create embedding</span></span><br><span class="line">embeddings = OpenAIEmbeddings(openai_api_key=os.environ.get(<span class="string">&quot;OPENAI_API_KEY&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ingest data to vector db</span></span><br><span class="line">PineconeVectorStore.from_documents(texts, embeddings, index_name=os.environ[<span class="string">&#x27;INDEX_NAME&#x27;</span>])<span class="built_in">print</span>(<span class="string">&quot;finish&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="data-retrieval-from-pinecone-vectorestore-rag">Data Retrieval
from Pinecone Vectorestore (RAG)</h2>
<p><code>langchain-ai/retrieval-qa-chat</code> is a <a
target="_blank" rel="noopener" href="https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat?organizationId=9f3e510d-ce6b-4f98-98a3-ff383fda2d96">ChatPromptTemplate</a>
ensuring answers are based solely on the context.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build user query prompt</span></span><br><span class="line">query = <span class="string">&quot;what is Pinecone in machine learning?&quot;</span></span><br><span class="line">chain = PromptTemplate.from_template(template=query) | llm</span><br><span class="line"></span><br><span class="line"><span class="comment"># store query prompt to vector db</span></span><br><span class="line">vectorstore = PineconeVectorStore(</span><br><span class="line">    index_name=os.environ[<span class="string">&quot;INDEX_NAME&quot;</span>], embedding=embeddings</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a retrieval qa prompt</span></span><br><span class="line">retrieval_qa_chat_prompt = hub.pull(<span class="string">&quot;langchain-ai/retrieval-qa-chat&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a prompt chain</span></span><br><span class="line">combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create retrieval chain</span></span><br><span class="line">retrival_chain = create_retrieval_chain(</span><br><span class="line">    retriever=vectorstore.as_retriever(), combine_docs_chain=combine_docs_chain</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute the retrieval</span></span><br><span class="line">result = retrival_chain.invoke(<span class="built_in">input</span>=&#123;<span class="string">&quot;input&quot;</span>: query&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Customized retrieval prompt:</p>
<p><code>RunnablePassthrough</code> is used to pass through arguments <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/passthrough/">from one
step to the next</a>. It allows us to pass on the user's question to the
prompt and model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_pinecone <span class="keyword">import</span> PineconeVectorStore</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_docs</span>(<span class="params">docs</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;\n\n&quot;</span>.join(doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs)</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line">query = <span class="string">&quot;what is Pinecone in machine learning?&quot;</span></span><br><span class="line">chain = PromptTemplate.from_template(template=query) | llm</span><br><span class="line"></span><br><span class="line">vectorstore = PineconeVectorStore(</span><br><span class="line">    index_name=os.environ[<span class="string">&quot;INDEX_NAME&quot;</span>], embedding=embeddings</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Use the following pieces of context to answer the question at the end. </span></span><br><span class="line"><span class="string">If you don&#x27;t know the answer, you can say &quot;I don&#x27;t know&quot;. Don&#x27;t make up an answer.</span></span><br><span class="line"><span class="string">Use three sentences maximum and keep the answer short and to the point.</span></span><br><span class="line"><span class="string">Always say &quot;thanks for the question&quot; before answering the question.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;context&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: &#123;question&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Answer:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">custom_rag_prompt = PromptTemplate.from_template(template=template)</span><br><span class="line">rag_chain = (</span><br><span class="line">    &#123;<span class="string">&quot;context&quot;</span>: vectorstore.as_retriever() | format_docs, <span class="string">&quot;question&quot;</span>: RunnablePassthrough()&#125;</span><br><span class="line">    | custom_rag_prompt</span><br><span class="line">    | llm</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">res = rag_chain.invoke(query)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="chat-with-a-pdf-rag-with-faiss">Chat with a PDF (RAG with
FAISS)</h2>
<p>Using <code>PyPDFLoader</code>, <code>CharacterTextSplitter</code>,
<code>OpenAIEmbeddings</code>, and <code>FAISS</code> local vector
database.</p>
<p>Please refer to <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/document_loader_pdf/">PDF
loader</a> ; <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/vectorstores/faiss/">Langchain
FAISS vectorstore</a>; <a target="_blank" rel="noopener" href="https://faiss.ai/">FAISS</a> for more
details.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> PyPDFLoader</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.chains.retrieval <span class="keyword">import</span> create_retrieval_chain</span><br><span class="line"><span class="keyword">from</span> langchain.chains.combine_documents <span class="keyword">import</span> create_stuff_documents_chain</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"></span><br><span class="line">pdf_path = <span class="string">&quot;react.pdf&quot;</span></span><br><span class="line">loader = PyPDFLoader(file_path=pdf_path)</span><br><span class="line">documents = loader.load()</span><br><span class="line">text_splitter = CharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">30</span>, separator=<span class="string">&quot;\n&quot;</span></span><br><span class="line">)</span><br><span class="line">docs = text_splitter.split_documents(documents=documents)</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">vectorstore = FAISS.from_documents(docs, embeddings)</span><br><span class="line">vectorstore.save_local(<span class="string">&quot;faiss_index_react&quot;</span>)</span><br><span class="line"></span><br><span class="line">new_vectorstore = FAISS.load_local(</span><br><span class="line">    <span class="string">&quot;faiss_index_react&quot;</span>, embeddings, allow_dangerous_deserialization=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">retrieval_qa_chat_prompt = hub.pull(<span class="string">&quot;langchain-ai/retrieval-qa-chat&quot;</span>)</span><br><span class="line">combine_docs_chain = create_stuff_documents_chain(</span><br><span class="line">    OpenAI(), retrieval_qa_chat_prompt</span><br><span class="line">)</span><br><span class="line">retrieval_chain = create_retrieval_chain(</span><br><span class="line">    new_vectorstore.as_retriever(), combine_docs_chain</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">res = retrieval_chain.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Give me the gist of ReAct in 3 sentences&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(res[<span class="string">&quot;answer&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="create-a-react-agent">Create a ReAct Agent</h2>
<p>Using <code>langchain-ai/react-agent-template</code> to build a <a
target="_blank" rel="noopener" href="https://smith.langchain.com/hub/langchain-ai/react-agent-template?organizationId=9f3e510d-ce6b-4f98-98a3-ff383fda2d96">ReAct
prompt</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> create_react_agent, AgentExecutor</span><br><span class="line"><span class="keyword">from</span> langchain_experimental.tools <span class="keyword">import</span> PythonREPLTool</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create an instruction</span></span><br><span class="line">instructions = <span class="string">&quot;&quot;&quot;You are an agent designed to write and execute python code to answer questions.</span></span><br><span class="line"><span class="string">You have access to a python REPL, which you can use to execute python code.</span></span><br><span class="line"><span class="string">If you get an error, debug your code and try again.</span></span><br><span class="line"><span class="string">Only use the output of your code to answer the question. </span></span><br><span class="line"><span class="string">You might know the answer without running any code, but you should still run the code to get the answer.</span></span><br><span class="line"><span class="string">If it does not seem like you can write code to answer the question, just return &quot;I don&#x27;t know&quot; as the answer.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use an ReAct prompt template</span></span><br><span class="line">base_prompt = hub.pull(<span class="string">&quot;langchain-ai/react-agent-template&quot;</span>)</span><br><span class="line">prompt = base_prompt.partial(instructions=instructions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make use a tool to execute python code</span></span><br><span class="line">tools = [PythonREPLTool()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a ReAct agent</span></span><br><span class="line">agent = create_react_agent(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o-mini&quot;</span>),</span><br><span class="line">    tools=tools,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create the ReAct agent executor</span></span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute the ReAct agent</span></span><br><span class="line">agent_executor.invoke(</span><br><span class="line">    <span class="built_in">input</span>=&#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;&quot;&quot;generate and save in current working directory a QRcode</span></span><br><span class="line"><span class="string">                    that point to https://jokerdii.github.io/di-blog, you have qrcode package installed already&quot;&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">agent_executor.invoke(</span><br><span class="line">    <span class="built_in">input</span>=&#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;&quot;&quot;generate and save in current working directory a synthetic csv dataset </span></span><br><span class="line"><span class="string">                    with 1000 rows and 2 columns that is about Amazon product description and price.&quot;&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="using-an-langchain-agent-for-tasks">Using an LangChain Agent for
Tasks</h2>
<p><code>create_csv_agent</code> is an <code>AgentExecutor</code> object
able to perform operations in CSVs.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_experimental.agents.agent_toolkits <span class="keyword">import</span> create_csv_agent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># make use a CSV agent from langchain</span></span><br><span class="line">csv_agent = create_csv_agent(</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o-mini&quot;</span>),</span><br><span class="line">    path=<span class="string">&quot;episode_info.csv&quot;</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute the agent</span></span><br><span class="line">csv_agent.invoke(</span><br><span class="line">    <span class="built_in">input</span>=&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;how many columns are there in file episode_info.csv&quot;</span>&#125;</span><br><span class="line">)</span><br><span class="line">csv_agent.invoke(</span><br><span class="line">    <span class="built_in">input</span>=&#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;print the seasons by ascending order of the number of episodes they have.&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2
id="creating-an-react-agent-with-multiple-agents-provided-as-tools">Creating
an ReAct Agent with Multiple Agents Provided as Tools</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> (</span><br><span class="line">    create_react_agent,</span><br><span class="line">    AgentExecutor,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> langchain_experimental.tools <span class="keyword">import</span> PythonREPLTool</span><br><span class="line"><span class="keyword">from</span> langchain_experimental.agents.agent_toolkits <span class="keyword">import</span> create_csv_agent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">instructions = <span class="string">&quot;&quot;&quot;You are an agent designed to write and execute python code to answer questions.</span></span><br><span class="line"><span class="string">You have access to a python REPL, which you can use to execute python code.</span></span><br><span class="line"><span class="string">You have qrcode package installed</span></span><br><span class="line"><span class="string">If you get an error, debug your code and try again.</span></span><br><span class="line"><span class="string">Only use the output of your code to answer the question. </span></span><br><span class="line"><span class="string">You might know the answer without running any code, but you should still run the code to get the answer.</span></span><br><span class="line"><span class="string">If it does not seem like you can write code to answer the question, just return &quot;I don&#x27;t know&quot; as the answer.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">base_prompt = hub.pull(<span class="string">&quot;langchain-ai/react-agent-template&quot;</span>)</span><br><span class="line">prompt = base_prompt.partial(instructions=instructions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define python agent</span></span><br><span class="line">tools = [PythonREPLTool()]</span><br><span class="line">python_agent = create_react_agent(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4-turbo&quot;</span>),</span><br><span class="line">    tools=tools,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">python_agent_executor = AgentExecutor(agent=python_agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define CSV agent</span></span><br><span class="line">csv_agent_executor: AgentExecutor = create_csv_agent(</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>),</span><br><span class="line">    path=<span class="string">&quot;episode_info.csv&quot;</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### router grand agent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># list agent tools</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">python_agent_executor_wrapper</span>(<span class="params">original_prompt: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">    <span class="keyword">return</span> python_agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: original_prompt&#125;)</span><br><span class="line"></span><br><span class="line">tools = [</span><br><span class="line">    Tool(</span><br><span class="line">        name=<span class="string">&quot;Python Agent&quot;</span>,</span><br><span class="line">        func=python_agent_executor_wrapper,</span><br><span class="line">        description=<span class="string">&quot;&quot;&quot;useful when you need to transform natural language to python and execute the python code,</span></span><br><span class="line"><span class="string">                        returning the results of the code execution</span></span><br><span class="line"><span class="string">                        DOES NOT ACCEPT CODE AS INPUT&quot;&quot;&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">    Tool(</span><br><span class="line">        name=<span class="string">&quot;CSV Agent&quot;</span>,</span><br><span class="line">        func=csv_agent_executor.invoke,</span><br><span class="line">        description=<span class="string">&quot;&quot;&quot;useful when you need to answer question over episode_info.csv file,</span></span><br><span class="line"><span class="string">                        takes an input the entire question and returns the answer after running pandas calculations&quot;&quot;&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># create grand ReAct agent</span></span><br><span class="line">prompt = base_prompt.partial(instructions=<span class="string">&quot;&quot;</span>)</span><br><span class="line">grand_agent = create_react_agent(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    llm=ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4-turbo&quot;</span>),</span><br><span class="line">    tools=tools,</span><br><span class="line">)</span><br><span class="line">grand_agent_executor = AgentExecutor(agent=grand_agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute grand ReAct agent and print output</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    grand_agent_executor.invoke(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;which season has the most episodes?&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    grand_agent_executor.invoke(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;Generate and save in current working directory 15 qrcodes that point to `www.udemy.com/course/langchain`&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="function-tool-calling">Function / Tool Calling</h2>
<p>LangChain provides a standardized interface for connecting tools to
models.</p>
<ul>
<li><code>ChatModel.bind_tools()</code>: a method for attaching tool
definitions to model calls.</li>
<li><code>AIMessage.tool_calls</code>: an attribute on the
<code>AIMessage</code> returned from the model for easily accessing the
tool aclls the model decided to make.</li>
<li><code>create_tool_calling_agent</code>: an agent constsructor that
works with ANY model that implements <code>bind_tools</code> and returns
<code>tool_calls</code>.</li>
</ul>
<p>Directly using <code>PythonREPLTool</code> which is already a tool
object. Use with caution because <a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/tools/python/">Python
REPL</a> can execute arbitrary code on the host machine (e.g., delete
files, make network requests).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> tool</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> create_tool_calling_agent, AgentExecutor</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilySearchResults</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">x: <span class="built_in">float</span>, y: <span class="built_in">float</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply &#x27;x&#x27; times &#x27;y&#x27;.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> x * y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;you&#x27;re a helpful assistant&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;placeholder&quot;</span>, <span class="string">&quot;&#123;agent_scratchpad&#125;&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tools = [TavilySearchResults(), multiply]</span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>)</span><br><span class="line"></span><br><span class="line">agent = create_tool_calling_agent(llm, tools, prompt)</span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools)</span><br><span class="line"></span><br><span class="line">response = agent_executor.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;what is the weather in dubai right now? compare it with San Fransisco, output should in in celsious&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="tool-calling-with-langchain">Tool Calling with Langchain</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply a and b.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        a: first int</span></span><br><span class="line"><span class="string">        b: second int</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line">llm_with_tools = tool_calling_model.bind_tools([multiply])</span><br><span class="line"></span><br><span class="line">result = llm_with_tools.invoke(<span class="string">&quot;What is 2 multiplied by 3?&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="token-limitation-handling-strategies">Token Limitation Handling
Strategies</h2>
<p>when passing documents into the LLM context window, there are three
approaches for handling context window limitations:</p>
<ol type="1">
<li><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/tutorials/summarization/">Stuffing</a>:
suff all documents into a single prompt</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.combine_documents.stuff <span class="keyword">import</span> StuffDocumentsChain</span><br><span class="line"><span class="keyword">from</span> langchain.chains.llm <span class="keyword">import</span> LLMChain</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># define prompt</span></span><br><span class="line">prompt_template = <span class="string">&quot;&quot;&quot;Write a concise summary of the following:</span></span><br><span class="line"><span class="string">&quot;&#123;text&#125;&quot;</span></span><br><span class="line"><span class="string">CONCISE SUMMARY:&quot;&quot;&quot;</span></span><br><span class="line">prompt = PromptTemplate.from_template(prompt_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define LLM chain</span></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model_name=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>)</span><br><span class="line">llm_chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define StuffDocumentsChain</span></span><br><span class="line">stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=<span class="string">&quot;text&quot;</span>)</span><br><span class="line">docs = loader.load()</span><br><span class="line"><span class="built_in">print</span>(stuff_chain.run(docs))</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/tutorials/summarization/">Map-reduce</a>:
summarize each document on its own in parallel and put them into a final
summary.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> MapReduceDocumentsChain, ReduceDocumentsChain</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line"><span class="comment"># Map</span></span><br><span class="line">map_template = <span class="string">&quot;&quot;&quot;The following is a set of documents</span></span><br><span class="line"><span class="string">&#123;docs&#125;</span></span><br><span class="line"><span class="string">Based on this list of docs, please identify the main themes </span></span><br><span class="line"><span class="string">Helpful Answer:&quot;&quot;&quot;</span></span><br><span class="line">map_prompt = PromptTemplate.from_template(map_template)</span><br><span class="line">map_chain = LLMChain(llm=llm, prompt=map_prompt)</span><br><span class="line"><span class="comment"># Reduce</span></span><br><span class="line">reduce_template = <span class="string">&quot;&quot;&quot;The following is set of summaries:</span></span><br><span class="line"><span class="string">&#123;docs&#125;</span></span><br><span class="line"><span class="string">Take these and distill it into a final, consolidated summary of the main themes. </span></span><br><span class="line"><span class="string">Helpful Answer:&quot;&quot;&quot;</span></span><br><span class="line">reduce_prompt = PromptTemplate.from_template(reduce_template)</span><br><span class="line">reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)</span><br><span class="line"><span class="comment"># Combine documents by mapping a chain over them, then combining results</span></span><br><span class="line">map_reduce_chain = MapReduceDocumentsChain(</span><br><span class="line">    llm_chain=map_chain,</span><br><span class="line">    reduce_documents_chain=reduce_documents_chain,</span><br><span class="line">    document_variable_name=<span class="string">&quot;docs&quot;</span>,</span><br><span class="line">    return_intermediate_steps=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line">text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">split_docs = text_splitter.split_documents(docs)</span><br><span class="line"><span class="built_in">print</span>(map_reduce_chain.run(split_docs))</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>Refine: The refine documents chain constructs a response by looping
over the input documents and iteratively updating its answer.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.summarize <span class="keyword">import</span> load_summarize_chain</span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">                  Please provide a summary of the following text.</span></span><br><span class="line"><span class="string">                  TEXT: &#123;text&#125;</span></span><br><span class="line"><span class="string">                  SUMMARY:</span></span><br><span class="line"><span class="string">                  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">question_prompt = PromptTemplate(</span><br><span class="line">    template=question_prompt_template, input_variables=[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">refine_prompt_template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              Write a concise summary of the following text delimited by triple backquotes.</span></span><br><span class="line"><span class="string">              Return your response in bullet points which covers the key points of the text.</span></span><br><span class="line"><span class="string">              ```&#123;text&#125;```</span></span><br><span class="line"><span class="string">              BULLET POINT SUMMARY:</span></span><br><span class="line"><span class="string">              &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">refine_template = PromptTemplate(</span><br><span class="line">    template=refine_prompt_template, input_variables=[<span class="string">&quot;text&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load refine chain</span></span><br><span class="line">chain = load_summarize_chain(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">&quot;refine&quot;</span>,</span><br><span class="line">    question_prompt=question_prompt,</span><br><span class="line">    refine_prompt=refine_prompt,</span><br><span class="line">    return_intermediate_steps=<span class="literal">True</span>,</span><br><span class="line">    input_key=<span class="string">&quot;input_documents&quot;</span>,</span><br><span class="line">    output_key=<span class="string">&quot;output_text&quot;</span>,</span><br><span class="line">)</span><br><span class="line">result = chain(&#123;<span class="string">&quot;input_documents&quot;</span>: split_docs&#125;, return_only_outputs=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="coreference-resolution">Coreference Resolution</h2>
<p><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/chatbots_memory/">Adding
memory to chatbots.</a></p>
<p>LangChain provides a way to build applications that have memory using
LangGraph's <a
target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/concepts/persistence/">persistence</a>.
You can <a
target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/persistence/">enable
persistence</a> in LangGraph applications by providing a
<code>checkpointer</code> when compiling the graph. Every iteration,
LangGraph takes the information and saves it in a DB (PostgreSQL, MySQL,
Redis, and MongoDB saver).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> START, MessagesState, StateGraph</span><br><span class="line"></span><br><span class="line">workflow = StateGraph(state_schema=MessagesState)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the function that calls the model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: MessagesState</span>):</span><br><span class="line">    system_prompt = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant. &quot;</span></span><br><span class="line">        <span class="string">&quot;Answer all questions to the best of your ability.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    messages = [SystemMessage(content=system_prompt)] + state[<span class="string">&quot;messages&quot;</span>]</span><br><span class="line">    response = model.invoke(messages)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: response&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the node and edge</span></span><br><span class="line">workflow.add_node(<span class="string">&quot;model&quot;</span>, call_model)</span><br><span class="line">workflow.add_edge(START, <span class="string">&quot;model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add simple in-memory checkpointer</span></span><br><span class="line">memory = MemorySaver()</span><br><span class="line">app = workflow.<span class="built_in">compile</span>(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<p>Langchain has three main strategies to manage state:</p>
<ol type="1">
<li>Simply stuffing previous messages into a chat model prompt.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> AIMessage, HumanMessage, SystemMessage</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        SystemMessage(</span><br><span class="line">            content=<span class="string">&quot;You are a helpful assistant. Answer all questions to the best of your ability.&quot;</span></span><br><span class="line">        ),</span><br><span class="line">        MessagesPlaceholder(variable_name=<span class="string">&quot;messages&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain = prompt | model</span><br><span class="line"></span><br><span class="line">ai_msg = chain.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: [</span><br><span class="line">            HumanMessage(</span><br><span class="line">                content=<span class="string">&quot;Translate from English to French: I love programming.&quot;</span></span><br><span class="line">            ),</span><br><span class="line">            AIMessage(content=<span class="string">&quot;J&#x27;adore la programmation.&quot;</span>),</span><br><span class="line">            HumanMessage(content=<span class="string">&quot;What did you just say?&quot;</span>),</span><br><span class="line">        ],</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(ai_msg.content)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>The above, but trimming old messages to reduce the amount of
distracting information the model has to deal with.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> trim_messages</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> START, MessagesState, StateGraph</span><br><span class="line"></span><br><span class="line"><span class="comment"># define trimmer</span></span><br><span class="line"><span class="comment"># count each message as 1 &quot;token&quot; (token_counter=len) and keep only the last two messages</span></span><br><span class="line">trimmer = trim_messages(strategy=<span class="string">&quot;last&quot;</span>, max_tokens=<span class="number">2</span>, token_counter=<span class="built_in">len</span>)</span><br><span class="line"></span><br><span class="line">workflow = StateGraph(state_schema=MessagesState)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the function that calls the model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: MessagesState</span>):</span><br><span class="line">    trimmed_messages = trimmer.invoke(state[<span class="string">&quot;messages&quot;</span>])</span><br><span class="line">    system_prompt = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant. &quot;</span></span><br><span class="line">        <span class="string">&quot;Answer all questions to the best of your ability.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    messages = [SystemMessage(content=system_prompt)] + trimmed_messages</span><br><span class="line">    response = model.invoke(messages)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: response&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the node and edge</span></span><br><span class="line">workflow.add_node(<span class="string">&quot;model&quot;</span>, call_model)</span><br><span class="line">workflow.add_edge(START, <span class="string">&quot;model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ddd simple in-memory checkpointer</span></span><br><span class="line">memory = MemorySaver()</span><br><span class="line">app = workflow.<span class="built_in">compile</span>(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>More complex modifications like synthesizing summaries for long
running conversations.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage, RemoveMessage</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> START, MessagesState, StateGraph</span><br><span class="line"></span><br><span class="line">workflow = StateGraph(state_schema=MessagesState)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function that calls the model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: MessagesState</span>):</span><br><span class="line">    system_prompt = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant. &quot;</span></span><br><span class="line">        <span class="string">&quot;Answer all questions to the best of your ability. &quot;</span></span><br><span class="line">        <span class="string">&quot;The provided chat history includes a summary of the earlier conversation.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    system_message = SystemMessage(content=system_prompt)</span><br><span class="line">    message_history = state[<span class="string">&quot;messages&quot;</span>][:-<span class="number">1</span>]  <span class="comment"># exclude the most recent user input</span></span><br><span class="line">    <span class="comment"># Summarize the messages if the chat history reaches a certain size</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(message_history) &gt;= <span class="number">4</span>:</span><br><span class="line">        last_human_message = state[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Invoke the model to generate conversation summary</span></span><br><span class="line">        summary_prompt = (</span><br><span class="line">            <span class="string">&quot;Distill the above chat messages into a single summary message. &quot;</span></span><br><span class="line">            <span class="string">&quot;Include as many specific details as you can.&quot;</span></span><br><span class="line">        )</span><br><span class="line">        summary_message = model.invoke(</span><br><span class="line">            message_history + [HumanMessage(content=summary_prompt)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Delete messages that we no longer want to show up</span></span><br><span class="line">        delete_messages = [RemoveMessage(<span class="built_in">id</span>=m.<span class="built_in">id</span>) <span class="keyword">for</span> m <span class="keyword">in</span> state[<span class="string">&quot;messages&quot;</span>]]</span><br><span class="line">        <span class="comment"># Re-add user message</span></span><br><span class="line">        human_message = HumanMessage(content=last_human_message.content)</span><br><span class="line">        <span class="comment"># Call the model with summary &amp; response</span></span><br><span class="line">        response = model.invoke([system_message, summary_message, human_message])</span><br><span class="line">        message_updates = [summary_message, human_message, response] + delete_messages</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        message_updates = model.invoke([system_message] + state[<span class="string">&quot;messages&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: message_updates&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the node and edge</span></span><br><span class="line">workflow.add_node(<span class="string">&quot;model&quot;</span>, call_model)</span><br><span class="line">workflow.add_edge(START, <span class="string">&quot;model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add simple in-memory checkpointer</span></span><br><span class="line">memory = MemorySaver()</span><br><span class="line">app = workflow.<span class="built_in">compile</span>(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<h2 id="tracing-application-with-langsmith">Tracing Application with
LangSmith</h2>
<p><a target="_blank" rel="noopener" href="https://www.langchain.com/langsmith">LangSmith</a> traces
LLM calls, tool usage, LLM model latency, token count, and cost.</p>
<p>To integrate LangSmith to our application, we need to generate an API
key, add it as "LANGCHAIN_API_KEY" in environment variables, install
langsmith dependency, setup our environment. Please refer to <a
target="_blank" rel="noopener" href="https://smith.langchain.com/o/9f3e510d-ce6b-4f98-98a3-ff383fda2d96/?paginationState=%7B%22pageIndex%22%3A0%2C%22pageSize%22%3A5%7D">set
up tracing</a> for detailed steps.</p>
<h2 id="langchain-hub">LangChain Hub</h2>
<p><a
target="_blank" rel="noopener" href="https://smith.langchain.com/hub?organizationId=9f3e510d-ce6b-4f98-98a3-ff383fda2d96">LangChain
Hub</a> is a comprehensive platform that serves as a repository for
pre-built components, tools, and configurations designed to accelerate
the development of LLM applications. It simplifies the integration of
various building blocks—models, prompts, chains, and agents—enabling
developers to create robust and scalable applications without starting
from scratch.</p>
<h2 id="langchain-text-splitter-playground">LangChain Text Splitter
Playground</h2>
<p><a target="_blank" rel="noopener" href="https://langchain-text-splitter.streamlit.app/">Text
Splitter Playground</a> is a user-friendly interface designed to help
developers experiment with and fine-tune text-splitting strategies. In
many LLM applications, particularly those involving large documents or
retrieval-augmented generation (RAG), it is essential to divide text
into manageable chunks while preserving context. This tool allows users
to optimize the chunking process for their specific needs.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/di-blog/tags/knowledge/" rel="tag"># knowledge</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/di-blog/2024/12/01/2024-December/" rel="prev" title="2024 December - What I Have Read">
                  <i class="fa fa-angle-left"></i> 2024 December - What I Have Read
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/di-blog/2024/12/25/Understanding-LoRA/" rel="next" title="Understanding LoRA">
                  Understanding LoRA <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Di Zhen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/di-blog/js/comments.js"></script><script src="/di-blog/js/utils.js"></script><script src="/di-blog/js/motion.js"></script><script src="/di-blog/js/sidebar.js"></script><script src="/di-blog/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/di-blog/js/third-party/math/mathjax.js"></script>



</body>
</html>
