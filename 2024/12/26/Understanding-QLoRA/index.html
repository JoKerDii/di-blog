<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/di-blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/di-blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/di-blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/di-blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/di-blog/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jokerdii.github.io","root":"/di-blog/","images":"/di-blog/images","scheme":"Muse","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/di-blog/js/config.js"></script>

    <meta name="description" content="QLoRA is never as simple as a single line of code. Let&#39;s start from the scaling law... Neural Scaling Law In the context of LLMs, a scaling law refers to an empirical relationship that describes how t">
<meta property="og:type" content="article">
<meta property="og:title" content="Understanding QLoRA">
<meta property="og:url" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/index.html">
<meta property="og:site_name" content="Di&#39;s Blog">
<meta property="og:description" content="QLoRA is never as simple as a single line of code. Let&#39;s start from the scaling law... Neural Scaling Law In the context of LLMs, a scaling law refers to an empirical relationship that describes how t">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/nvidia_moorelaw.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/fp32_example.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/quant_granularity.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/finetune_cost.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/quantile_function.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/double_quant.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/qlora_forward.png">
<meta property="og:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/qlora_backward.png">
<meta property="article:published_time" content="2024-12-26T22:05:05.000Z">
<meta property="article:modified_time" content="2024-12-26T22:05:05.000Z">
<meta property="article:author" content="Di Zhen">
<meta property="article:tag" content="knowledge">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/nvidia_moorelaw.png">


<link rel="canonical" href="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/","path":"2024/12/26/Understanding-QLoRA/","title":"Understanding QLoRA"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Understanding QLoRA | Di's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/di-blog/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/di-blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Di's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-scaling-law"><span class="nav-number">1.</span> <span class="nav-text">Neural Scaling Law</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#quantization"><span class="nav-number">2.</span> <span class="nav-text">Quantization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#background"><span class="nav-number">2.1.</span> <span class="nav-text">Background</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#data-structure---fp32"><span class="nav-number">2.2.</span> <span class="nav-text">Data Structure - FP32</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#current-llm-training-method-in-fp8"><span class="nav-number">2.3.</span> <span class="nav-text">Current LLM Training Method
in FP8</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#quantization-process"><span class="nav-number">2.4.</span> <span class="nav-text">Quantization Process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#quant-in-general-matrix-multiply-gemm"><span class="nav-number">2.5.</span> <span class="nav-text">Quant in General Matrix
Multiply (GEMM)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qlora"><span class="nav-number">3.</span> <span class="nav-text">QLoRA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#fine-tuning-cost"><span class="nav-number">3.1.</span> <span class="nav-text">Fine-Tuning Cost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#qloras-contributions"><span class="nav-number">3.2.</span> <span class="nav-text">QLoRA&#39;s Contributions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bit-normalfloat-quantitzation"><span class="nav-number">3.2.1.</span> <span class="nav-text">4-bit NormalFloat
Quantitzation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#double-quantization"><span class="nav-number">3.2.2.</span> <span class="nav-text">Double Quantization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#paged-optimizers"><span class="nav-number">3.2.3.</span> <span class="nav-text">Paged Optimizers</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#forward-and-backward-implementation"><span class="nav-number">3.3.</span> <span class="nav-text">Forward and Backward
Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#qlora-usage"><span class="nav-number">3.4.</span> <span class="nav-text">QLora Usage</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Di Zhen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/di-blog/archives/">
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/di-blog/2024/12/26/Understanding-QLoRA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/di-blog/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Understanding QLoRA | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Understanding QLoRA
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-26 17:05:05" itemprop="dateCreated datePublished" datetime="2024-12-26T17:05:05-05:00">2024-12-26</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>QLoRA is never as simple as a single line of code. Let's start from
the scaling law...</p>
<h2 id="neural-scaling-law">Neural Scaling Law</h2>
<p>In the context of LLMs, a <strong>scaling law</strong> refers to an
empirical relationship that describes how the performance of a model
changes as key resources—such as model size (number of parameters),
dataset size, and computational power—are scaled up or down. These laws
provide insights into how increasing these factors impacts model
accuracy, efficiency, and generalization capabilities.</p>
<figure>
<img
src="/di-blog/2024/12/26/Understanding-QLoRA/scalinglaw_3charts.png"
alt="scalinglaw_3charts" />
<figcaption aria-hidden="true">scalinglaw_3charts</figcaption>
</figure>
<p>Compute, dataset size, and model size are not independent of each
other. Data size and model size together determine compute. The paper
"<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.05812v1">Algorithmic progress in
language models</a>" came up with a rule <span
class="math inline">\(C=6ND\)</span> where <span
class="math inline">\(C\)</span> is compute, <span
class="math inline">\(N\)</span> is model size, and <span
class="math inline">\(D\)</span> is data size.</p>
<p>According to the paper "<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language
Models</a>" and "<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15556">Training
Compute-Optimal Large Language Models</a>", below are the key takeaways
of scaling laws for LLMs:</p>
<ul>
<li><strong>Performance Improvement</strong>: Model performance often
improves predictably with increases in size, data, and compute,
typically following a power-law relationship.</li>
<li><strong>Diminishing Returns</strong>: Beyond certain thresholds, the
benefits of scaling diminish, meaning further increases in resources
yield smaller performance gains.</li>
<li><strong>Trade-offs</strong>: Effective scaling requires balancing
resources like model parameters and training data. For example, the
"Chinchilla scaling law" highlights that increasing data size can
sometimes yield better results than merely increasing model size in
compute-constrained settings.</li>
</ul>
<p>These observations are critical for LLM research:</p>
<ul>
<li><p><strong>Guidance for Optimization</strong>: Scaling laws help
researchers allocate resources efficiently and predict the outcomes of
scaling efforts, guiding both model design and training strategies. For
example, within fixed computational constraints and limited training
duration, scaling laws provide a principled approach to determining the
optimal model size that minimizes test loss.</p></li>
<li><p><strong>Predicting model performance</strong>: As demonstrated in
<a target="_blank" rel="noopener" href="https://cdn.openai.com/papers/gpt-4.pdf">GPT-4 Technical
Report</a>, by fitting the scaling law to the loss of smaller models,
the loss of a bigger model can be predicted accurately.</p>
<figure>
<img
src="/di-blog/2024/12/26/Understanding-QLoRA/gpt4_report_scalinglaw.png"
alt="gpt4_report_scalinglaw" />
<figcaption aria-hidden="true">gpt4_report_scalinglaw</figcaption>
</figure></li>
</ul>
<p>The scaling law overlooks a critical practical consideration, which
can lead to misconceptions. While it suggests that larger models yield
better performance, <strong>in reality, the primary compute bottleneck
lies in inference rather than</strong> training. Training compute is
often less constrained because training time can be extended, but
deployment costs are significantly higher. From a practical standpoint,
a more efficient approach is to train a smaller model for an extended
period, as this substantially reduces inference compute
requirements.</p>
<h2 id="quantization">Quantization</h2>
<h3 id="background">Background</h3>
<p>As the scaling law suggested, when training a LLM, reducing the
number of parameters is probably not an optimal idea for saving
computational resource. Luckily, <strong>neural nets are robust in low
precision</strong>, which means lowering precision won't reduce much
model performance.</p>
<p>In <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Y2F8yisiS6E&amp;t=2604s&amp;ab_channel=NVIDIA">GTC
March 2024 Keynote with NVIDIA CEO Jensen Huang</a>, Jensen stated that
NVIDIA has achieved 1000X increase compute power for the past 8 years,
faster than Moore’s law. It can be noticed that in the graph they show
TFLOPs on FP8 precision in 2022 and TFLOPs on FP4 precision in 2024.
This is a trick because it's easier to achieve higher TFLOPs when the
precision is lower. And it shows that there is a trend in hardware
industry to achieve higher TFLOPs in low precision.</p>
<figure>
<img src="/di-blog/2024/12/26/Understanding-QLoRA/nvidia_moorelaw.png"
alt="nvidia_moorelaw" />
<figcaption aria-hidden="true">nvidia_moorelaw</figcaption>
</figure>
<h3 id="data-structure---fp32">Data Structure - FP32</h3>
<p>The <strong>IEEE 754 single-precision floating-point format
(FP32)</strong> represents a 32-bit number in binary form. It is used
for approximating real numbers. The FP32 format has three
components:</p>
<ol type="1">
<li><strong>Sign bit (1 bit):</strong> Indicates whether the number is
positive (<code>0</code>) or negative (<code>1</code>).</li>
<li><strong>Exponent (8 bits):</strong> Encodes the exponent, biased by
127 to allow both positive and negative exponents.</li>
<li><strong>Mantissa or Fraction (23 bits):</strong> Represents the
significant digits of the number.</li>
</ol>
<p>The formula for FP32 is as follows: <span class="math display">\[
\text{Value} = (-1)^{\text{Sign}} \times (1.\text{Mantissa}) \times
2^{\text{Exponent}-127}
\]</span> Following this formula, we can calculate FP32 number. Below is
an example:</p>
<figure>
<img src="/di-blog/2024/12/26/Understanding-QLoRA/fp32_example.png"
alt="fp32_example" />
<figcaption aria-hidden="true">fp32_example</figcaption>
</figure>
<p>FP32 provides a wider range and higher precision, making it suitable
for tasks requiring numerical accuracy, such as training large-scale
deep learning models. FP16, with its lower precision, is optimized for
speed and memory efficiency. It is particularly effective for inference
tasks or mixed-precision training when paired with FP32 for critical
calculations.</p>
<p>However, the <strong>overflow problem of FP16</strong> arises due to
its limited range of representable values. FP16 has a maximum
representable value of 65,504 (<span class="math inline">\(2^{15} \times
(2 - \epsilon)\)</span>), which is much smaller compared to FP32's
maximum value of approximately <span class="math inline">\(3.4 \times
10^{38}\)</span>. When computations produce results exceeding this
range, an <strong>overflow</strong> occurs, and the value is replaced by
infinity (<span class="math inline">\(\pm \infty\)</span>). Overflow in
FP16 can occur during operations like matrix multiplications or
summations in deep learning if the intermediate values exceed the
maximum representable range. For example, scaling large tensors or
performing high-magnitude computations without normalization can easily
result in overflow when using FP16. Overflow leads to loss of numerical
accuracy and can destabilize training processes in machine learning. It
also affects applications like image processing or scientific
simulations where precision and stability are critical.</p>
<p>There are some strategies to mitigate this overflow problem:</p>
<ul>
<li>Use <strong>mixed-precision training</strong>. FP16 is used for most
computations but critical operations (e.g., gradient accumulation) are
performed in FP32 to prevent overflow.</li>
<li>Normalize inputs and intermediate values to keep them within the
representable range of FP16.</li>
<li>Use alternative formats like <strong>BF16</strong>, which have a
larger dynamic range while maintaining reduced precision.</li>
</ul>
<p><strong>Googel Brain BF16</strong> uses the same number of exponent
bits as FP32 (8 bits), giving it a much larger dynamic range compared to
FP16. This means BF16 can represent very large and very small numbers
similar to FP32, avoiding underflows and overflows that are common in
FP16. Converting from FP32 to BF16 is straightforward because both
formats share the same exponent size. The conversion simply involves
truncating the mantissa from 23 bits to 7 bits. BF16 uses only 16 bits
per value, reducing memory usage by half compared to FP32. This allows
for larger batch sizes and models to fit into limited GPU or TPU memory
without sacrificing as much numerical range as FP16 does.</p>
<p>Recently, people have started discussing <strong>NVIDIA’s FP8 formats
(E4M3 and E5M2)</strong> as alternatives to BF16 because of their
potential to significantly reduce computational and memory costs while
maintaining competitive performance in large-scale machine learning
tasks. E4M3 offers higher precision, making it suitable for
<strong>inference</strong> and <strong>forward-pass
computations</strong> where precision is critical. E5M2 provides a wider
dynamic range, making it ideal for <strong>backward-pass
computations</strong> during training where large gradients can occur.
This flexibility allows FP8 to adapt to different stages of training
more effectively than BF16.</p>
<p>NVIDIA’s H100 GPUs are specifically designed to support FP8 with
optimized Tensor Cores, achieving up to 9x faster training and 30x
faster inference compared to previous-generation GPUs using FP16 or
BF16. The Hopper architecture dynamically manages precision transitions
(e.g., between FP8 and higher-precision formats like FP32), ensuring
stability without manual intervention. "<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.08719?t">Balancing Speed and Stability:
The Trade-offs of FP8 vs. BF16 Training in LLMs</a>" shows that FP8 can
deliver similar convergence behavior and accuracy as BF16 in many LLM
tasks, with minimal degradation in performance. For inference, FP8
quantization (e.g., <a
target="_blank" rel="noopener" href="https://docs.vllm.ai/en/v0.4.2/quantization/fp8_e4m3_kvcache.html">E4M3
for KV cache</a>) has been shown to minimally impact accuracy while
significantly improving memory efficiency.</p>
<p>However, FP8 comes with challenges such as occasional instability
during training (e.g., loss spikes) and sensitivity in certain tasks
like code generation or mathematical reasoning. As a result, training
LLMs with FP8 precision remains an active area of research and
exploration.</p>
<figure>
<img
src="/di-blog/2024/12/26/Understanding-QLoRA/summary_5_precision.png"
alt="summary_5_precision" />
<figcaption aria-hidden="true">summary_5_precision</figcaption>
</figure>
<table>
<colgroup>
<col style="width: 8%" />
<col style="width: 19%" />
<col style="width: 18%" />
<col style="width: 19%" />
<col style="width: 17%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr>
<th><strong>Feature</strong></th>
<th><strong>IEEE 754 FP32</strong></th>
<th><strong>IEEE 754 FP16</strong></th>
<th><strong>Google BF16</strong></th>
<th><strong>NVIDIA FP8 E4M3</strong></th>
<th><strong>NVIDIA FP8 E5M2</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bit Width</strong></td>
<td>32 bits</td>
<td>16 bits</td>
<td>16 bits</td>
<td>8 bits</td>
<td>8 bits</td>
</tr>
<tr>
<td><strong>Sign Bit</strong></td>
<td>1 bit</td>
<td>1 bit</td>
<td>1 bit</td>
<td>1 bit</td>
<td>1 bit</td>
</tr>
<tr>
<td><strong>Exponent Bits</strong></td>
<td>8 bits (bias = 127)</td>
<td>5 bits (bias = 15)</td>
<td>8 bits (bias = 127)</td>
<td>4 bits (bias = 7)</td>
<td>5 bits (bias = 15)</td>
</tr>
<tr>
<td><strong>Mantissa Bits</strong></td>
<td>23 bits</td>
<td>10 bits</td>
<td>7 bits</td>
<td>3 bits</td>
<td>2 bits</td>
</tr>
<tr>
<td><strong>Dynamic Range</strong></td>
<td><span class="math display">\[ \pm(2^{-126} \text{ to } 2^{127})
\]</span></td>
<td><span class="math display">\[ \pm(2^{-14} \text{ to } 2^{15})
\]</span></td>
<td><span class="math display">\[ \pm(2^{-126} \text{ to } 2^{127})
\]</span></td>
<td><span class="math display">\[ \pm(2^{-6} \text{ to } 2^{7})
\]</span></td>
<td><span class="math display">\[ \pm(2^{-14} \text{ to } 2^{15})
\]</span></td>
</tr>
<tr>
<td><strong>Precision</strong></td>
<td>~7 decimal digits</td>
<td>~3.3 decimal digits</td>
<td>~2.3 decimal digits</td>
<td>Lower precision</td>
<td>Lower precision</td>
</tr>
<tr>
<td><strong>Memory Usage</strong></td>
<td>High</td>
<td>Medium</td>
<td>Medium</td>
<td>Low</td>
<td>Low</td>
</tr>
<tr>
<td><strong>Performance</strong></td>
<td>Slower</td>
<td>Faster than FP32</td>
<td>Faster than FP32</td>
<td>Much faster than FP16/BF16</td>
<td>Much faster than FP16/BF16</td>
</tr>
<tr>
<td><strong>Applications</strong></td>
<td>Training requiring high precision</td>
<td>Inference or mixed-precision training</td>
<td>Mixed-precision training and inference</td>
<td>Optimized for inference</td>
<td>Optimized for training and inference</td>
</tr>
</tbody>
</table>
<h3 id="current-llm-training-method-in-fp8">Current LLM Training Method
in FP8</h3>
<p>A <a
target="_blank" rel="noopener" href="https://developer.nvidia.com/zh-cn/blog/nvidia-gpu-fp8-training-inference/">training
approach</a> has been developed to leverage FP8's efficiency for
specific operations while maintaining numerical stability and precision
with BF16 for critical components of the model.</p>
<p>During the training process, <strong>FP8</strong> is utilized
exclusively for computations within the MLP layers, while
<strong>BF16</strong> is employed for other components of the
Transformer architecture, such as Attention, Activation, and Layer
Normalization. Both weights and gradients are maintained in BF16
precision.</p>
<ul>
<li><p>In the forward pass, weights in BF16 are converted to FP8 (E4M3)
for matrix multiplications within the MLP layers. Once the computation
is completed, the results are immediately converted back to
BF16.</p></li>
<li><p>In the backward pass, gradients in BF16 are temporarily converted
to FP8 (E5M2) when passing through the MLP layers. After the
computations are performed, the results are promptly converted back to
BF16.</p></li>
</ul>
<figure>
<img
src="/di-blog/2024/12/26/Understanding-QLoRA/fp8_transformer_current.png"
alt="fp8_transformer_current" />
<figcaption aria-hidden="true">fp8_transformer_current</figcaption>
</figure>
<p>Even when FP8 is used, RAM savings may not be as significant during
training because high precision gradients and weights must be maintained
in memory to ensure model stability and convergence. The primary benefit
of FP8 lies in its ability to reduce memory usage during inference,
where weights can be stored in FP8 format, significantly decreasing the
memory footprint compared to higher precision formats like FP16 or BF16.
Despite this, FP8 is still utilized during training because it allows
for faster computations due to its lower precision. This results in
accelerated training processes and improved efficiency, especially on
hardware optimized for FP8 operations, such as NVIDIA’s H100 GPUs.</p>
<h3 id="quantization-process">Quantization Process</h3>
<p>The process of quantization in LLMs refers to a model compression
technique that maps high-precision values (e.g., FP32) to
lower-precision representations (e.g., INT8 or FP8).</p>
<p>Here is an example of a simple step-by-step quantization from FP16 to
INT4:</p>
<ol type="1">
<li><p><strong>Range Calculation</strong>: Determine the range of FP16
values for the weights or activations. This is typically defined by the
minimum and maximum values (<span class="math inline">\([min,
max]\)</span>) in the data.</p></li>
<li><p><strong>Scale Factor and Zero-Point Computation</strong>: Compute
a <strong>scaling factor (S)</strong> that maps the FP16 range to the
INT4 range (<span class="math inline">\([-8, 7]\)</span> for signed INT4
or <span class="math inline">\([0, 15]\)</span> for unsigned INT4).
Optionally, calculate a <strong>zero-point (Z)</strong> to handle
asymmetric quantization, where zero in FP16 does not align with zero in
INT4.</p>
<p>The formula for quantization is: <span class="math display">\[
x_q = \text{round}\left(\frac{x}{S} + Z\right)
\]</span> where <span class="math inline">\(x_q\)</span> is the
quantized INT4 value, <span class="math inline">\(x\)</span> is the
original FP16 value, <span class="math inline">\(S\)</span> is the
scaling factor, and <span class="math inline">\(Z\)</span> is the
zero-point.</p></li>
<li><p><strong>Quantization</strong>: Map each FP16 value to its
corresponding INT4 representation using the computed scale factor and
zero-point. This step reduces precision but compresses the data
significantly.</p></li>
</ol>
<p>There are different types of quantization:</p>
<ul>
<li>Asymmetric Quantization vs Summetric Quantization</li>
<li>Uniform Quantization vs Non-uniform Quantization</li>
</ul>
<h3 id="quant-in-general-matrix-multiply-gemm">Quant in General Matrix
Multiply (GEMM)</h3>
<p>Quantized matrices are stored in memory in their compressed form.
During matrix multiplication, these matrices are dequantized back to
higher precision (e.g., FP16 or FP32) to perform computations. This
process balances memory efficiency with computational precision.</p>
<p>Quantization can be applied at different levels of granularity, which
determines how scaling factors are assigned and used. The "<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.10438">SmoothQuant: Accurate and
Efficient Post-Training Quantization for Large Language Models</a>"
paper introduced several quantization granularity techniques, including
per-tensor quantization, per-token quantization, and per-channel
quantization:</p>
<ol type="1">
<li><strong>Per-Tensor Quantization</strong>: A single scaling factor is
applied to the entire tensor (e.g., a weight matrix or activation
matrix). It is highly memory-efficient since only one scaling factor
needs to be stored. But it is not recommended in practice because
outlier values can dominate the scaling factor, leading to significant
quantization errors for the rest of the tensor.</li>
<li><strong>Per-Channel Quantization</strong>: Each channel (e.g., each
column of a weight matrix or each feature map in activations) has its
own scaling factor. Commonly used for weight matrices in neural
networks. It mitigates the impact of outliers by isolating them within
individual channels, improving quantization accuracy compared to
per-tensor methods. But it can introduce computational overhead during
dequantization due to the need for multiple scaling factors.</li>
<li><strong>Per-Token Quantization</strong>: Each token's activations
are assigned a unique scaling factor. Typically used for activations in
transformer models. It captures token-specific variations in
activations, leading to better precision for tasks with dynamic token
distributions. Per-token quantization can be computationally expensive
and slower because it requires more scaling factors and additional
computations.</li>
<li><strong>Group-Wise Quantization (GWQ)</strong>: this method groups
multiple channels or tokens together and applies a shared scaling factor
across the group. It reduces computational overhead compared to
per-channel or per-token quantization while maintaining finer
granularity than per-tensor methods. It's often used for both weights
and activations to strike a balance between accuracy and
efficiency.</li>
</ol>
<figure>
<img src="/di-blog/2024/12/26/Understanding-QLoRA/quant_granularity.png"
alt="quant_granularity" />
<figcaption aria-hidden="true">quant_granularity</figcaption>
</figure>
<h2 id="qlora">QLoRA</h2>
<h3 id="fine-tuning-cost">Fine-Tuning Cost</h3>
<p>Comparing cost of full fine tuning, LoRA fine tuning, and QLoRA fine
tuning:</p>
<figure>
<img src="/di-blog/2024/12/26/Understanding-QLoRA/finetune_cost.png"
alt="finetune_cost" />
<figcaption aria-hidden="true">finetune_cost</figcaption>
</figure>
<table style="width:100%;">
<colgroup>
<col style="width: 18%" />
<col style="width: 25%" />
<col style="width: 28%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th></th>
<th>Full Finetuning</th>
<th>LoRA</th>
<th>QLoRA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Weight</td>
<td>16 bits</td>
<td>16 bits</td>
<td><strong>4 bits</strong></td>
</tr>
<tr>
<td>Weight Gradient</td>
<td>16 bits</td>
<td>~0.4 bits</td>
<td>~0.4 bits</td>
</tr>
<tr>
<td>Optimizer stage</td>
<td>64 bits</td>
<td>~0.8 bits</td>
<td>~0.8 bits</td>
</tr>
<tr>
<td>Adapter weights</td>
<td>/</td>
<td>~0.4 bits</td>
<td>~0.4 bits</td>
</tr>
<tr>
<td>Totel</td>
<td>96 bits per parameter</td>
<td>17.6 bits per parameter</td>
<td>5.2 bits per parameter</td>
</tr>
</tbody>
</table>
<h3 id="qloras-contributions">QLoRA's Contributions</h3>
<p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient
Finetuning of Quantized LLMs</a></p>
<h4 id="bit-normalfloat-quantitzation"><strong>4-bit NormalFloat
Quantitzation</strong></h4>
<p>4-bit NormalFloat Quantitzation adopts the idea of <strong>Quantile
Quantization</strong> which is an information-theoretic method that maps
values based on quantiles of the weight distribution. It's a method of
data compression where data is quantized (reduced to a smaller set of
discrete values) in a way that aims to minimize the information entropy
of the resulting data, essentially achieving the most efficient
representation possible while still introducing some loss in
information, making it a "<strong>lossy minimum entropy
encoding</strong>" technique.</p>
<p>To compute the <strong>quantile function</strong> for 4-bit
NormalFloat (NF4) quantization, the process involves mapping cumulative
probabilities to quantization levels optimized for normally distributed
data. The <strong>quantile function</strong> is the inverse of the
cumulative distribution function (CDF). For example, as shown in the
description, if the probability of <span class="math inline">\(x &lt;
1.2\)</span> is 0.9, then 1.2 is the corresponding quantile for a
cumulative probability of 0.9.</p>
<figure>
<img src="/di-blog/2024/12/26/Understanding-QLoRA/quantile_function.png"
alt="quantile_function" />
<figcaption aria-hidden="true">quantile_function</figcaption>
</figure>
<p>With this quantile function, the probability range from 0 to 1 is
divided into <strong>16 equal-sized buckets</strong>, as 4 bits can
represent <span class="math inline">\(2^4 = 16\)</span> distinct values.
The steps are as follows:</p>
<ol type="1">
<li><strong>Divide the Probability Range</strong>: The range of
cumulative probabilities <span class="math inline">\([0, 1]\)</span> is
divided into 16 equal intervals or "buckets." These intervals represent
equal portions of the probability mass.</li>
<li><strong>Apply the Quantile Function</strong>: For each bucket's
cumulative probability value (e.g., <span class="math inline">\(p_i =
\frac{i}{16}\)</span>, where <span class="math inline">\(i \in [1,
15]\)</span>), the corresponding quantile value is computed using the
inverse CDF of a standard normal distribution (<span
class="math inline">\(\Phi^{-1}(p_i)\)</span>).</li>
<li><strong>Normalize Quantiles</strong>: The resulting quantiles are
normalized to fit within a predefined range, typically <span
class="math inline">\([-1, 1]\)</span>. This ensures that all
quantization levels are symmetrically distributed around zero and fall
within a compact range suitable for efficient representation.</li>
<li><strong>Assign NF4 Values</strong>: The normalized quantiles become
the 16 discrete values used by NF4 to represent weights or activations
in a compressed format. These values are spaced closer together near
zero (where most of the normal distribution's probability mass lies) and
farther apart at the extremes, optimizing precision where it matters
most.</li>
</ol>
<h4 id="double-quantization">Double Quantization</h4>
<p>Double Quantization (DQ) as introduced in paper "<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of
Quantized LLMs</a>" is a memory optimization technique that quantizes
the quantization constants themselves to further reduce the memory
footprint of LLMs. It involves two quantization steps:</p>
<ol type="1">
<li><p>The first quantization involves quantizing the original weights
of the model into <strong>4-bit NormalFloat (NF4) format</strong>.
Weights are divided into small blocks (e.g., <strong>64</strong>
elements per block), and each block is scaled by a <strong>quantization
constant</strong> (also known as a scaling factor). This constant
ensures that the range of values in each block fits within the
representable range of NF4. The quantized weights and their
corresponding quantization constants are stored. However, these
constants (usually in <strong>FP32</strong>) can add significant memory
overhead.</p>
<p><strong>To calculate the memory overhead</strong>: for a block size
of 64, storing a 32 bit quantization constant for each block adds <span
class="math inline">\(32/64=0.5\)</span> bits per parameter on
average.</p></li>
<li><p>The second quantization aims to reduce the memory overhead caused
by storing quantization constants. Those quantization constants <span
class="math inline">\(c^{FP32}_2\)</span> are further quantized into
<strong>8-bit floating-point values (FP8)</strong> with a larger block
size (e.g., <strong>256</strong> elements per block). This is a
<strong>summetric quantization</strong> where the mean of the first
level factors <span class="math inline">\(c^{FP32}_2\)</span> is
subtracted to center their distribution around zero. This reduces their
memory footprint while maintaining sufficient precision for scaling
operations. Additionally, another set of quantization constants <span
class="math inline">\(c^{FP32}_1\)</span> is introduced to scale these
second-level quantized values.</p>
<p><strong>To calculate the memory savings</strong>: after double
quantization, the memory footprint per parameter for scaling factors is
reduced from <span class="math inline">\(32/64=0.5\)</span> bits to
<span class="math inline">\(8/64 + 32/(64\times 256)=0.127\)</span> bits
per parameter. This results in saving <span
class="math inline">\(0.5-0.127=0.373\)</span> bits per
parameter.</p></li>
</ol>
<figure>
<img src="/di-blog/2024/12/26/Understanding-QLoRA/double_quant.png"
alt="double_quant" />
<figcaption aria-hidden="true">double_quant</figcaption>
</figure>
<p>The authors of paper "<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of
Quantized LLMs</a>" compared LLaMA models with different 4-bit data
types. They show that the NormalFloat data type significantly improves
the bit-for-bit accuracy gains compared to regular 4-bit Floats. While
Double Quantization only leads to minor gains, it allows for a more
fine-grained control over the memory footprint to fit models of certain
size (33B/65B) into certain GPUs (24/48GB). This empirical results show
that using FP8 for second-level quantization does not degrade model
performance, making it an effective trade-off between precision and
memory efficiency.</p>
<figure>
<img
src="/di-blog/2024/12/26/Understanding-QLoRA/nf4_compare_result.png"
alt="nf4_compare_result" />
<figcaption aria-hidden="true">nf4_compare_result</figcaption>
</figure>
<h4 id="paged-optimizers">Paged Optimizers</h4>
<p>As described in the QLoRA paper, paged optimizers are a memory
management innovation that leverages <strong>NVIDIA Unified
Memory</strong> to handle the memory spikes that occur during gradient
checkpointing or when processing large mini-batches with long sequence
lengths. NVIDIA Unified Memory allows seamless memory sharing between
the GPU and CPU. When the GPU runs out of memory during training,
optimizer states (e.g., gradients, momentum, or scaling factors) are
<strong>paged out</strong> (evicted) to CPU RAM. These states are
<strong>paged back</strong> into GPU memory only when needed for
computations like gradient updates.</p>
<h3 id="forward-and-backward-implementation">Forward and Backward
Implementation</h3>
<p>Forward:</p>
<figure>
<img src="/di-blog/2024/12/26/Understanding-QLoRA/qlora_forward.png"
alt="qlora_forward" />
<figcaption aria-hidden="true">qlora_forward</figcaption>
</figure>
<p>Backward:</p>
<figure>
<img src="/di-blog/2024/12/26/Understanding-QLoRA/qlora_backward.png"
alt="qlora_backward" />
<figcaption aria-hidden="true">qlora_backward</figcaption>
</figure>
<h3 id="qlora-usage">QLora Usage</h3>
<p>QLoRA utilizes <a
target="_blank" rel="noopener" href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a> for
quantization and is seamlessly integrated with Hugging Face's <a
target="_blank" rel="noopener" href="https://github.com/huggingface/peft">PEFT</a> and <a
target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/">transformers</a>
libraries, making it user-friendly. To explore the implementation
further, let's dive into the <a
target="_blank" rel="noopener" href="https://github.com/artidoro/qlora/tree/main">QLoRA code</a> and
examine the <a
target="_blank" rel="noopener" href="https://github.com/artidoro/qlora/blob/main/qlora.py#L688"><code>train()</code></a>
function in <a
target="_blank" rel="noopener" href="https://github.com/artidoro/qlora/blob/main/qlora.py"><code>qlora.py</code></a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    hfparser = transformers.HfArgumentParser((</span><br><span class="line">        ModelArguments, DataArguments, TrainingArguments, GenerationArguments</span><br><span class="line">    ))</span><br><span class="line">    model_args, data_args, training_args, generation_args, extra_args = \</span><br><span class="line">        hfparser.parse_args_into_dataclasses(return_remaining_strings=<span class="literal">True</span>)</span><br><span class="line">    training_args.generation_config = transformers.GenerationConfig(**<span class="built_in">vars</span>(generation_args))</span><br><span class="line">    args = argparse.Namespace(</span><br><span class="line">        **<span class="built_in">vars</span>(model_args), **<span class="built_in">vars</span>(data_args), **<span class="built_in">vars</span>(training_args)</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line">    </span><br><span class="line">    checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)</span><br><span class="line">    <span class="keyword">if</span> completed_training:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Detected that training was already completed!&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    model, tokenizer = get_accelerate_model(args, checkpoint_dir)</span><br><span class="line"></span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>The <a
target="_blank" rel="noopener" href="https://github.com/artidoro/qlora/blob/main/qlora.py#L289"><code>get_accelerate_model()</code></a>
function initializes your model and is a crucial component of
implementing QLoRA. Notably, within the
<code>AutoModelForCausalLM.from_pretrained()</code> method, it loads the
quantization configuration through <code>BitsAndBytesConfig</code>. This
setup ensures that the model weights are automatically quantized.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_accelerate_model</span>(<span class="params">args, checkpoint_dir</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        n_gpus = torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">if</span> is_ipex_available() <span class="keyword">and</span> torch.xpu.is_available():</span><br><span class="line">        n_gpus = torch.xpu.device_count()</span><br><span class="line">        </span><br><span class="line">    max_memory = <span class="string">f&#x27;<span class="subst">&#123;args.max_memory_MB&#125;</span>MB&#x27;</span></span><br><span class="line">    max_memory = &#123;i: max_memory <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_gpus)&#125;</span><br><span class="line">    device_map = <span class="string">&quot;auto&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># if we are in a distributed setting, we need to set the device map and max memory per device</span></span><br><span class="line">    <span class="keyword">if</span> os.environ.get(<span class="string">&#x27;LOCAL_RANK&#x27;</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        local_rank = <span class="built_in">int</span>(os.environ.get(<span class="string">&#x27;LOCAL_RANK&#x27;</span>, <span class="string">&#x27;0&#x27;</span>))</span><br><span class="line">        device_map = &#123;<span class="string">&#x27;&#x27;</span>: local_rank&#125;</span><br><span class="line">        max_memory = &#123;<span class="string">&#x27;&#x27;</span>: max_memory[local_rank]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.full_finetune: <span class="keyword">assert</span> args.bits <span class="keyword">in</span> [<span class="number">16</span>, <span class="number">32</span>]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loading base model <span class="subst">&#123;args.model_name_or_path&#125;</span>...&#x27;</span>)</span><br><span class="line">    compute_dtype = (torch.float16 <span class="keyword">if</span> args.fp16 <span class="keyword">else</span> (torch.bfloat16 <span class="keyword">if</span> args.bf16 <span class="keyword">else</span> torch.float32))</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">        args.model_name_or_path,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">        load_in_4bit=args.bits == <span class="number">4</span>,</span><br><span class="line">        load_in_8bit=args.bits == <span class="number">8</span>,</span><br><span class="line">        device_map=device_map,</span><br><span class="line">        max_memory=max_memory,</span><br><span class="line">        quantization_config=BitsAndBytesConfig(</span><br><span class="line">            load_in_4bit=args.bits == <span class="number">4</span>,</span><br><span class="line">            load_in_8bit=args.bits == <span class="number">8</span>,</span><br><span class="line">            llm_int8_threshold=<span class="number">6.0</span>,</span><br><span class="line">            llm_int8_has_fp16_weight=<span class="literal">False</span>,</span><br><span class="line">            bnb_4bit_compute_dtype=compute_dtype,</span><br><span class="line">            bnb_4bit_use_double_quant=args.double_quant,</span><br><span class="line">            bnb_4bit_quant_type=args.quant_type,</span><br><span class="line">        ),</span><br><span class="line">        torch_dtype=(torch.float32 <span class="keyword">if</span> args.fp16 <span class="keyword">else</span> (torch.bfloat16 <span class="keyword">if</span> args.bf16 <span class="keyword">else</span> torch.float32)),</span><br><span class="line">        trust_remote_code=args.trust_remote_code,</span><br><span class="line">        use_auth_token=args.use_auth_token</span><br><span class="line">    )</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>Other than some necessary components like tokenizer,
<code>train()</code> gives an option of LoRA in addition to full
finetune. It requires setup of LoRA config and
<code>get_peft_model</code> function from <code>peft</code> package.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    ......</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> args.full_finetune:</span><br><span class="line">          model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> args.full_finetune:</span><br><span class="line">          <span class="keyword">if</span> checkpoint_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">              <span class="built_in">print</span>(<span class="string">&quot;Loading adapters from checkpoint.&quot;</span>)</span><br><span class="line">              model = PeftModel.from_pretrained(model, join(checkpoint_dir, <span class="string">&#x27;adapter_model&#x27;</span>), is_trainable=<span class="literal">True</span>)</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">              <span class="built_in">print</span>(<span class="string">f&#x27;adding LoRA modules...&#x27;</span>)</span><br><span class="line">              modules = find_all_linear_names(args, model)</span><br><span class="line">              config = LoraConfig(</span><br><span class="line">                  r=args.lora_r,</span><br><span class="line">                  lora_alpha=args.lora_alpha,</span><br><span class="line">                  target_modules=modules,</span><br><span class="line">                  lora_dropout=args.lora_dropout,</span><br><span class="line">                  bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">                  task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">              )</span><br><span class="line">              model = get_peft_model(model, config)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>Not every layers are quantized. QLoRA only quantizes linear
projection layers. Some layers like Layer Norm is sensitive to
precision, so high precision is required.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/di-blog/tags/knowledge/" rel="tag"># knowledge</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/di-blog/2024/12/25/Understanding-LoRA/" rel="prev" title="Understanding LoRA">
                  <i class="fa fa-angle-left"></i> Understanding LoRA
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/di-blog/2024/12/29/Understanding-RLHF/" rel="next" title="Understanding RLHF">
                  Understanding RLHF <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Di Zhen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/di-blog/js/comments.js"></script><script src="/di-blog/js/utils.js"></script><script src="/di-blog/js/motion.js"></script><script src="/di-blog/js/sidebar.js"></script><script src="/di-blog/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/di-blog/js/third-party/math/mathjax.js"></script>



</body>
</html>
